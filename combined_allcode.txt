================================================
File: combine_backend_files.py
================================================
import os

def read_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return f"\n\n# File: {file_path}\n\n" + file.read()
    except FileNotFoundError:
        return f"\n\n# File: {file_path}\n\n# Error: File not found"
    except Exception as e:
        return f"\n\n# File: {file_path}\n\n# Error: {str(e)}"

def combine_files():
    base_path = "dashboardbackend"
    files_to_combine = [
        "src/services/analytics_service.py",
        "src/services/metrics_service.py",
        "src/services/historical_data_service.py",
        "src/services/descope_service.py",
        "src/services/caching_service.py",
        "src/services/opensearch_service.py",
        "src/utils/query_builder.py",
        "src/api/metrics.py",
        "src/core/__init__.py",
        "tests/test_analytics_service.py",
        "requirements.txt",
        "run.py"
    ]

    combined_content = "# Combined Backend Files\n\n"

    for file_path in files_to_combine:
        full_path = os.path.join(base_path, file_path)
        combined_content += read_file(full_path)

    with open("combined_backend_files.txt", "w", encoding="utf-8") as output_file:
        output_file.write(combined_content)

    print("Files combined successfully. Output: combined_backend_files.txt")

if __name__ == "__main__":
    combine_files()

================================================
File: combine_frontend_files.py
================================================
import os

def combine_files(base_path, file_list, output_file):
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for file_path in file_list:
            full_path = os.path.join(base_path, file_path)
            outfile.write(f"\n\n# File: {file_path}\n")
            try:
                with open(full_path, 'r', encoding='utf-8') as infile:
                    outfile.write(infile.read())
            except FileNotFoundError:
                outfile.write(f"# Error: File not found - {file_path}\n")

base_path = "analytics-dashboard/src"
files_to_combine = [
    "components/analytics/UserActivity.tsx",
    "components/common/DateRangeSelector.tsx",
    "components/dashboard/Dashboard.tsx",
    "components/layout/Layout.tsx",
    "components/layout/Navbar.tsx",
    "components/layout/Sidebar.tsx",
    "components/metrics/MetricCard.tsx",
    "components/metrics/MetricGrid.tsx",
    "components/users/UserTable.tsx",
    "services/metricService.ts",
    "types/metrics.ts",
    "utils/App.tsx"
]

output_file = "combined_frontend_files.txt"

combine_files(base_path, files_to_combine, output_file)
print(f"Files combined successfully into {output_file}")

================================================
File: combined_frontend_files.txt
================================================


# File: components/analytics/UserActivity.tsx
import React, { useState, useEffect } from 'react';
import { format } from 'date-fns';
import DateRangeSelector from '../common/DateRangeSelector';

interface UserActivityData {
  trace_id: string;
  email: string;
  firstAction: string;
  lastAction: string;
  daysBetween: number;
  totalActions: number;
}

const UserActivity: React.FC = () => {
  const [startDate, setStartDate] = useState<Date>(new Date('2024-01-27'));
  const [endDate, setEndDate] = useState<Date>(new Date());
  const [filterType, setFilterType] = useState<string>('consecutive_days');
  const [loading, setLoading] = useState<boolean>(true);
  const [users, setUsers] = useState<UserActivityData[]>([]);

  useEffect(() => {
    fetchUserActivity();
  }, [startDate, endDate, filterType]);

  const fetchUserActivity = async () => {
    try {
      setLoading(true);
      const response = await fetch(`/metrics/user-activity?startDate=${startDate.toISOString()}&endDate=${endDate.toISOString()}&filterType=${filterType}`);
      const data = await response.json();
      if (data.status === 'success') {
        setUsers(data.users);
      }
    } catch (error) {
      console.error('Error fetching user activity:', error);
    } finally {
      setLoading(false);
    }
  };

  const handleDateChange = (start: Date, end: Date) => {
    setStartDate(start);
    setEndDate(end);
  };

  return (
    <div className="container mx-auto px-4 py-8">
      <div className="flex justify-between items-center mb-8">
        <h1 className="text-2xl font-bold text-gray-900">User Activity Analysis</h1>
      </div>

      <div className="mb-8 space-y-4">
        <DateRangeSelector
          startDate={startDate}
          endDate={endDate}
          onDateChange={handleDateChange}
        />

        <div className="flex gap-4">
          <button
            onClick={() => setFilterType('consecutive_days')}
            className={`px-4 py-2 rounded-md ${
              filterType === 'consecutive_days'
                ? 'bg-blue-600 text-white'
                : 'bg-gray-100 text-gray-700 hover:bg-gray-200'
            }`}
          >
            Consecutive Days
          </button>
          <button
            onClick={() => setFilterType('one_to_two_weeks')}
            className={`px-4 py-2 rounded-md ${
              filterType === 'one_to_two_weeks'
                ? 'bg-blue-600 text-white'
                : 'bg-gray-100 text-gray-700 hover:bg-gray-200'
            }`}
          >
            1-2 Weeks Apart
          </button>
          <button
            onClick={() => setFilterType('two_to_three_weeks')}
            className={`px-4 py-2 rounded-md ${
              filterType === 'two_to_three_weeks'
                ? 'bg-blue-600 text-white'
                : 'bg-gray-100 text-gray-700 hover:bg-gray-200'
            }`}
          >
            2-3 Weeks Apart
          </button>
          <button
            onClick={() => setFilterType('month_apart')}
            className={`px-4 py-2 rounded-md ${
              filterType === 'month_apart'
                ? 'bg-blue-600 text-white'
                : 'bg-gray-100 text-gray-700 hover:bg-gray-200'
            }`}
          >
            Month Apart
          </button>
        </div>
      </div>

      {loading ? (
        <div className="flex justify-center items-center h-64">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
        </div>
      ) : (
        <div className="bg-white rounded-lg shadow overflow-hidden">
          <div className="overflow-x-auto">
            <table className="min-w-full divide-y divide-gray-200">
              <thead className="bg-gray-50">
                <tr>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    User
                  </th>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    First Action
                  </th>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    Last Action
                  </th>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    Days Between
                  </th>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    Total Actions
                  </th>
                </tr>
              </thead>
              <tbody className="bg-white divide-y divide-gray-200">
                {users.map((user, index) => (
                  <tr key={user.trace_id} className="hover:bg-gray-50">
                    <td className="px-6 py-4 whitespace-nowrap">
                      <div className="text-sm text-gray-900">{user.email}</div>
                      <div className="text-sm text-gray-500">{user.trace_id}</div>
                    </td>
                    <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                      {format(new Date(user.firstAction), 'MMM d, yyyy HH:mm')}
                    </td>
                    <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                      {format(new Date(user.lastAction), 'MMM d, yyyy HH:mm')}
                    </td>
                    <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                      {user.daysBetween}
                    </td>
                    <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                      {user.totalActions}
                    </td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        </div>
      )}
    </div>
  );
};

export default UserActivity;

# File: components/common/DateRangeSelector.tsx
import React from 'react';
import { format } from 'date-fns';

interface DateRangeSelectorProps {
  startDate: Date;
  endDate: Date;
  onDateChange: (start: Date, end: Date) => void;
}

const DateRangeSelector: React.FC<DateRangeSelectorProps> = ({
  startDate,
  endDate,
  onDateChange,
}) => {
  return (
    <div className="flex items-center gap-4">
      <div className="flex items-center gap-2">
        <label htmlFor="start-date" className="text-sm text-gray-600">
          From
        </label>
        <input
          id="start-date"
          type="date"
          className="border border-gray-300 rounded-md px-3 py-1.5 text-sm"
          value={format(startDate, 'yyyy-MM-dd')}
          onChange={(e) => {
            const newStart = new Date(e.target.value);
            onDateChange(newStart, endDate);
          }}
          max={format(endDate, 'yyyy-MM-dd')}
        />
      </div>
      <div className="flex items-center gap-2">
        <label htmlFor="end-date" className="text-sm text-gray-600">
          To
        </label>
        <input
          id="end-date"
          type="date"
          className="border border-gray-300 rounded-md px-3 py-1.5 text-sm"
          value={format(endDate, 'yyyy-MM-dd')}
          onChange={(e) => {
            const newEnd = new Date(e.target.value);
            onDateChange(startDate, newEnd);
          }}
          min={format(startDate, 'yyyy-MM-dd')}
          max={format(new Date(), 'yyyy-MM-dd')}
        />
      </div>
      <div className="flex gap-2">
        <button
          onClick={() => {
            const end = new Date();
            const start = new Date();
            start.setDate(end.getDate() - 7);
            onDateChange(start, end);
          }}
          className="px-3 py-1.5 text-sm bg-gray-100 hover:bg-gray-200 rounded-md"
        >
          Last 7 days
        </button>
        <button
          onClick={() => {
            const end = new Date();
            const start = new Date();
            start.setDate(end.getDate() - 30);
            onDateChange(start, end);
          }}
          className="px-3 py-1.5 text-sm bg-gray-100 hover:bg-gray-200 rounded-md"
        >
          Last 30 days
        </button>
      </div>
    </div>
  );
};

export default DateRangeSelector;

# File: components/dashboard/Dashboard.tsx
import React, { useState, useEffect } from 'react';
import MetricGrid from '../metrics/MetricGrid';
import DateRangeSelector from '../common/DateRangeSelector';
import UserTable from '../users/UserTable';
import { fetchMetrics } from '../../services/metricService';
import { Metric } from '../../types/metrics';

const Dashboard: React.FC = () => {
  const [metrics, setMetrics] = useState<Metric[]>([]);
  const [startDate, setStartDate] = useState<Date>(new Date(Date.now() - 30 * 24 * 60 * 60 * 1000));
  const [endDate, setEndDate] = useState<Date>(new Date());
  const [includeV1, setIncludeV1] = useState<boolean>(true);
  const [selectedMetric, setSelectedMetric] = useState<string | null>(null);
  const [loading, setLoading] = useState<boolean>(true);

  useEffect(() => {
    loadMetrics();
  }, [startDate, endDate, includeV1]);

  const loadMetrics = async () => {
    try {
      setLoading(true);
      const response = await fetchMetrics(startDate, endDate, includeV1);
      setMetrics(response.metrics);
    } catch (error) {
      console.error('Error loading metrics:', error);
    } finally {
      setLoading(false);
    }
  };

  const handleMetricClick = (metricId: string) => {
    setSelectedMetric(metricId);
  };

  const handleDateChange = (start: Date, end: Date) => {
    setStartDate(start);
    setEndDate(end);
  };

  return (
    <div className="container mx-auto px-4 py-8">
      <div className="flex flex-col space-y-4 mb-8">
        <h1 className="text-2xl font-bold text-gray-900">Analytics Dashboard</h1>
        <div className="flex flex-wrap gap-4 items-center">
          <label className="flex items-center space-x-2 min-w-[150px]">
            <input
              type="checkbox"
              checked={includeV1}
              onChange={(e) => setIncludeV1(e.target.checked)}
              className="form-checkbox h-4 w-4 text-blue-600"
            />
            <span className="text-sm text-gray-700">Include V1</span>
          </label>
          <div className="flex-grow">
            <DateRangeSelector
              startDate={startDate}
              endDate={endDate}
              onDateChange={handleDateChange}
            />
          </div>
        </div>
      </div>

      {loading ? (
        <div className="flex justify-center items-center h-64">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
        </div>
      ) : (
        <>
          <MetricGrid metrics={metrics} onMetricClick={handleMetricClick} />
          {selectedMetric && (
            <UserTable 
              gaugeType={selectedMetric}
              timeRange={{
                start: startDate,
                end: endDate
              }}
            />
          )}
        </>
      )}
    </div>
  );
};

export default Dashboard;

# File: components/layout/Layout.tsx
// src/components/layout/Layout.tsx
import React from 'react'
import Navbar from './Navbar'
import Sidebar from './Sidebar'

interface LayoutProps {
  children: React.ReactNode
}

const Layout = ({ children }: LayoutProps) => {
  return (
    <div className="min-h-screen flex flex-col">
      <Navbar />
      <div className="flex flex-1 pt-16">
        <Sidebar />
        <main className="flex-1 ml-64 bg-gray-50 p-8">
          <div className="max-w-7xl mx-auto">
            {children}
          </div>
        </main>
      </div>
    </div>
  )
}

export default Layout

# File: components/layout/Navbar.tsx
// src/components/layout/Navbar.tsx
import React from 'react'
import { Menu } from 'lucide-react'

const Navbar = () => {
  return (
    <header className="fixed top-0 left-0 right-0 h-16 bg-white border-b border-gray-200 z-30">
      <div className="h-full mx-auto px-4">
        <div className="h-full flex items-center justify-between">
          <span className="text-lg font-semibold text-gray-900">
            Analytics Dashboard
          </span>
          <button 
            type="button"
            className="p-1.5 rounded-md text-gray-400 hover:text-gray-500 transition-colors"
          >
            <Menu size={18} />
          </button>
        </div>
      </div>
    </header>
  )
}

export default Navbar

# File: components/layout/Sidebar.tsx
// src/components/layout/Sidebar.tsx
import React, { useState, useEffect } from 'react'
import { Home, BarChart2, Users, Activity } from 'lucide-react'
import { useNavigate, useLocation } from 'react-router-dom'

const navigation = [
  { name: 'Overview', icon: Home, path: '/dashboard' },
  { name: 'Analytics', icon: BarChart2, path: '/dashboard' },
  { name: 'Users', icon: Users, path: '/dashboard' },
  { name: 'User Activity', icon: Activity, path: '/user-activity' },
]

const Sidebar = () => {
  const navigate = useNavigate()
  const location = useLocation()
  const [selected, setSelected] = useState(navigation[0].name)

  useEffect(() => {
    const currentPath = location.pathname
    const currentNav = navigation.find(nav => nav.path === currentPath)
    if (currentNav) {
      setSelected(currentNav.name)
    }
  }, [location])
  
  const handleNavigation = (item: typeof navigation[0]) => {
    setSelected(item.name)
    navigate(item.path)
  }
  
  return (
    <aside className="fixed top-16 left-0 w-64 h-[calc(100vh-4rem)] bg-white border-r border-gray-200">
      <nav className="h-full py-4">
        <div className="px-3 space-y-1">
          {navigation.map((item) => {
            const Icon = item.icon
            const isActive = selected === item.name
            
            return (
              <button
                key={item.name}
                onClick={() => handleNavigation(item)}
                className={`
                  flex items-center w-full px-3 py-2 text-sm rounded-md
                  ${isActive 
                    ? 'bg-gray-100 text-gray-900 font-medium' 
                    : 'text-gray-600 hover:bg-gray-50 hover:text-gray-900'}
                `}
              >
                <Icon 
                  size={18} 
                  className="mr-3 flex-shrink-0" 
                />
                {item.name}
              </button>
            )
          })}
        </div>
      </nav>
    </aside>
  )
}

export default Sidebar

# File: components/metrics/MetricCard.tsx
import React from 'react';
import { ArrowUpIcon, ArrowDownIcon } from '@heroicons/react/24/solid';
import { Metric } from '../../types/metrics';

interface MetricCardProps {
  metric: Metric;
  onClick?: () => void;
}

const MetricCard: React.FC<MetricCardProps> = ({ metric, onClick }) => {
  const { name, description, data } = metric;
  const showV1Data = metric.id === 'descope_users' || metric.id === 'thread_users';

  const formatNumber = (num: number | undefined) => {
    if (!num && num !== 0) return '0';
    if (num >= 1000000) {
      return `${(num / 1000000).toFixed(1)}M`;
    }
    if (num >= 1000) {
      return `${(num / 1000).toFixed(1)}k`;
    }
    return num.toString();
  };

  const getTrendIcon = () => {
    if (data.trend === 'up') {
      return <ArrowUpIcon className="h-4 w-4 text-green-500" />;
    }
    if (data.trend === 'down') {
      return <ArrowDownIcon className="h-4 w-4 text-red-500" />;
    }
    return null;
  };

  const getTotalValue = () => {
    const baseValue = data.value || 0;
    if (!showV1Data || !data.v1Value) {
      return baseValue;
    }
    return baseValue + data.v1Value;
  };

  const getValue = () => {
    return data.value !== undefined ? data.value : null;
  };

  return (
    <div 
      className="bg-white rounded-lg shadow-md p-6 cursor-pointer hover:shadow-lg transition-shadow"
      onClick={onClick}
    >
      <div className="flex flex-col space-y-2">
        <div className="flex justify-between items-start">
          <div>
            <h3 className="text-lg font-semibold text-gray-900">{name}</h3>
            <p className="text-sm text-gray-500">{description}</p>
          </div>
          <div className="flex items-center space-x-1">
            {getTrendIcon()}
          </div>
        </div>
        
        <div className="mt-4">
          <div className="flex flex-col">
            <div className="text-4xl font-bold text-gray-900">
              {formatNumber(getValue())}
            </div>
            {showV1Data && data.v1Value !== undefined && data.v1Value > 0 && (
              <div className="mt-2 flex flex-col">
                <div className="text-sm text-gray-500">
                  With V1: {formatNumber(getTotalValue())}
                </div>
                <div className="text-xs text-gray-400">
                  (V1: +{formatNumber(data.v1Value)})
                </div>
              </div>
            )}
          </div>
        </div>
      </div>
    </div>
  );
};

export default MetricCard;

# File: components/metrics/MetricGrid.tsx
import { FC } from 'react'
import { Metric } from '../../types/metrics'
import MetricCard from './MetricCard'

interface MetricGridProps {
  metrics: Metric[]
  onMetricClick: (metricId: string) => void
  includeV1?: boolean
}

const MetricGrid: FC<MetricGridProps> = ({ metrics, onMetricClick, includeV1 = true }) => {
  // Define the order of metrics and their display names
  const orderedMetricIds = [
    'descope_users',
    'thread_users',
    'render_users',
    'active_chat_users',
    'medium_chat_users',
    'sketch_users'
  ]

  // Custom display names and descriptions
  const displayConfig: { [key: string]: { name: string; description: string } } = {
    'descope_users': {
      name: 'Total Users',
      description: 'Total number of registered users'
    },
    'thread_users': {
      name: 'Active Users',
      description: 'Users who have started at least one message thread'
    },
    'render_users': {
      name: 'Producers',
      description: 'Users who have completed at least one render'
    },
    'active_chat_users': {
      name: 'Power Users',
      description: 'Users with more than 20 message threads'
    },
    'medium_chat_users': {
      name: 'Moderate Users',
      description: 'Users with 5-20 message threads'
    },
    'sketch_users': {
      name: 'Producers Attempting',
      description: 'Users who have uploaded at least one sketch'
    }
  }

  // Order and transform the metrics
  const orderedMetrics = orderedMetricIds
    .map(id => {
      const metric = metrics.find(m => m.id === id)
      if (!metric) return null

      return {
        ...metric,
        name: displayConfig[id]?.name || metric.name,
        description: displayConfig[id]?.description || metric.description
      }
    })
    .filter(Boolean)

  return (
    <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
      {orderedMetrics.map(metric => (
        <MetricCard
          key={metric!.id}
          metric={metric!}
          onClick={metric!.id !== 'descope_users' ? () => onMetricClick(metric!.id) : undefined}
          includeV1={includeV1}
        />
      ))}
    </div>
  )
}

export default MetricGrid

# File: components/users/UserTable.tsx
import React, { useState, useEffect } from 'react';
import { fetchUserStats } from '../../services/metricService';
import UserEventsModal from '../modal/UserEventsModal';

export interface UserStats {
  email: string;
  trace_id: string;
  messageCount: number;
  sketchCount: number;
  renderCount: number;
}

interface UserTableProps {
  gaugeType?: 'thread_users' | 'sketch_users' | 'render_users' | 'medium_chat_users' | 'active_chat_users';
  timeRange?: {
    start: Date;
    end: Date;
  };
}

const UserTable: React.FC<UserTableProps> = ({ gaugeType = 'thread_users', timeRange }) => {
  const [users, setUsers] = useState<UserStats[]>([]);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string>('');
  const [selectedUserId, setSelectedUserId] = useState<string | null>(null);
  const [sortConfig, setSortConfig] = useState<{
    key: keyof UserStats;
    direction: 'ascending' | 'descending';
  }>({ key: 'messageCount', direction: 'descending' });

  useEffect(() => {
    const fetchData = async () => {
      if (!timeRange) return;

      try {
        setLoading(true);
        setError('');
        const data = await fetchUserStats(timeRange.start, timeRange.end, gaugeType);
        setUsers(data);
      } catch (err) {
        setError('Error fetching user statistics');
        console.error('Error fetching users:', err);
      } finally {
        setLoading(false);
      }
    };

    fetchData();
  }, [gaugeType, timeRange]);

  const handleSort = (key: keyof UserStats) => {
    setSortConfig(current => ({
      key,
      direction: current.key === key && current.direction === 'ascending' 
        ? 'descending' 
        : 'ascending'
    }));
  };

  const handleUserClick = (userId: string) => {
    setSelectedUserId(userId);
  };

  const handleCloseModal = () => {
    setSelectedUserId(null);
  };

  const sortedUsers = [...users].sort((a, b) => {
    if (sortConfig.direction === 'ascending') {
      return a[sortConfig.key] > b[sortConfig.key] ? 1 : -1;
    }
    return a[sortConfig.key] < b[sortConfig.key] ? 1 : -1;
  });

  const getSortIcon = (key: keyof UserStats) => {
    if (sortConfig.key !== key) return '↕️';
    return sortConfig.direction === 'ascending' ? '↑' : '↓';
  };

  const getTableTitle = () => {
    switch (gaugeType) {
      case 'thread_users':
        return 'Thread Users';
      case 'sketch_users':
        return 'Sketch Users';
      case 'render_users':
        return 'Render Users';
      case 'medium_chat_users':
        return 'Medium Chat Users';
      case 'active_chat_users':
        return 'Power Users';
      default:
        return 'Users';
    }
  };

  const columns = [
    {
      key: 'email' as keyof UserStats,
      label: 'Email',
      sortable: true
    },
    {
      key: 'messageCount' as keyof UserStats,
      label: 'Messages',
      sortable: true
    },
    {
      key: 'sketchCount' as keyof UserStats,
      label: 'Sketches',
      sortable: true
    },
    {
      key: 'renderCount' as keyof UserStats,
      label: 'Renders',
      sortable: true
    },
    {
      key: 'trace_id' as keyof UserStats,
      label: 'ID',
      sortable: false
    }
  ];

  return (
    <div className="mt-8 bg-white rounded-lg shadow">
      <div className="px-6 py-4 border-b border-gray-200">
        <h2 className="text-xl font-semibold text-gray-800">{getTableTitle()} Statistics</h2>
      </div>
      
      {loading && (
        <div className="flex justify-center items-center py-8">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
        </div>
      )}
      
      {error && (
        <div className="text-red-500 p-4 text-center bg-red-50">
          {error}
        </div>
      )}
      
      {!loading && !error && users.length === 0 && (
        <div className="text-gray-500 text-center py-8">
          No user statistics available
        </div>
      )}
      
      {users.length > 0 && (
        <div className="overflow-x-auto">
          <table className="min-w-full divide-y divide-gray-200">
            <thead className="bg-gray-50">
              <tr>
                {columns.map(column => (
                  <th 
                    key={column.key}
                    className={`px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider ${
                      column.sortable ? 'cursor-pointer hover:bg-gray-100' : ''
                    }`}
                    onClick={() => column.sortable && handleSort(column.key)}
                  >
                    {column.label} {column.sortable && getSortIcon(column.key)}
                  </th>
                ))}
              </tr>
            </thead>
            <tbody className="bg-white divide-y divide-gray-200">
              {sortedUsers.map((user, index) => (
                <tr 
                  key={user.trace_id || index}
                  className="hover:bg-gray-50 transition-colors duration-150 ease-in-out cursor-pointer"
                  onClick={() => handleUserClick(user.trace_id)}
                >
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-blue-600 hover:text-blue-800">
                      {user.email}
                    </div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-gray-900">{user.messageCount}</div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-gray-900">{user.sketchCount}</div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-gray-900">{user.renderCount}</div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-gray-500">{user.trace_id}</div>
                  </td>
                </tr>
              ))}
            </tbody>
          </table>
        </div>
      )}

      {selectedUserId && (
        <UserEventsModal
          userId={selectedUserId}
          onClose={handleCloseModal}
        />
      )}
    </div>
  );
};

export default UserTable;

# File: services/metricService.ts
import axios from 'axios';
import { MetricResponse, Metric, UserStats } from '../types/metrics';

const API_URL = import.meta.env.VITE_API_URL || 'http://localhost:5001';

export const fetchMetrics = async (
  startDate: Date,
  endDate: Date,
  includeV1: boolean = true
): Promise<MetricResponse> => {
  try {
    const response = await axios.get(`${API_URL}/metrics`, {
      params: {
        startDate: startDate.toISOString(),
        endDate: endDate.toISOString(),
        includeV1
      }
    });

    if (!response.data || !response.data.metrics) {
      throw new Error('Invalid response format from backend');
    }

    const metrics: Metric[] = response.data.metrics.map((metric: any) => ({
      id: metric.id,
      name: metric.name,
      description: metric.description,
      category: metric.category,
      interval: metric.interval,
      data: {
        value: metric.data.value !== undefined ? Number(metric.data.value) : undefined,
        previousValue: metric.data.previousValue !== undefined ? Number(metric.data.previousValue) : undefined,
        trend: metric.data.trend || 'neutral',
        changePercentage: metric.data.changePercentage,
        v1Value: metric.data.v1Value !== undefined ? Number(metric.data.v1Value) : undefined
      }
    }));

    return {
      metrics,
      timeRange: {
        start: new Date(response.data.timeRange.start),
        end: new Date(response.data.timeRange.end)
      }
    };

  } catch (error) {
    console.error('Error fetching metrics:', error);
    const defaultMetrics: Metric[] = [
      'descope_users',
      'thread_users',
      'render_users',
      'active_chat_users',
      'medium_chat_users',
      'sketch_users'
    ].map(id => ({
      id,
      name: id.split('_').map(word => word.charAt(0).toUpperCase() + word.slice(1)).join(' '),
      description: 'Error loading data',
      category: id.includes('chat') || id === 'thread_users' ? 'engagement' : 'user',
      interval: 'daily',
      data: {
        value: 0,
        trend: 'neutral',
        changePercentage: 0
      }
    }));

    return {
      metrics: defaultMetrics,
      timeRange: { start: startDate, end: endDate }
    };
  }
};

export const fetchUserStats = async (
  startDate: Date,
  endDate: Date,
  gaugeType: string
): Promise<UserStats[]> => {
  try {
    const response = await axios.get<{ status: string; data: UserStats[] }>(`${API_URL}/metrics/user-stats`, {
      params: {
        startDate: startDate.toISOString(),
        endDate: endDate.toISOString(),
        gaugeType
      }
    });

    if (response.data.status === 'success' && Array.isArray(response.data.data)) {
      return response.data.data;
    }
    return [];
  } catch (error) {
    console.error('Error fetching user statistics:', error);
    return [];
  }
};

export interface UserEvent {
  event_name: string;
  timestamp: string;
  trace_id: string;
  flow_id?: string;
  [key: string]: any;
}

export interface UserEventsResponse {
  status: string;
  data: UserEvent[];
  timeRange: {
    start: string;
    end: string;
  };
}

export const fetchUserEvents = async (
  userId: string,
  startDate: Date = new Date(Date.now() - 30 * 24 * 60 * 60 * 1000),
  endDate: Date = new Date()
): Promise<UserEventsResponse> => {
  try {
    const response = await axios.get<UserEventsResponse>(`${API_URL}/metrics/user-events`, {
      params: { 
        traceId: userId,
        startDate: startDate.toISOString(),
        endDate: endDate.toISOString()
      }
    });

    if (response.data.status !== 'success' || !Array.isArray(response.data.data)) {
      throw new Error('Invalid response format from backend');
    }

    return response.data;
  } catch (error) {
    console.error('Error fetching user events:', error);
    throw error;
  }
};

# File: types/metrics.ts
export interface MetricValue {
  value: number;
  previousValue?: number;
  trend?: 'up' | 'down' | 'neutral';
  changePercentage?: number;
  v1Value?: number;
}

export interface Metric {
  id: string;
  name: string;
  description: string;
  category: 'user' | 'engagement' | 'performance';
  interval: 'daily' | 'weekly' | 'monthly';
  data: MetricValue;
}

export interface MetricResponse {
  metrics: Metric[];
  timeRange: {
    start: Date;
    end: Date;
  };
}

export interface UserStats {
  email: string;
  trace_id: string;
  messageCount: number;
  sketchCount: number;
  renderCount: number;
}

# File: utils/App.tsx
# Error: File not found - utils/App.tsx


================================================
File: package.json
================================================
{
  "dependencies": {
    "lucide-react": "^0.474.0"
  }
}


================================================
File: reference_code.txt
================================================
import logging
from typing import Dict, List, Optional
from opensearchpy import AsyncOpenSearch
from opensearchpy.exceptions import ConnectionError
import csv
import io
from datetime import datetime
from collections import defaultdict
import jwt

logger = logging.getLogger(__name__)

class EventDiscoveryService:
    def __init__(self, client: AsyncOpenSearch):
        self.client = client
        self.index = "events-v2"

    def _extract_user_id_from_token(self, auth_header: str) -> Optional[str]:
        """Extract v2UserId from JWT token."""
        try:
            if not auth_header or not auth_header.startswith('Bearer '):
                logger.debug("Invalid authorization header format")
                return None
            token = auth_header.split(' ')[1]
            payload = jwt.decode(token, options={"verify_signature": False})
            user_id = payload.get('v2UserId')
            if not user_id:
                logger.debug("No v2UserId found in token payload")
            return user_id
        except Exception as e:
            logger.warning(f"Failed to extract user ID from token: {str(e)}")
            return None

    async def get_events_by_trace_id(self, trace_id: str) -> List[Dict]:
        """Get all events for a specific trace_id."""
        try:
            # Query to get all events for the trace_id
            query = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"trace_id": trace_id}}
                        ]
                    }
                },
                "sort": [{"timestamp": "desc"}],
                "size": 1000  # Get up to 1000 events
            }

            # Search both indices
            response = await self.client.search(
                index=["events", "events-v2"],
                body=query
            )

            hits = response['hits']['hits']
            total_hits = response['hits']['total']['value']
            logger.info(f"Found {total_hits} events for trace_id {trace_id}")

            # Return full event data
            events = []
            for hit in hits:
                event = hit['_source']
                event['_index'] = hit['_index']  # Include which index it came from
                events.append(event)

            return {
                "total_events": total_hits,
                "events": events,
                "event_names": list(set(e.get('event_name') for e in events)),
                "service_names": list(set(e.get('service_name') for e in events)),
                "timestamp_range": {
                    "first": min(e.get('timestamp', 0) for e in events) if events else None,
                    "last": max(e.get('timestamp', 0) for e in events) if events else None
                }
            }

        except Exception as e:
            logger.error(f"Error getting events by trace_id: {str(e)}", exc_info=True)
            raise

    async def get_user_events(self, user_id: str) -> List[Dict]:
        """Get all events for a specific user."""
        try:
            # Query to get all events for the user
            query = {
                "query": {
                    "match_all": {}  # Get all events first to see the structure
                },
                "sort": [{"timestamp": "desc"}],
                "size": 100  # Limit to 100 most recent events
            }

            # Search both indices
            response = await self.client.search(
                index=["events", "events-v2"],
                body=query
            )

            hits = response['hits']['hits']
            logger.info(f"Found {len(hits)} events")

            # Return full event data for analysis
            events = []
            for hit in hits:
                event = hit['_source']
                event['_index'] = hit['_index']  # Include which index it came from
                events.append(event)

            return events

        except Exception as e:
            logger.error(f"Error getting user events: {str(e)}", exc_info=True)
            raise

    async def export_events_to_csv(self) -> str:
        """Export user_id and associated sketch_ids to CSV."""
        try:
            # Create a query to get events with both authorization header and sketchId
            query = {
                "query": {
                    "bool": {
                        "must": [
                            {"exists": {"field": "event_data.headers.authorization"}},
                            {"exists": {"field": "event_data.body.sketchId"}}
                        ]
                    }
                },
                "_source": ["event_data.headers.authorization", "event_data.body.sketchId"],
                "sort": [{"timestamp": "desc"}],
                "size": 10000
            }

            # Create a dictionary to store user_id -> set of sketch_ids mapping
            user_sketches = defaultdict(set)

            # Search with scroll to handle large datasets
            response = await self.client.search(
                index=self.index,
                body=query,
                scroll='2m'
            )
            scroll_id = response.get('_scroll_id')
            hits = response['hits']['hits']

            logger.info(f"Initial search returned {len(hits)} hits")
            total_processed = 0
            valid_pairs = 0

            while hits:
                for hit in hits:
                    total_processed += 1
                    source = hit.get('_source', {})
                    event_data = source.get('event_data', {})
                    
                    # Extract user ID from authorization header
                    auth_header = event_data.get('headers', {}).get('authorization')
                    user_id = self._extract_user_id_from_token(auth_header)
                    
                    # Extract sketch ID
                    sketch_id = event_data.get('body', {}).get('sketchId')
                    
                    if user_id and sketch_id:
                        valid_pairs += 1
                        user_sketches[user_id].add(sketch_id)
                        logger.debug(f"Added pair - user_id: {user_id}, sketch_id: {sketch_id}")

                # Get next batch of results
                if scroll_id:
                    response = await self.client.scroll(
                        scroll_id=scroll_id,
                        scroll='2m'
                    )
                    hits = response['hits']['hits']
                    logger.debug(f"Scroll returned {len(hits)} additional hits")
                else:
                    break

            logger.info(f"Processing complete - Total events: {total_processed}, "
                       f"Valid pairs: {valid_pairs}, "
                       f"Unique users: {len(user_sketches)}")

            # Create CSV in memory
            output = io.StringIO()
            writer = csv.writer(output)
            
            # Write header
            writer.writerow(['user_id', 'sketch_ids'])
            
            # Write data
            for user_id, sketch_ids in user_sketches.items():
                writer.writerow([user_id, ', '.join(sorted(sketch_ids))])

            # Generate filename with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"user_sketches_{timestamp}.csv"

            return output.getvalue(), filename

        except ConnectionError as e:
            logger.warning(f"OpenSearch connection failed during export: {str(e)}")
            # Return mock data for development
            output = io.StringIO()
            writer = csv.writer(output)
            writer.writerow(['user_id', 'sketch_ids'])
            writer.writerow(['user123', 'sketch456, sketch789'])
            writer.writerow(['user456', 'sketch123, sketch234'])
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"user_sketches_{timestamp}.csv"
            return output.getvalue(), filename

        except Exception as e:
            logger.error(f"Error exporting events: {str(e)}", exc_info=True)
            raise



import asyncio
import logging
from typing import Dict, List, Optional
from opensearchpy import AsyncOpenSearch
from opensearchpy.exceptions import ConnectionError, TransportError
from dashboardbackend.utils.query_builder import OpenSearchQueryBuilder

logger = logging.getLogger(__name__)

class EventRepository:
    def __init__(self, client: AsyncOpenSearch):
        self.client = client
        self.query_builder = OpenSearchQueryBuilder()
        self.index = "events-v2"  # Default index name
        self.max_retries = 3
        self.base_delay = 1  # Base delay in seconds

    async def _execute_with_retry(self, operation):
        """Execute an OpenSearch operation with exponential backoff retry."""
        for attempt in range(self.max_retries):
            try:
                return await operation()
            except (ConnectionError, TransportError) as e:
                if attempt == self.max_retries - 1:
                    raise e
                delay = self.base_delay * (2**attempt)  # Exponential backoff
                await asyncio.sleep(delay)

    async def get_producers_count(self) -> int:
        """Get count of users who have at least one sketch"""
        logger.info("Starting get_producers_count")
        
        query = {
            "aggs": {
                "unique_producers": {
                    "cardinality": {
                        "field": "event_data.body.butcherId.keyword"
                    }
                }
            },
            "query": {
                "bool": {
                    "must": [
                        {"term": {"event_name.keyword": "uploadSketch_end"}}
                    ]
                }
            }
        }

        async def execute():
            try:
                logger.info(f"Executing producers search query on index: {self.index}")
                response = await self.client.search(
                    index=self.index,
                    body=query,
                    size=0,  # We only need the aggregation
                    request_timeout=30
                )
                logger.info("Search response received")
                producer_count = response.get('aggregations', {}).get('unique_producers', {}).get('value', 0)
                logger.info(f"Found {producer_count} producers")
                return producer_count
            except Exception as e:
                logger.error(f"Error executing producers search: {str(e)}", exc_info=True)
                raise

        logger.info("Calling _execute_with_retry for producers count")
        return await self._execute_with_retry(execute)

    async def get_event_counts(
        self,
        start_time: str,
        end_time: str,
        event_name: Optional[str] = None,
        event_type: Optional[str] = None,
        interval: Optional[str] = "day",
    ) -> List[Dict]:
        """Get event counts with time-based aggregation."""
        must_conditions = [
            self.query_builder.build_date_range_query(start_time, end_time)
        ]

        if event_name:
            must_conditions.append({"term": {"event_name": event_name}})
        if event_type:
            must_conditions.append({"term": {"type": event_type}})

        # Use the query builder's aggregation functionality
        aggs = self.query_builder.build_aggregation_query("timestamp", interval)

        query = self.query_builder.build_composite_query(
            must_conditions=must_conditions, aggregations=aggs
        )

        async def execute():
            response = await self.client.search(
                index=self.index, body=query, size=0  # We only need aggregations
            )
            return self._process_time_series_response(response)

        return await self._execute_with_retry(execute)

    async def get_user_events(
        self,
        user_id: str,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        event_name: Optional[str] = None,
        page_token: Optional[str] = None,
        page_size: int = 100,
    ) -> Dict:
        """Get paginated events for a specific user."""
        must_conditions = [{"term": {"trace_id": user_id}}]

        if start_time and end_time:
            must_conditions.append(
                self.query_builder.build_date_range_query(start_time, end_time)
            )
        if event_name:
            must_conditions.append({"term": {"event_name": event_name}})

        pagination = self.query_builder.build_paginated_query(
            search_after=page_token, size=page_size
        )

        query = self.query_builder.build_composite_query(
            must_conditions=must_conditions,
            source_fields=["event_name", "timestamp", "type", "event_data"],
            pagination=pagination,
        )

        async def execute():
            response = await self.client.search(index=self.index, body=query)
            return self._process_user_events_response(response)

        return await self._execute_with_retry(execute)

    async def get_error_summary(
        self, start_time: str, end_time: str, interval: Optional[str] = None
    ) -> Dict:
        """Get error events summary and trends."""
        must_conditions = [
            self.query_builder.build_date_range_query(start_time, end_time),
            {"term": {"type": "error"}},
        ]

        aggs = {
            "aggs": {
                "errors_by_name": {
                    "terms": {"field": "error_name", "size": 100},
                    "aggs": {"latest_occurrence": {"max": {"field": "timestamp"}}},
                }
            }
        }

        if interval:
            aggs["aggs"]["error_trends"] = {
                "date_histogram": {
                    "field": "timestamp",
                    "fixed_interval": f"1{interval}",
                }
            }

        query = self.query_builder.build_composite_query(
            must_conditions=must_conditions, aggregations=aggs
        )

        async def execute():
            response = await self.client.search(index=self.index, body=query, size=0)
            return self._process_error_summary_response(response, interval)

        return await self._execute_with_retry(execute)

    async def get_path_analytics(
        self, start_time: str, end_time: str, limit: int = 10
    ) -> Dict:
        """Get analytics for popular request paths."""
        must_conditions = [
            self.query_builder.build_date_range_query(start_time, end_time)
        ]

        aggs = {
            "aggs": {
                "popular_paths": {
                    "terms": {"field": "path", "size": limit},
                    "aggs": {
                        "average_status": {"avg": {"field": "status_code"}},
                        "error_count": {
                            "filter": {"range": {"status_code": {"gte": 400}}}
                        },
                    },
                }
            }
        }

        query = self.query_builder.build_composite_query(
            must_conditions=must_conditions, aggregations=aggs
        )

        async def execute():
            response = await self.client.search(index=self.index, body=query, size=0)
            return self._process_path_analytics_response(response)

        return await self._execute_with_retry(execute)

    def _process_time_series_response(self, response: Dict) -> List[Dict]:
        """Process the response from get_event_counts query into time series format."""
        if "time_buckets" not in response.get("aggregations", {}):
            return []

        return [
            {"timestamp": bucket["key_as_string"], "count": bucket["doc_count"]}
            for bucket in response["aggregations"]["time_buckets"]["buckets"]
        ]

    def _process_user_events_response(self, response: Dict) -> Dict:
        """Process the response from get_user_events query."""
        hits = response["hits"]["hits"]
        events = [
            {
                "event_name": hit["_source"]["event_name"],
                "timestamp": hit["_source"]["timestamp"],
                "type": hit["_source"]["type"],
                "event_data": hit["_source"]["event_data"],
            }
            for hit in hits
        ]

        result = {"events": events}

        # Add next page token if there are more results
        if hits:
            last_hit = hits[-1]
            result["next_page_token"] = f"{last_hit['sort'][0]},{last_hit['sort'][1]}"

        return result

    def _process_error_summary_response(
        self, response: Dict, interval: Optional[str]
    ) -> Dict:
        """Process the response from get_error_summary query."""
        result = {
            "total_errors": response["hits"]["total"]["value"],
            "errors_by_name": [
                {
                    "error_name": bucket["key"],
                    "count": bucket["doc_count"],
                    "latest_occurrence": bucket["latest_occurrence"]["value_as_string"],
                }
                for bucket in response["aggregations"]["errors_by_name"]["buckets"]
            ],
        }

        if interval and "error_trends" in response["aggregations"]:
            result["error_trends"] = [
                {"timestamp": bucket["key_as_string"], "count": bucket["doc_count"]}
                for bucket in response["aggregations"]["error_trends"]["buckets"]
            ]

        return result

    def _process_path_analytics_response(self, response: Dict) -> Dict:
        """Process the response from get_path_analytics query."""
        paths = []
        for bucket in response["aggregations"]["popular_paths"]["buckets"]:
            total_requests = bucket["doc_count"]
            error_count = bucket["error_count"]["doc_count"]

            paths.append({
                "path": bucket["key"],
                "total_requests": total_requests,
                "average_status": bucket["average_status"]["value"],
                "error_rate": (error_count / total_requests if total_requests > 0 else 0),
            })

        return {"paths": paths}



================================================
File: .env.example
================================================
# Redis Configuration
REDIS_URL=redis://localhost:6379

OPENSEARCH_URL="https://localhost:9200"
OPENSEARCH_USERNAME="elkadmin"
OPENSEARCH_PASSWORD="^4/F;C\KMXiUOAJxkTh#TEV*TyDye&."
MAX_QUERY_TIME="30"
PORT="5001"
# Descope API Configuration
DESCOPE_API_URL="https://api.descope.com/v1/mgmt/user/search"
DESCOPE_BEARER_TOKEN="P2riizmYDJ2VAIjBw7ST0Qb2cNpd:K2ea9hNp0afhXPmXqZkJCLLFQLNRyJeqMOVZxU4qv1B43FmNXoM1UraFZcOJzpU5QXFzneU"
# Google Sheets Configuration
GOOGLE_SHEET_ID="1puVBbrVY1Gjr6UuWnp02hq6XBjJKpzILYW2YX16oBFA"

================================================
File: analytics-dashboard/README.md
================================================
# React + TypeScript + Vite

This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

Currently, two official plugins are available:

- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react/README.md) uses [Babel](https://babeljs.io/) for Fast Refresh
- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh

## Expanding the ESLint configuration

If you are developing a production application, we recommend updating the configuration to enable type aware lint rules:

- Configure the top-level `parserOptions` property like this:

```js
export default tseslint.config({
  languageOptions: {
    // other options...
    parserOptions: {
      project: ['./tsconfig.node.json', './tsconfig.app.json'],
      tsconfigRootDir: import.meta.dirname,
    },
  },
})
```

- Replace `tseslint.configs.recommended` to `tseslint.configs.recommendedTypeChecked` or `tseslint.configs.strictTypeChecked`
- Optionally add `...tseslint.configs.stylisticTypeChecked`
- Install [eslint-plugin-react](https://github.com/jsx-eslint/eslint-plugin-react) and update the config:

```js
// eslint.config.js
import react from 'eslint-plugin-react'

export default tseslint.config({
  // Set the react version
  settings: { react: { version: '18.3' } },
  plugins: {
    // Add the react plugin
    react,
  },
  rules: {
    // other rules...
    // Enable its recommended rules
    ...react.configs.recommended.rules,
    ...react.configs['jsx-runtime'].rules,
  },
})
```


================================================
File: analytics-dashboard/eslint.config.js
================================================
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import tseslint from 'typescript-eslint'

export default tseslint.config(
  { ignores: ['dist'] },
  {
    extends: [js.configs.recommended, ...tseslint.configs.recommended],
    files: ['**/*.{ts,tsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...reactHooks.configs.recommended.rules,
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
)


================================================
File: analytics-dashboard/index.html
================================================
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + React + TS</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>


================================================
File: analytics-dashboard/package.json
================================================
{
    "name": "analytics-dashboard",
    "private": true,
    "version": "0.0.0",
    "type": "module",
    "scripts": {
        "dev": "vite",
        "build": "tsc && vite build",
        "lint": "eslint src --ext ts,tsx --report-unused-disable-directives --max-warnings 0",
        "preview": "vite preview",
        "format": "prettier --write \"src/**/*.{ts,tsx}\""
    },
    "dependencies": {
        "@headlessui/react": "^1.7.17",
        "@heroicons/react": "^2.2.0",
        "@tanstack/react-query": "^4.29.19",
        "@tanstack/react-query-devtools": "^4.29.19",
        "axios": "^1.4.0",
        "date-fns": "^2.30.0",
        "lucide-react": "^0.474.0",
        "react": "^18.2.0",
        "react-circular-progressbar": "^2.1.0",
        "react-dom": "^18.2.0",
        "react-router-dom": "^7.1.5",
        "react-toastify": "^11.0.3",
        "recharts": "^2.7.2",
        "socket.io-client": "^4.7.1"
    },
    "devDependencies": {
        "@types/node": "^20.4.5",
        "@types/react": "^18.2.15",
        "@types/react-dom": "^18.2.7",
        "@types/socket.io-client": "^3.0.0",
        "@typescript-eslint/eslint-plugin": "^6.0.0",
        "@typescript-eslint/parser": "^6.0.0",
        "@vitejs/plugin-react": "^4.0.3",
        "autoprefixer": "^10.4.14",
        "eslint": "^8.45.0",
        "eslint-plugin-react-hooks": "^4.6.0",
        "eslint-plugin-react-refresh": "^0.4.3",
        "postcss": "^8.4.27",
        "prettier": "^3.0.0",
        "tailwindcss": "^3.3.3",
        "typescript": "^5.0.2",
        "vite": "^4.4.5"
    }
}


================================================
File: analytics-dashboard/postcss.config.js
================================================
export default {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}

================================================
File: analytics-dashboard/tailwind.config.js
================================================
/** @type {import('tailwindcss').Config} */
export default {
  content: [
    "./index.html",
    "./src/**/*.{js,ts,jsx,tsx}",
  ],
  theme: {
    extend: {},
  },
  plugins: [],
}

================================================
File: analytics-dashboard/tsconfig.app.json
================================================
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "isolatedModules": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["src"]
}


================================================
File: analytics-dashboard/tsconfig.json
================================================
{
  "files": [],
  "references": [
    { "path": "./tsconfig.app.json" },
    { "path": "./tsconfig.node.json" }
  ]
}


================================================
File: analytics-dashboard/tsconfig.node.json
================================================
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
    "target": "ES2022",
    "lib": ["ES2023"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "isolatedModules": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["vite.config.ts"]
}


================================================
File: analytics-dashboard/vite.config.ts
================================================
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
})


================================================
File: analytics-dashboard/.gitignore
================================================
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?


================================================
File: analytics-dashboard/src/App.tsx
================================================
import React from 'react';
import { BrowserRouter as Router, Routes, Route, Navigate } from 'react-router-dom';
import Dashboard from './components/dashboard/Dashboard';
import UserActivity from './components/analytics/UserActivity';
import Layout from './components/layout/Layout';

const App: React.FC = () => {
  return (
    <Router>
      <Layout>
        <div className="min-h-screen bg-gray-50">
          <Routes>
            <Route path="/" element={<Navigate to="/dashboard" replace />} />
            <Route path="/dashboard" element={<Dashboard />} />
            <Route path="/user-activity" element={<UserActivity />} />
          </Routes>
        </div>
      </Layout>
    </Router>
  );
};

export default App;

================================================
File: analytics-dashboard/src/dub_App.tsx
================================================
import React from 'react';
import { BrowserRouter as Router, Routes, Route } from 'react-router-dom';
import Dashboard from './components/dashboard/Dashboard';
import AnalyticsDashboard from './components/dashboard/dub_AnalyticsDashboard';
import Layout from './components/layout/Layout';
import Sidebar from './components/layout/dub_Sidebar';

const DubLayout: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  return (
    <div className="min-h-screen flex flex-col">
      <div className="flex flex-1">
        <Sidebar />
        <main className="flex-1 ml-64 bg-gray-50 p-8">
          <div className="max-w-7xl mx-auto">
            {children}
          </div>
        </main>
      </div>
    </div>
  );
};

const App: React.FC = () => {
  return (
    <Router>
      <DubLayout>
        <Routes>
          <Route path="/" element={<Dashboard />} />
          <Route path="/analytics" element={<Dashboard />} />
          <Route path="/user-activity" element={<AnalyticsDashboard />} />
          <Route path="/users" element={<Dashboard />} />
        </Routes>
      </DubLayout>
    </Router>
  );
};

export default App;

================================================
File: analytics-dashboard/src/dub_main.tsx
================================================
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './dub_App'
import './index.css'

ReactDOM.createRoot(document.getElementById('root') as HTMLElement).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
)

================================================
File: analytics-dashboard/src/index.css
================================================
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --foreground-rgb: 0, 0, 0;
  --background-rgb: 249, 250, 251;
}

body {
  color: rgb(var(--foreground-rgb));
  background: rgb(var(--background-rgb));
  min-height: 100vh;
}

#root {
  min-height: 100vh;
}

================================================
File: analytics-dashboard/src/main.tsx
================================================
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App'
import './index.css'

ReactDOM.createRoot(document.getElementById('root') as HTMLElement).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
)

================================================
File: analytics-dashboard/src/vite-env.d.ts
================================================
/// <reference types="vite/client" />


================================================
File: analytics-dashboard/src/components/analytics/UserActivity.tsx
================================================
import React, { useState, useEffect } from 'react';
import { format } from 'date-fns';
import DateRangeSelector from '../common/DateRangeSelector';

interface UserActivityData {
  trace_id: string;
  email: string;
  firstAction: string;
  lastAction: string;
  daysBetween: number;
  totalActions: number;
}

const UserActivity: React.FC = () => {
  const [startDate, setStartDate] = useState<Date>(new Date('2024-01-27'));
  const [endDate, setEndDate] = useState<Date>(new Date());
  const [filterType, setFilterType] = useState<string>('consecutive_days');
  const [loading, setLoading] = useState<boolean>(true);
  const [users, setUsers] = useState<UserActivityData[]>([]);

  useEffect(() => {
    fetchUserActivity();
  }, [startDate, endDate, filterType]);

  const fetchUserActivity = async () => {
    try {
      setLoading(true);
      const response = await fetch(`/metrics/user-activity?startDate=${startDate.toISOString()}&endDate=${endDate.toISOString()}&filterType=${filterType}`);
      const data = await response.json();
      if (data.status === 'success') {
        setUsers(data.users);
      }
    } catch (error) {
      console.error('Error fetching user activity:', error);
    } finally {
      setLoading(false);
    }
  };

  const handleDateChange = (start: Date, end: Date) => {
    setStartDate(start);
    setEndDate(end);
  };

  return (
    <div className="container mx-auto px-4 py-8">
      <div className="flex justify-between items-center mb-8">
        <h1 className="text-2xl font-bold text-gray-900">User Activity Analysis</h1>
      </div>

      <div className="mb-8 space-y-4">
        <DateRangeSelector
          startDate={startDate}
          endDate={endDate}
          onDateChange={handleDateChange}
        />

        <div className="flex gap-4">
          <button
            onClick={() => setFilterType('consecutive_days')}
            className={`px-4 py-2 rounded-md ${
              filterType === 'consecutive_days'
                ? 'bg-blue-600 text-white'
                : 'bg-gray-100 text-gray-700 hover:bg-gray-200'
            }`}
          >
            Consecutive Days
          </button>
          <button
            onClick={() => setFilterType('one_to_two_weeks')}
            className={`px-4 py-2 rounded-md ${
              filterType === 'one_to_two_weeks'
                ? 'bg-blue-600 text-white'
                : 'bg-gray-100 text-gray-700 hover:bg-gray-200'
            }`}
          >
            1-2 Weeks Apart
          </button>
          <button
            onClick={() => setFilterType('two_to_three_weeks')}
            className={`px-4 py-2 rounded-md ${
              filterType === 'two_to_three_weeks'
                ? 'bg-blue-600 text-white'
                : 'bg-gray-100 text-gray-700 hover:bg-gray-200'
            }`}
          >
            2-3 Weeks Apart
          </button>
          <button
            onClick={() => setFilterType('month_apart')}
            className={`px-4 py-2 rounded-md ${
              filterType === 'month_apart'
                ? 'bg-blue-600 text-white'
                : 'bg-gray-100 text-gray-700 hover:bg-gray-200'
            }`}
          >
            Month Apart
          </button>
        </div>
      </div>

      {loading ? (
        <div className="flex justify-center items-center h-64">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
        </div>
      ) : (
        <div className="bg-white rounded-lg shadow overflow-hidden">
          <div className="overflow-x-auto">
            <table className="min-w-full divide-y divide-gray-200">
              <thead className="bg-gray-50">
                <tr>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    User
                  </th>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    First Action
                  </th>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    Last Action
                  </th>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    Days Between
                  </th>
                  <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                    Total Actions
                  </th>
                </tr>
              </thead>
              <tbody className="bg-white divide-y divide-gray-200">
                {users.map((user, index) => (
                  <tr key={user.trace_id} className="hover:bg-gray-50">
                    <td className="px-6 py-4 whitespace-nowrap">
                      <div className="text-sm text-gray-900">{user.email}</div>
                      <div className="text-sm text-gray-500">{user.trace_id}</div>
                    </td>
                    <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                      {format(new Date(user.firstAction), 'MMM d, yyyy HH:mm')}
                    </td>
                    <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                      {format(new Date(user.lastAction), 'MMM d, yyyy HH:mm')}
                    </td>
                    <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                      {user.daysBetween}
                    </td>
                    <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
                      {user.totalActions}
                    </td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        </div>
      )}
    </div>
  );
};

export default UserActivity;

================================================
File: analytics-dashboard/src/components/common/DateRangeSelector.tsx
================================================
import React from 'react';
import { format } from 'date-fns';

interface DateRangeSelectorProps {
  startDate: Date;
  endDate: Date;
  onDateChange: (start: Date, end: Date) => void;
}

const DateRangeSelector: React.FC<DateRangeSelectorProps> = ({
  startDate,
  endDate,
  onDateChange,
}) => {
  return (
    <div className="flex items-center gap-4">
      <div className="flex items-center gap-2">
        <label htmlFor="start-date" className="text-sm text-gray-600">
          From
        </label>
        <input
          id="start-date"
          type="date"
          className="border border-gray-300 rounded-md px-3 py-1.5 text-sm"
          value={format(startDate, 'yyyy-MM-dd')}
          onChange={(e) => {
            const newStart = new Date(e.target.value);
            onDateChange(newStart, endDate);
          }}
          max={format(endDate, 'yyyy-MM-dd')}
        />
      </div>
      <div className="flex items-center gap-2">
        <label htmlFor="end-date" className="text-sm text-gray-600">
          To
        </label>
        <input
          id="end-date"
          type="date"
          className="border border-gray-300 rounded-md px-3 py-1.5 text-sm"
          value={format(endDate, 'yyyy-MM-dd')}
          onChange={(e) => {
            const newEnd = new Date(e.target.value);
            onDateChange(startDate, newEnd);
          }}
          min={format(startDate, 'yyyy-MM-dd')}
          max={format(new Date(), 'yyyy-MM-dd')}
        />
      </div>
      <div className="flex gap-2">
        <button
          onClick={() => {
            const end = new Date();
            const start = new Date();
            start.setDate(end.getDate() - 7);
            onDateChange(start, end);
          }}
          className="px-3 py-1.5 text-sm bg-gray-100 hover:bg-gray-200 rounded-md"
        >
          Last 7 days
        </button>
        <button
          onClick={() => {
            const end = new Date();
            const start = new Date();
            start.setDate(end.getDate() - 30);
            onDateChange(start, end);
          }}
          className="px-3 py-1.5 text-sm bg-gray-100 hover:bg-gray-200 rounded-md"
        >
          Last 30 days
        </button>
      </div>
    </div>
  );
};

export default DateRangeSelector;

================================================
File: analytics-dashboard/src/components/dashboard/Dashboard.tsx
================================================
import React, { useState, useEffect } from 'react';
import MetricGrid from '../metrics/MetricGrid';
import DateRangeSelector from '../common/DateRangeSelector';
import UserTable from '../users/UserTable';
import { fetchMetrics } from '../../services/metricService';
import { Metric } from '../../types/metrics';

type GaugeType = 'thread_users' | 'sketch_users' | 'render_users' | 'medium_chat_users' | 'active_chat_users';

const Dashboard: React.FC = () => {
  const [metrics, setMetrics] = useState<Metric[]>([]);
  const [startDate, setStartDate] = useState<Date>(() => {
    const now = new Date();
    return new Date(now.getFullYear(), now.getMonth(), 1); // First day of current month
  });
  const [endDate, setEndDate] = useState<Date>(new Date());
  const [selectedMetric, setSelectedMetric] = useState<GaugeType | null>(null);
  const [loading, setLoading] = useState<boolean>(true);

  useEffect(() => {
    loadMetrics();
  }, [startDate, endDate]);

  const loadMetrics = async () => {
    try {
      setLoading(true);
      const response = await fetchMetrics(startDate, endDate);
      setMetrics(response.metrics);
    } catch (error) {
      console.error('Error loading metrics:', error);
    } finally {
      setLoading(false);
    }
  };

  const handleMetricClick = (metricId: string) => {
    setSelectedMetric(metricId as GaugeType);
  };

  const handleDateChange = (start: Date, end: Date) => {
    setStartDate(start);
    setEndDate(end);
  };

  return (
    <div className="container mx-auto px-4 py-8">
      <div className="flex flex-col space-y-4 mb-8">
        <h1 className="text-2xl font-bold text-gray-900">Analytics Dashboard</h1>
        <div className="flex flex-wrap gap-4 items-center">
          <div className="flex-grow">
            <DateRangeSelector
              startDate={startDate}
              endDate={endDate}
              onDateChange={handleDateChange}
            />
          </div>
        </div>
      </div>

      {loading ? (
        <div className="flex justify-center items-center h-64">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
        </div>
      ) : (
        <>
          <MetricGrid metrics={metrics} onMetricClick={handleMetricClick} />
          {selectedMetric && (
            <UserTable 
              gaugeType={selectedMetric}
              timeRange={{
                start: startDate,
                end: endDate
              }}
            />
          )}
        </>
      )}
    </div>
  );
};

export default Dashboard;

================================================
File: analytics-dashboard/src/components/layout/Layout.tsx
================================================
// src/components/layout/Layout.tsx
import React from 'react'
import Navbar from './Navbar'
import Sidebar from './Sidebar'

interface LayoutProps {
  children: React.ReactNode
}

const Layout = ({ children }: LayoutProps) => {
  return (
    <div className="min-h-screen flex flex-col">
      <Navbar />
      <div className="flex flex-1 pt-16">
        <Sidebar />
        <main className="flex-1 ml-64 bg-gray-50 p-8">
          <div className="max-w-7xl mx-auto">
            {children}
          </div>
        </main>
      </div>
    </div>
  )
}

export default Layout

================================================
File: analytics-dashboard/src/components/layout/Navbar.tsx
================================================
// src/components/layout/Navbar.tsx
import React from 'react'
import { Menu } from 'lucide-react'

const Navbar = () => {
  return (
    <header className="fixed top-0 left-0 right-0 h-16 bg-white border-b border-gray-200 z-30">
      <div className="h-full mx-auto px-4">
        <div className="h-full flex items-center justify-between">
          <span className="text-lg font-semibold text-gray-900">
            Analytics Dashboard
          </span>
          <button 
            type="button"
            className="p-1.5 rounded-md text-gray-400 hover:text-gray-500 transition-colors"
          >
            <Menu size={18} />
          </button>
        </div>
      </div>
    </header>
  )
}

export default Navbar

================================================
File: analytics-dashboard/src/components/layout/Sidebar.tsx
================================================
// src/components/layout/Sidebar.tsx
import React, { useState, useEffect } from 'react'
import { Home, BarChart2, Users, Activity } from 'lucide-react'
import { useNavigate, useLocation } from 'react-router-dom'

const navigation = [
  { name: 'Overview', icon: Home, path: '/dashboard' },
  { name: 'Analytics', icon: BarChart2, path: '/dashboard' },
  { name: 'Users', icon: Users, path: '/dashboard' },
  { name: 'User Activity', icon: Activity, path: '/user-activity' },
]

const Sidebar = () => {
  const navigate = useNavigate()
  const location = useLocation()
  const [selected, setSelected] = useState(navigation[0].name)

  useEffect(() => {
    const currentPath = location.pathname
    const currentNav = navigation.find(nav => nav.path === currentPath)
    if (currentNav) {
      setSelected(currentNav.name)
    }
  }, [location])
  
  const handleNavigation = (item: typeof navigation[0]) => {
    setSelected(item.name)
    navigate(item.path)
  }
  
  return (
    <aside className="fixed top-16 left-0 w-64 h-[calc(100vh-4rem)] bg-white border-r border-gray-200">
      <nav className="h-full py-4">
        <div className="px-3 space-y-1">
          {navigation.map((item) => {
            const Icon = item.icon
            const isActive = selected === item.name
            
            return (
              <button
                key={item.name}
                onClick={() => handleNavigation(item)}
                className={`
                  flex items-center w-full px-3 py-2 text-sm rounded-md
                  ${isActive 
                    ? 'bg-gray-100 text-gray-900 font-medium' 
                    : 'text-gray-600 hover:bg-gray-50 hover:text-gray-900'}
                `}
              >
                <Icon 
                  size={18} 
                  className="mr-3 flex-shrink-0" 
                />
                {item.name}
              </button>
            )
          })}
        </div>
      </nav>
    </aside>
  )
}

export default Sidebar

================================================
File: analytics-dashboard/src/components/metrics/MetricCard.tsx
================================================
import React from 'react';
import { ArrowUpIcon, ArrowDownIcon } from '@heroicons/react/24/solid';
import { Metric } from '../../types/metrics';

interface MetricCardProps {
  metric: Metric;
  onClick?: () => void;
}

const MetricCard: React.FC<MetricCardProps> = ({ metric, onClick }) => {
  const { name, description, data } = metric;
  const showV1Data = metric.id === 'descope_users' || metric.id === 'thread_users';

  const formatNumber = (num: number | undefined) => {
    if (!num && num !== 0) return '0';
    if (num >= 1000000) {
      return `${(num / 1000000).toFixed(1)}M`;
    }
    if (num >= 1000) {
      return `${(num / 1000).toFixed(1)}k`;
    }
    return num.toString();
  };

  const getTrendIcon = () => {
    if (data.trend === 'up') {
      return <ArrowUpIcon className="h-4 w-4 text-green-500" />;
    }
    if (data.trend === 'down') {
      return <ArrowDownIcon className="h-4 w-4 text-red-500" />;
    }
    return null;
  };

  const getTotalValue = () => {
    const baseValue = data.value || 0;
    if (!showV1Data || !data.v1Value) {
      return baseValue;
    }
    return baseValue + data.v1Value;
  };

  const getValue = () => {
    return data.value !== undefined ? data.value : null;
  };

  return (
    <div 
      className="bg-white rounded-lg shadow-md p-6 cursor-pointer hover:shadow-lg transition-shadow"
      onClick={onClick}
    >
      <div className="flex flex-col space-y-2">
        <div className="flex justify-between items-start">
          <div>
            <h3 className="text-lg font-semibold text-gray-900">{name}</h3>
            <p className="text-sm text-gray-500">{description}</p>
          </div>
          <div className="flex items-center space-x-1">
            {getTrendIcon()}
          </div>
        </div>
        
        <div className="mt-4">
          <div className="flex flex-col">
            <div className="text-4xl font-bold text-gray-900">
              {formatNumber(getValue())}
            </div>
            {showV1Data && data.v1Value !== undefined && data.v1Value > 0 && (
              <div className="mt-2 flex flex-col">
                <div className="text-sm text-gray-500">
                  With V1: {formatNumber(getTotalValue())}
                </div>
                <div className="text-xs text-gray-400">
                  (V1: +{formatNumber(data.v1Value)})
                </div>
              </div>
            )}
          </div>
        </div>
      </div>
    </div>
  );
};

export default MetricCard;

================================================
File: analytics-dashboard/src/components/metrics/MetricGauge.tsx
================================================
import React, { useState } from 'react';
import { Metric } from '../../types/metrics';
import { setMetricTarget } from '../../services/metricService';
import { toast } from 'react-toastify';

interface MetricGaugeProps {
  metric: Metric;
  onClick?: () => void;
  size?: 'small' | 'normal';
  className?: string;
  onTargetUpdate?: () => void;
}

const MetricGauge: React.FC<MetricGaugeProps> = ({ 
  metric, 
  onClick, 
  size = 'normal',
  className = '',
  onTargetUpdate
}) => {
  const [showTargetForm, setShowTargetForm] = useState(false);
  const [target, setTarget] = useState<number>(metric.data.target || 0);
  const [isSaving, setIsSaving] = useState(false);

  const value = metric.data.value;
  const percentage = target > 0 ? (value / target) * 100 : 0;

  const getColor = () => {
    if (target === 0) return 'bg-gray-200';
    if (percentage >= 100) return 'bg-green-500';
    if (percentage >= 75) return 'bg-yellow-500';
    return 'bg-red-500';
  };

  const handleTargetSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setIsSaving(true);
    try {
      await setMetricTarget(metric.id, target);
      toast.success('Target updated successfully');
      setShowTargetForm(false);
      if (onTargetUpdate) {
        onTargetUpdate();
      }
    } catch (error) {
      toast.error('Failed to update target');
      console.error('Error saving target:', error);
    } finally {
      setIsSaving(false);
    }
  };

  return (
    <div 
      className={`flex flex-col items-center p-4 ${className} relative bg-white rounded-lg shadow-sm`}
      style={{ width: size === 'small' ? '200px' : '250px' }}
    >
      <div className="w-full mb-2">
        <div className="flex justify-between items-center mb-1">
          <span className="text-sm font-medium text-gray-700">{metric.name}</span>
          <button 
            onClick={(e) => {
              e.stopPropagation();
              setShowTargetForm(true);
            }}
            className="text-xs text-blue-600 hover:text-blue-800"
          >
            Set Target
          </button>
        </div>
        <div className="w-full bg-gray-200 rounded-full h-4">
          <div 
            className={`${getColor()} h-4 rounded-full transition-all duration-500`}
            style={{ width: `${Math.min(percentage, 100)}%` }}
          />
        </div>
      </div>
      <div className="text-center" onClick={onClick}>
        <div className="text-2xl font-bold text-gray-900">
          {value.toLocaleString()}
        </div>
        {target > 0 && (
          <div className="text-sm text-gray-500">
            Target: {target.toLocaleString()} ({Math.round(percentage)}%)
          </div>
        )}
        <div className="text-sm text-gray-500 mt-1">
          {metric.description}
        </div>
      </div>

      {showTargetForm && (
        <div className="absolute top-0 left-0 w-full h-full bg-white bg-opacity-95 p-4 rounded-lg shadow-lg z-10">
          <form onSubmit={handleTargetSubmit} className="space-y-4">
            <div>
              <label className="block text-sm font-medium text-gray-700">
                Target Value for {metric.name}
              </label>
              <input
                type="number"
                value={target}
                onChange={(e) => setTarget(Number(e.target.value))}
                className="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-blue-500 focus:ring-blue-500 sm:text-sm"
                min="0"
                step="1"
              />
            </div>
            <div className="flex justify-end space-x-2">
              <button
                type="button"
                onClick={() => setShowTargetForm(false)}
                className="px-3 py-2 text-sm font-medium text-gray-700 bg-white border border-gray-300 rounded-md hover:bg-gray-50"
                disabled={isSaving}
              >
                Cancel
              </button>
              <button
                type="submit"
                className="px-3 py-2 text-sm font-medium text-white bg-blue-600 rounded-md hover:bg-blue-700 disabled:opacity-50"
                disabled={isSaving}
              >
                {isSaving ? 'Saving...' : 'Save'}
              </button>
            </div>
          </form>
        </div>
      )}
    </div>
  );
};

export default MetricGauge;


================================================
File: analytics-dashboard/src/components/metrics/MetricGrid.tsx
================================================
import { FC } from 'react'
import { Metric } from '../../types/metrics'
import MetricGauge from './MetricGauge'

interface MetricGridProps {
  metrics: Metric[]
  onMetricClick: (metricId: string) => void
}

const MetricGrid: FC<MetricGridProps> = ({ metrics, onMetricClick }) => {
  const orderedMetrics = metrics
    .map(metric => metric)
    .sort((a, b) => {
      if (a.category === b.category) {
        return 0;
      }
      if (a.category === 'user') {
        return -1;
      }
      if (b.category === 'user') {
        return 1;
      }
      return 0;
    })
    .filter(Boolean);

  return (
    <div>
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
        {orderedMetrics.filter(metric => metric.category !== 'historical').map(metric => (
          <div key={metric!.id} className="flex flex-col items-center">
            <MetricGauge metric={metric!} onClick={() => onMetricClick(metric!.id)} />
          </div>
        ))}
      </div>
      
      {/* Historical Metrics Section */}
      <div className="mt-12 border-t pt-8">
        <h2 className="text-xl font-semibold mb-6 text-center">Historical Totals (V1 + Current)</h2>
        <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
          {orderedMetrics.filter(metric => metric.category === 'historical').map(metric => (
            <div key={metric!.id} className="flex flex-col items-center">
              <MetricGauge 
                metric={metric!}
                size="small"
                className="transform scale-75"
              />
            </div>
          ))}
        </div>
      </div>
    </div>
  );
}

export default MetricGrid

================================================
File: analytics-dashboard/src/components/modal/GaugeUserModal.tsx
================================================
import React, { useState, useEffect } from 'react';
import axios from 'axios';

interface GaugeUserModalProps {
  metricId: string;
  startDate: string; // ISO string
  endDate: string;   // ISO string
  onClose: () => void;
}

const GaugeUserModal: React.FC<GaugeUserModalProps> = ({ metricId, startDate, endDate, onClose }) => {
  const [userIds, setUserIds] = useState<string[]>([]);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string>('');

  useEffect(() => {
    const fetchGaugeUsers = async () => {
      try {
        setLoading(true);
        setError('');
        const apiUrl = import.meta.env.VITE_API_URL || 'http://localhost:5001';
        const response = await axios.get(`${apiUrl}/getGaugeUserCandidates`, {
          params: { metricId, startDate, endDate }
        });
        
        if (response.data.error) {
          throw new Error(response.data.error);
        }
        
        if (!response.data.userIds || !Array.isArray(response.data.userIds)) {
          throw new Error('Invalid response format');
        }
        
        setUserIds(response.data.userIds);
      } catch (err: any) {
        console.error('Error fetching gauge users:', err);
        setError(err.message || 'Error fetching gauge users');
      } finally {
        setLoading(false);
      }
    };

    fetchGaugeUsers();
  }, [metricId, startDate, endDate]);

  const getMetricTitle = () => {
    switch (metricId) {
      case 'power_users':
        return 'Power Users (20+ messages)';
      case 'medium_chat_users':
        return 'Medium Chat Users (5-20 messages)';
      case 'active_users':
        return 'Active Users';
      case 'thread_users':
        return 'Thread Users';
      case 'sketch_users':
        return 'Sketch Users';
      case 'render_users':
        return 'Render Users';
      default:
        return 'Users';
    }
  };

  return (
    <div className="fixed inset-0 bg-black bg-opacity-50 flex justify-center items-center z-50 p-4">
      <div className="bg-white p-6 rounded-lg w-full max-w-lg">
        <div className="flex justify-between items-center mb-4">
          <h2 className="text-xl font-semibold">{getMetricTitle()}</h2>
          <button 
            onClick={onClose}
            className="text-gray-500 hover:text-gray-700 transition-colors"
          >
            Close
          </button>
        </div>
        
        {loading && (
          <div className="flex justify-center items-center py-8">
            <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
          </div>
        )}
        
        {error && (
          <div className="bg-red-50 text-red-500 p-4 rounded-md mb-4">
            {error}
          </div>
        )}
        
        {!loading && !error && (
          <>
            <div className="mb-2 text-sm text-gray-500">
              Total Users: {userIds.length}
            </div>
            <div className="max-h-96 overflow-y-auto">
              {userIds.length === 0 ? (
                <div className="text-center py-8 text-gray-500">
                  No users found for this criteria
                </div>
              ) : (
                <ul className="divide-y divide-gray-200">
                  {userIds.map((userId, index) => (
                    <li 
                      key={userId} 
                      className="py-2 hover:bg-gray-50 transition-colors"
                    >
                      <div className="flex items-center">
                        <span className="w-8 text-gray-500 text-sm">
                          {index + 1}.
                        </span>
                        <span className="flex-1 font-mono text-sm">
                          {userId}
                        </span>
                      </div>
                    </li>
                  ))}
                </ul>
              )}
            </div>
          </>
        )}
      </div>
    </div>
  );
};

export default GaugeUserModal;

================================================
File: analytics-dashboard/src/components/modal/UserDetailModal.tsx
================================================
import React, { useState, useEffect } from 'react'
import { fetchUserEvents, UserEvent, UserEventsResponse } from '../../services/metricService'

interface UserDetailModalProps {
  metricId: string
  onClose: () => void
}

const UserDetailModal: React.FC<UserDetailModalProps> = ({ metricId, onClose }) => {
  const [data, setData] = useState<UserEvent[] | null>(null)
  const [loading, setLoading] = useState<boolean>(true)
  const [error, setError] = useState<string>('')
  const [startDate, setStartDate] = useState<Date>(new Date(Date.now() - 30 * 24 * 60 * 60 * 1000)) // 30 days ago
  const [endDate, setEndDate] = useState<Date>(new Date())

  useEffect(() => {
    const fetchData = async () => {
      try {
        setLoading(true)
        setError('')
        const response: UserEventsResponse = await fetchUserEvents(metricId, startDate, endDate)
        console.log('Fetched user events:', response)
        if (response.status === 'success' && Array.isArray(response.data)) {
          setData(response.data)
        } else {
          throw new Error('Invalid response format')
        }
      } catch (err: any) {
        console.error('Error fetching user events:', err)
        setError('Error fetching data: ' + (err.message || 'Unknown error'))
      } finally {
        setLoading(false)
      }
    }
    fetchData()
  }, [metricId, startDate, endDate])

  const handleDateChange = (isStartDate: boolean) => (e: React.ChangeEvent<HTMLInputElement>) => {
    const date = new Date(e.target.value)
    if (isStartDate) {
      setStartDate(date)
    } else {
      setEndDate(date)
    }
  }

  return (
    <div className="fixed inset-0 bg-black bg-opacity-50 flex justify-center items-center z-50">
      <div className="bg-white p-6 rounded-lg max-w-3xl w-full max-h-[80vh] overflow-auto">
        <button onClick={onClose} className="text-red-500 mb-4">Close</button>
        <div className="mb-4">
          <label className="mr-2">Start Date:</label>
          <input
            type="date"
            value={startDate.toISOString().split('T')[0]}
            onChange={handleDateChange(true)}
            className="border rounded p-1"
          />
          <label className="mx-2">End Date:</label>
          <input
            type="date"
            value={endDate.toISOString().split('T')[0]}
            onChange={handleDateChange(false)}
            className="border rounded p-1"
          />
        </div>
        {loading && <div>Loading data...</div>}
        {error && <div className="text-red-500">{error}</div>}
        {!loading && !error && data && (
          <div>
            <h2 className="text-xl font-bold mb-4">User Events for {metricId}</h2>
            {data.length === 0 ? (
              <p>No events found for this user in the selected date range.</p>
            ) : (
              <ul className="space-y-4">
                {data.map((event, index) => (
                  <li key={index} className="border-b pb-2">
                    <p><strong>Event Name:</strong> {event.event_name}</p>
                    <p><strong>Timestamp:</strong> {new Date(event.timestamp).toLocaleString()}</p>
                    {event.flow_id && <p><strong>Flow ID:</strong> {event.flow_id}</p>}
                    <details>
                      <summary className="cursor-pointer text-blue-500">Event Details</summary>
                      <pre className="text-sm bg-gray-100 p-2 mt-2 rounded">
                        {JSON.stringify(event, null, 2)}
                      </pre>
                    </details>
                  </li>
                ))}
              </ul>
            )}
          </div>
        )}
      </div>
    </div>
  )
}

export default UserDetailModal

================================================
File: analytics-dashboard/src/components/modal/UserEventsModal.tsx
================================================
import React, { useState, useEffect } from 'react';
import { fetchUserEvents, UserEvent, UserEventsResponse } from '../../services/metricService';

interface UserEventsModalProps {
  userId: string;
  onClose: () => void;
}

interface GroupedEvents {
  [flowId: string]: UserEvent[];
}

const UserEventsModal: React.FC<UserEventsModalProps> = ({ userId, onClose }) => {
  const [events, setEvents] = useState<UserEvent[]>([]);
  const [groupedEvents, setGroupedEvents] = useState<GroupedEvents>({});
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string>('');
  const [expandedFlows, setExpandedFlows] = useState<Set<string>>(new Set());
  const [expandedEvents, setExpandedEvents] = useState<Set<string>>(new Set());

  useEffect(() => {
    const loadUserEvents = async () => {
      try {
        setLoading(true);
        setError('');
        const response: UserEventsResponse = await fetchUserEvents(userId);
        if (response.status === 'success' && Array.isArray(response.data)) {
          setEvents(response.data);
          groupEventsByFlow(response.data);
        } else {
          throw new Error('Invalid response format from server');
        }
      } catch (err: any) {
        setError(err.message || 'Failed to load user events');
        console.error('Error loading user events:', err);
      } finally {
        setLoading(false);
      }
    };

    loadUserEvents();
  }, [userId]);

  const groupEventsByFlow = (events: UserEvent[]) => {
    const grouped = events.reduce((acc: GroupedEvents, event) => {
      const flowId = event.flow_id || 'No Flow';
      if (!acc[flowId]) {
        acc[flowId] = [];
      }
      acc[flowId].push(event);
      return acc;
    }, {});

    // Sort events within each flow by timestamp
    Object.keys(grouped).forEach(flowId => {
      grouped[flowId].sort((a, b) => 
        new Date(b.timestamp).getTime() - new Date(a.timestamp).getTime()
      );
    });

    setGroupedEvents(grouped);
  };

  const toggleFlow = (flowId: string) => {
    const newExpandedFlows = new Set(expandedFlows);
    if (expandedFlows.has(flowId)) {
      newExpandedFlows.delete(flowId);
    } else {
      newExpandedFlows.add(flowId);
    }
    setExpandedFlows(newExpandedFlows);
  };

  const toggleEvent = (eventId: string) => {
    const newExpandedEvents = new Set(expandedEvents);
    if (expandedEvents.has(eventId)) {
      newExpandedEvents.delete(eventId);
    } else {
      newExpandedEvents.add(eventId);
    }
    setExpandedEvents(newExpandedEvents);
  };

  const formatDate = (timestamp: string) => {
    return new Date(timestamp).toLocaleString();
  };

  return (
    <div className="fixed inset-0 bg-black bg-opacity-50 flex justify-center items-start overflow-y-auto py-8 z-50">
      <div className="bg-white rounded-lg shadow-xl max-w-4xl w-full mx-4 my-auto">
        <div className="p-6 border-b border-gray-200">
          <div className="flex justify-between items-center">
            <h2 className="text-2xl font-bold text-gray-900">
              User Activities
              <span className="ml-2 text-sm font-normal text-gray-500">
                {userId}
              </span>
            </h2>
            <button
              onClick={onClose}
              className="text-gray-400 hover:text-gray-500 transition-colors"
              aria-label="Close"
            >
              <svg className="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" />
              </svg>
            </button>
          </div>
        </div>

        <div className="p-6">
          {loading && (
            <div className="flex flex-col items-center justify-center py-12">
              <div className="animate-spin rounded-full h-12 w-12 border-b-2 border-blue-500"></div>
              <p className="mt-4 text-gray-500">Loading user activities...</p>
            </div>
          )}

          {error && (
            <div className="bg-red-50 border border-red-200 rounded-md p-4 mb-6">
              <div className="flex">
                <div className="flex-shrink-0">
                  <svg className="h-5 w-5 text-red-400" viewBox="0 0 20 20" fill="currentColor">
                    <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8.707 7.293a1 1 0 00-1.414 1.414L8.586 10l-1.293 1.293a1 1 0 101.414 1.414L10 11.414l1.293 1.293a1 1 0 001.414-1.414L11.414 10l1.293-1.293a1 1 0 00-1.414-1.414L10 8.586 8.707 7.293z" clipRule="evenodd" />
                  </svg>
                </div>
                <div className="ml-3">
                  <h3 className="text-sm font-medium text-red-800">Error loading activities</h3>
                  <p className="text-sm text-red-700 mt-1">{error}</p>
                </div>
              </div>
            </div>
          )}

          {!loading && !error && Object.entries(groupedEvents).length === 0 && (
            <div className="text-center py-12">
              <svg className="mx-auto h-12 w-12 text-gray-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
              </svg>
              <p className="mt-4 text-gray-500">No activities found for this user</p>
            </div>
          )}

          {!loading && !error && Object.entries(groupedEvents).map(([flowId, flowEvents]) => (
            <div key={flowId} className="mb-4 border rounded-lg overflow-hidden">
              <button
                className="w-full bg-gray-50 px-6 py-4 flex justify-between items-center hover:bg-gray-100 transition-colors"
                onClick={() => toggleFlow(flowId)}
              >
                <div className="flex items-center">
                  <h3 className="text-lg font-medium text-gray-900">
                    {flowId}
                  </h3>
                  <span className="ml-3 text-sm text-gray-500">
                    {flowEvents.length} {flowEvents.length === 1 ? 'event' : 'events'}
                  </span>
                </div>
                <svg
                  className={`h-5 w-5 text-gray-500 transform transition-transform ${
                    expandedFlows.has(flowId) ? 'rotate-180' : ''
                  }`}
                  viewBox="0 0 20 20"
                  fill="currentColor"
                >
                  <path fillRule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clipRule="evenodd" />
                </svg>
              </button>

              {expandedFlows.has(flowId) && (
                <div className="divide-y divide-gray-200">
                  {flowEvents.map((event) => (
                    <div key={event.id} className="px-6 py-4">
                      <button
                        className="w-full flex justify-between items-center text-left"
                        onClick={() => toggleEvent(event.id)}
                      >
                        <div>
                          <p className="text-sm font-medium text-gray-900">{event.event_name}</p>
                          <p className="text-sm text-gray-500">{formatDate(event.timestamp)}</p>
                        </div>
                        <svg
                          className={`h-5 w-5 text-gray-500 transform transition-transform ${
                            expandedEvents.has(event.id) ? 'rotate-180' : ''
                          }`}
                          viewBox="0 0 20 20"
                          fill="currentColor"
                        >
                          <path fillRule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clipRule="evenodd" />
                        </svg>
                      </button>

                      {expandedEvents.has(event.id) && (
                        <div className="mt-4 bg-gray-50 rounded-md p-4">
                          <pre className="text-sm text-gray-700 whitespace-pre-wrap break-words">
                            {JSON.stringify(event, null, 2)}
                          </pre>
                        </div>
                      )}
                    </div>
                  ))}
                </div>
              )}
            </div>
          ))}
        </div>
      </div>
    </div>
  );
};

export default UserEventsModal;

================================================
File: analytics-dashboard/src/components/modal/UserListModal.tsx
================================================
// analytics-dashboard/src/components/modal/UserListModal.tsx
import React, { useState, useEffect } from 'react';
import axios from 'axios';
import { fetchPowerUsers } from '../../services/metricService';

export interface User {
  userId?: string;
  email: string;
  trace_id?: string;
}

interface UserListModalProps {
  onClose: () => void;
  onSelectUser: (user: User) => void;
  mode?: 'regular' | 'power';
  gaugeType?: 'thread_users' | 'sketch_users' | 'render_users' | 'medium_chat_users' | 'active_chat_users';
  timeRange?: {
    start: Date;
    end: Date;
  };
}

const UserListModal: React.FC<UserListModalProps> = ({ 
  onClose, 
  onSelectUser, 
  mode = 'regular',
  gaugeType = 'thread_users',
  timeRange
}) => {
  const [users, setUsers] = useState<User[]>([]);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string>('');

  useEffect(() => {
    const fetchUsers = async () => {
      try {
        setLoading(true);
        setError('');
        const apiUrl = import.meta.env.VITE_API_URL || 'http://localhost:5001';

        if (mode === 'power' && timeRange) {
          // Fetch users from the gauge-users endpoint
          const response = await axios.get(`${apiUrl}/metrics/gauge-users`, {
            params: {
              startDate: timeRange.start.toISOString(),
              endDate: timeRange.end.toISOString(),
              gaugeType: gaugeType
            }
          });

          if (response.data.status === 'success') {
            setUsers(response.data.data);
          } else {
            setError('Failed to fetch user details');
          }
        } else {
          // Regular user fetch from Descope
          const response = await axios.get(`${apiUrl}/getDescopeUsers`);
          const descopeUsers = response.data.users || [];
          setUsers(descopeUsers.map((user: any) => ({
            userId: user.userId,
            email: user.email
          })));
        }
      } catch (err: any) {
        setError(mode === 'power' ? 'Error fetching power users' : 'Error fetching users');
        console.error('Error fetching users:', err);
      } finally {
        setLoading(false);
      }
    };
    fetchUsers();
  }, [mode, timeRange, gaugeType]);

  const getModalTitle = () => {
    if (mode === 'regular') return 'All Users';
    
    switch (gaugeType) {
      case 'thread_users':
        return 'Thread Users';
      case 'sketch_users':
        return 'Sketch Users';
      case 'render_users':
        return 'Render Users';
      case 'medium_chat_users':
        return 'Medium Chat Users';
      case 'active_chat_users':
        return 'Power Users';
      default:
        return 'Users';
    }
  };

  return (
    <div className="fixed inset-0 bg-black bg-opacity-50 flex justify-center items-center z-50 p-4">
      <div className="bg-white p-6 rounded-lg w-full max-w-lg">
        <div className="flex justify-between items-center mb-4">
          <h2 className="text-xl font-semibold">
            {getModalTitle()}
          </h2>
          <button 
            onClick={onClose}
            className="text-gray-500 hover:text-gray-700"
          >
            Close
          </button>
        </div>
        
        {loading && (
          <div className="flex justify-center items-center py-4">
            <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
          </div>
        )}
        
        {error && (
          <div className="text-red-500 p-4 text-center bg-red-50 rounded-md">
            {error}
          </div>
        )}
        
        {!loading && !error && users.length === 0 && (
          <div className="text-gray-500 text-center py-4">
            {mode === 'power' ? `No ${getModalTitle().toLowerCase()} found` : 'No users found'}
          </div>
        )}
        
        {users.length > 0 && (
          <div className="max-h-96 overflow-y-auto">
            <ul className="divide-y divide-gray-200">
              {users.map((user, index) => (
                <li
                  key={user.trace_id || user.userId || index}
                  className="py-3 hover:bg-gray-50 cursor-pointer transition-colors duration-150 ease-in-out"
                  onClick={() => onSelectUser(user)}
                >
                  <div className="flex items-center px-2">
                    <span className="mr-3 text-gray-500 w-6 text-right">
                      {index + 1}.
                    </span>
                    <div className="flex-1">
                      <div>
                        <span className="text-blue-600 hover:text-blue-800">
                          {user.email}
                        </span>
                      </div>
                      {user.trace_id && (
                        <div className="text-sm text-gray-500">
                          ID: {user.trace_id}
                        </div>
                      )}
                    </div>
                  </div>
                </li>
              ))}
            </ul>
          </div>
        )}
      </div>
    </div>
  );
};

export default UserListModal;

================================================
File: analytics-dashboard/src/components/users/UserTable.tsx
================================================
import React, { useState, useEffect } from 'react';
import { fetchUserStats } from '../../services/metricService';
import UserEventsModal from '../modal/UserEventsModal';

export interface UserStats {
  email: string;
  trace_id: string;
  messageCount: number;
  sketchCount: number;
  renderCount: number;
}

interface UserTableProps {
  gaugeType?: 'active_users' | 'power_users' | 'moderate_users' | 'producers' | 'producers_attempting';
  timeRange?: {
    start: Date;
    end: Date;
  };
}

const UserTable: React.FC<UserTableProps> = ({ gaugeType = 'active_users', timeRange }) => {
  const [users, setUsers] = useState<UserStats[]>([]);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string>('');
  const [selectedUserId, setSelectedUserId] = useState<string | null>(null);
  const [sortConfig, setSortConfig] = useState<{
    key: keyof UserStats;
    direction: 'ascending' | 'descending';
  }>({ key: 'messageCount', direction: 'descending' });

  useEffect(() => {
    const fetchData = async () => {
      if (!timeRange) return;

      try {
        setLoading(true);
        setError('');
        const data = await fetchUserStats(timeRange.start, timeRange.end, gaugeType);
        setUsers(data);
      } catch (err) {
        setError('Error fetching user statistics');
      } finally {
        setLoading(false);
      }
    };

    fetchData();
  }, [gaugeType, timeRange]);

  const handleSort = (key: keyof UserStats) => {
    setSortConfig(current => ({
      key,
      direction: current.key === key && current.direction === 'ascending' 
        ? 'descending' 
        : 'ascending'
    }));
  };

  const handleUserClick = (userId: string) => {
    setSelectedUserId(userId);
  };

  const handleCloseModal = () => {
    setSelectedUserId(null);
  };

  const sortedUsers = [...users].sort((a, b) => {
    if (sortConfig.direction === 'ascending') {
      return a[sortConfig.key] > b[sortConfig.key] ? 1 : -1;
    }
    return a[sortConfig.key] < b[sortConfig.key] ? 1 : -1;
  });

  const getSortIcon = (key: keyof UserStats) => {
    if (sortConfig.key !== key) return '↕️';
    return sortConfig.direction === 'ascending' ? '↑' : '↓';
  };

  const getTableTitle = () => {
    switch (gaugeType) {
      case 'active_users':
        return 'Active Users';
      case 'power_users':
        return 'Power Users';
      case 'moderate_users':
        return 'Moderate Users';
      case 'producers':
        return 'Producers';
      case 'producers_attempting':
        return 'Producers Attempting';
      default:
        return 'Users';
    }
  };

  const columns = [
    {
      key: 'email' as keyof UserStats,
      label: 'Email',
      sortable: true
    },
    {
      key: 'messageCount' as keyof UserStats,
      label: 'Messages',
      sortable: true
    },
    {
      key: 'sketchCount' as keyof UserStats,
      label: 'Sketches',
      sortable: true
    },
    {
      key: 'renderCount' as keyof UserStats,
      label: 'Renders',
      sortable: true
    },
    {
      key: 'trace_id' as keyof UserStats,
      label: 'ID',
      sortable: false
    }
  ];

  return (
    <div className="mt-8 bg-white rounded-lg shadow">
      <div className="px-6 py-4 border-b border-gray-200">
        <h2 className="text-xl font-semibold text-gray-800">{getTableTitle()} Statistics</h2>
      </div>
      
      {loading && (
        <div className="flex justify-center items-center py-8">
          <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-gray-900"></div>
        </div>
      )}
      
      {error && (
        <div className="text-red-500 p-4 text-center bg-red-50">
          {error}
        </div>
      )}
      
      {!loading && !error && users.length === 0 && (
        <div className="text-gray-500 text-center py-8">
          No user statistics available
        </div>
      )}
      
      {users.length > 0 && (
        <div className="overflow-x-auto">
          <table className="min-w-full divide-y divide-gray-200">
            <thead className="bg-gray-50">
              <tr>
                {columns.map(column => (
                  <th 
                    key={column.key}
                    className={`px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider ${
                      column.sortable ? 'cursor-pointer hover:bg-gray-100' : ''
                    }`}
                    onClick={() => column.sortable && handleSort(column.key)}
                  >
                    {column.label} {column.sortable && getSortIcon(column.key)}
                  </th>
                ))}
              </tr>
            </thead>
            <tbody className="bg-white divide-y divide-gray-200">
              {sortedUsers.map((user, index) => (
                <tr 
                  key={user.trace_id || index}
                  className="hover:bg-gray-50 transition-colors duration-150 ease-in-out cursor-pointer"
                  onClick={() => handleUserClick(user.trace_id)}
                >
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-blue-600 hover:text-blue-800">
                      {user.email}
                    </div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-gray-900">{user.messageCount}</div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-gray-900">{user.sketchCount}</div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-gray-900">{user.renderCount}</div>
                  </td>
                  <td className="px-6 py-4 whitespace-nowrap">
                    <div className="text-sm text-gray-500">{user.trace_id}</div>
                  </td>
                </tr>
              ))}
            </tbody>
          </table>
        </div>
      )}

      {selectedUserId && (
        <UserEventsModal
          userId={selectedUserId}
          onClose={handleCloseModal}
        />
      )}
    </div>
  );
};

export default UserTable;

================================================
File: analytics-dashboard/src/services/metricService.ts
================================================
import axios from 'axios';
import { MetricResponse, Metric, UserStats } from '../types/metrics';

const API_URL = import.meta.env.VITE_API_URL || 'http://localhost:5001';

export const fetchMetrics = async (
  startDate: Date,
  endDate: Date,
  includeV1: boolean = true
): Promise<MetricResponse> => {
  try {
    const response = await axios.get(`${API_URL}/metrics`, {
      params: {
        startDate: startDate.toISOString(),
        endDate: endDate.toISOString(),
        includeV1
      }
    });

    if (!response.data || !response.data.metrics) {
      throw new Error('Invalid response format from backend');
    }

    const metrics: Metric[] = response.data.metrics.map((metric: any) => ({
      id: metric.id,
      name: metric.name,
      description: metric.description,
      category: metric.category,
      interval: metric.interval,
      data: {
        value: metric.data.value !== undefined ? Number(metric.data.value) : undefined,
        previousValue: metric.data.previousValue !== undefined ? Number(metric.data.previousValue) : undefined,
        trend: metric.data.trend || 'neutral',
        changePercentage: metric.data.changePercentage,
        v1Value: metric.data.v1Value !== undefined ? Number(metric.data.v1Value) : undefined
      }
    }));

    return {
      metrics,
      timeRange: {
        start: new Date(response.data.timeRange.start),
        end: new Date(response.data.timeRange.end)
      }
    };

  } catch (error) {
    console.error('Error fetching metrics:', error);
    const defaultMetrics: Metric[] = [
      'descope_users',
      'thread_users',
      'render_users',
      'active_chat_users',
      'medium_chat_users',
      'sketch_users'
    ].map(id => ({
      id,
      name: id.split('_').map(word => word.charAt(0).toUpperCase() + word.slice(1)).join(' '),
      description: 'Error loading data',
      category: id.includes('chat') || id === 'thread_users' ? 'engagement' : 'user',
      interval: 'daily',
      data: {
        value: 0,
        trend: 'neutral',
        changePercentage: 0
      }
    }));

    return {
      metrics: defaultMetrics,
      timeRange: { start: startDate, end: endDate }
    };
  }
};

export const fetchUserStats = async (
  startDate: Date,
  endDate: Date,
  gaugeType: string
): Promise<UserStats[]> => {
  console.log(`Fetching user stats for gauge type: ${gaugeType}, start date: ${startDate}, end date: ${endDate}`);
  try {
    const response = await axios.get<{ status: string; data: UserStats[] }>(`${API_URL}/metrics/user-stats`, {
      params: {
        startDate: startDate.toISOString(),
        endDate: endDate.toISOString(),
        gaugeType
      }
    });

    console.log('User stats API response:', response.data);

    if (response.data.status === 'success' && Array.isArray(response.data.data)) {
      console.log(`Successfully fetched ${response.data.data.length} user stats`);
      return response.data.data;
    }
    console.warn('Unexpected response format from user stats API:', response.data);
    return [];
  } catch (error) {
    if (axios.isAxiosError(error)) {
      console.error('Axios error fetching user statistics:', error.response?.data || error.message);
    } else {
      console.error('Error fetching user statistics:', error);
    }
    return [];
  }
};

export interface UserEvent {
  event_name: string;
  timestamp: string;
  trace_id: string;
  flow_id?: string;
  [key: string]: any;
}

export interface UserEventsResponse {
  status: string;
  data: UserEvent[];
  timeRange: {
    start: string;
    end: string;
  };
}

export const fetchUserEvents = async (
  userId: string,
  startDate: Date = new Date(Date.now() - 30 * 24 * 60 * 60 * 1000),
  endDate: Date = new Date()
): Promise<UserEventsResponse> => {
  try {
    const response = await axios.get<UserEventsResponse>(`${API_URL}/metrics/user-events`, {
      params: { 
        traceId: userId,
        startDate: startDate.toISOString(),
        endDate: endDate.toISOString()
      }
    });

    if (response.data.status !== 'success' || !Array.isArray(response.data.data)) {
      throw new Error('Invalid response format from backend');
    }

    return response.data;
  } catch (error) {
    console.error('Error fetching user events:', error);
    throw error;
  }
};

export const setMetricTarget = async (metricId: string, target: number): Promise<void> => {
  try {
    const response = await fetch(`${API_URL}/metrics/${metricId}/target`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ target }),
    });

    if (!response.ok) {
      throw new Error('Failed to set metric target');
    }
  } catch (error) {
    console.error('Error setting metric target:', error);
    throw error;
  }
};

================================================
File: analytics-dashboard/src/types/metrics.ts
================================================
export interface MetricData {
  value: number;
  previousValue?: number;
  trend?: 'up' | 'down' | 'neutral';
  changePercentage?: number;
  target?: number;
  historical?: Array<{
    date: string;
    value: number;
  }>;
}

export interface Metric {
  id: string;
  name: string;
  description: string;
  category: string;
  interval: string;
  data: MetricData;
}

export interface MetricResponse {
  metrics: Metric[];
  timeRange: {
    start: Date;
    end: Date;
  };
}

export interface UserStats {
  email: string;
  trace_id: string;
  messageCount: number;
  sketchCount: number;
  renderCount: number;
}

================================================
File: dashboardbackend/README.md
================================================
# Analytics Dashboard Backend

This is the backend service for the Analytics Dashboard, providing APIs for fetching and analyzing user activity and metrics.

## Project Structure

The project is organized as follows:

```
dashboardbackend/
├── src/
│   ├── services/
│   │   ├── analytics_service.py
│   │   ├── metrics_service.py
│   │   ├── historical_data_service.py
│   │   ├── descope_service.py
│   │   ├── caching_service.py
│   │   └── opensearch_service.py
│   ├── utils/
│   │   └── query_builder.py
│   ├── api/
│   │   └── metrics.py
│   └── core/
│       └── __init__.py
├── tests/
│   └── test_analytics_service.py
├── requirements.txt
├── run.py
└── README.md
```

## Services

- `analytics_service.py`: Main service that orchestrates data fetching and processing.
- `metrics_service.py`: Handles all metrics-related operations (user, content, producer metrics). This service now includes the AnalyticsMetricsService, which consolidates functionality previously spread across multiple files.
- `historical_data_service.py`: Provides historical data for V1 metrics.
- `descope_service.py`: Handles interactions with the Descope API for user management and authentication.
- `caching_service.py`: Manages caching of data to improve performance. This service now accepts a Redis client for better flexibility and testability.
- `opensearch_service.py`: Handles interactions with OpenSearch for data querying and aggregation.

## Recent Changes

- Consolidated metrics-related functionality into the AnalyticsMetricsService within metrics_service.py.
- Updated the CachingService to accept a Redis client, improving its flexibility and testability.
- Streamlined the project structure by removing redundant files and consolidating related functionality.

## Setup and Running

1. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

2. Set up environment variables (see `.env.example` for required variables).

3. Run the server:
   ```
   python run.py
   ```

The server will start running on `http://0.0.0.0:5001` by default.

## Testing

To run the tests:

```
pytest
```

## API Endpoints

- `/metrics`: Get dashboard metrics
- `/metrics/user-stats`: Get user statistics
- `/metrics/user-events`: Get user events

For detailed API documentation, please refer to the API specification document.

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License.

================================================
File: dashboardbackend/requirements.txt
================================================
aiohttp==3.9.3
certifi==2022.12.7
opensearch-py==2.3.0
APScheduler==3.10.1
SQLAlchemy==1.4.47
python-dotenv==1.0.0
tenacity==8.2.3

================================================
File: dashboardbackend/run.py
================================================
"""
Run the application server
"""
import asyncio
import logging
import os
from hypercorn.config import Config
from hypercorn.asyncio import serve
from dotenv import load_dotenv
from src.app import init_app

import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# Initialize application
app = None

async def init():
    global app
    try:
        # Load environment variables
        load_dotenv()
        
        # Initialize application
        app = await init_app()
        return app
    except Exception as e:
        logger.error(f"Failed to initialize app: {str(e)}")
        raise

async def main():
    """Main entry point"""
    try:
        app = await init()
        
        # Configure Hypercorn
        config = Config()
        config.bind = [f"0.0.0.0:{os.getenv('PORT', '5001')}"]
        config.use_reloader = True
        
        # Start server
        await serve(app, config)
        
    except Exception as e:
        logger.error(f"Failed to start server: {str(e)}")
        raise

# Initialize app for Flask CLI
asyncio.run(init())

if __name__ == "__main__":
    asyncio.run(main())

================================================
File: dashboardbackend/.env.example
================================================
# Redis Configuration
REDIS_URL=redis://localhost:6379

OPENSEARCH_URL="https://localhost:9200"
OPENSEARCH_USERNAME="elkadmin"
OPENSEARCH_PASSWORD="password"
MAX_QUERY_TIME="30"
PORT="5001"

# Descope API Configuration
DESCOPE_API_URL="https://api.descope.com/v1/mgmt/user/search"
DESCOPE_BEARER_TOKEN="your_bearer_token"

# Google Sheets Configuration
GOOGLE_SHEET_ID="your_sheet_id"


================================================
File: dashboardbackend/scheduler/daily_snapshot.py
================================================
import os
import asyncio
import ssl
import certifi
import aiohttp
import logging
from datetime import datetime
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from sqlalchemy import create_engine, Column, Integer, DateTime, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# SQLAlchemy setup (using SQLite for demo; replace with your DB if needed)
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///daily_metrics.db")
engine = create_engine(DATABASE_URL, echo=False)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

class DailyMetric(Base):
    __tablename__ = "daily_metrics"
    id = Column(Integer, primary_key=True, index=True)
    date = Column(DateTime, unique=True, index=True)
    descope_total = Column(Integer, nullable=False)
    opensearch_total = Column(Integer, nullable=True)  # Optional if you want to store more metrics
    note = Column(String, nullable=True)

Base.metadata.create_all(bind=engine)

# Functions to fetch metrics

async def fetch_descope_total():
    """
    Fetch the cumulative user count from Descope.
    This uses the Descope endpoint without date filtering (since that’s how it behaves).
    """
    descope_url = os.getenv("DESCOPE_API_URL", "https://api.descope.com/v1/mgmt/user/search")
    bearer_token = os.getenv("DESCOPE_BEARER_TOKEN", "")
    headers = {
        "Authorization": f"Bearer {bearer_token}",
        "Content-Type": "application/json"
    }
    # Since the API doesn't filter by date, we just get the overall count.
    payload = {
        "tenantIds": [],
        "text": "",
        "roleNames": [],
        "loginIds": [],
        "ssoAppIds": [],
        "customAttributes": {}
    }
    ssl_context = ssl.create_default_context(cafile=certifi.where())
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=ssl_context)) as session:
        async with session.post(descope_url, headers=headers, json=payload) as response:
            if response.status == 200:
                data = await response.json()
                users = data.get("users", [])
                total = len(users)
                logger.debug("Fetched Descope total: %s", total)
                return total
            else:
                error_text = await response.text()
                logger.error("Descope API error: %s - %s", response.status, error_text)
                return 0

async def fetch_opensearch_total():
    """
    Fetch a cumulative count from OpenSearch.
    This could be an aggregation query over your index.
    Adjust the query as needed.
    """
    from opensearchpy import AsyncOpenSearch
    opensearch_url = os.getenv("OPENSEARCH_URL", "https://localhost:9200")
    opensearch_username = os.getenv("OPENSEARCH_USERNAME", "elkadmin")
    opensearch_password = os.getenv("OPENSEARCH_PASSWORD", "")
    ssl_context = ssl.create_default_context(cafile=certifi.where())
    client = AsyncOpenSearch(
        hosts=[opensearch_url],
        http_auth=(opensearch_username, opensearch_password),
        verify_certs=False
    )
    # Query that counts all documents in the index.
    query = {"query": {"match_all": {}}}
    try:
        response = await client.search(index="events-v2", body=query, size=0)
        total = response.get("hits", {}).get("total", {}).get("value", 0)
        logger.debug("Fetched OpenSearch total: %s", total)
        await client.close()
        return total
    except Exception as e:
        logger.error("Error fetching OpenSearch total: %s", str(e), exc_info=True)
        await client.close()
        return 0

async def store_daily_snapshot():
    """
    Fetch metrics from Descope and OpenSearch and store them in the database.
    This function should be scheduled to run at midnight every day.
    """
    snapshot_date = datetime.now()
    logger.info("Storing daily snapshot for date: %s", snapshot_date.isoformat())
    
    descope_total = await fetch_descope_total()
    opensearch_total = await fetch_opensearch_total()
    
    # Save to the database
    db = SessionLocal()
    try:
        # Check if a snapshot for today already exists
        existing = db.query(DailyMetric).filter(DailyMetric.date == snapshot_date.date()).first()
        if existing:
            logger.info("Snapshot for today already exists, updating it.")
            existing.descope_total = descope_total
            existing.opensearch_total = opensearch_total
        else:
            new_snapshot = DailyMetric(
                date=snapshot_date.date(),
                descope_total=descope_total,
                opensearch_total=opensearch_total,
                note="Daily snapshot at midnight"
            )
            db.add(new_snapshot)
            logger.info("New snapshot created.")
        db.commit()
    except Exception as e:
        db.rollback()
        logger.error("Error storing snapshot: %s", str(e), exc_info=True)
    finally:
        db.close()

async def main():
    scheduler = AsyncIOScheduler()
    # Schedule the store_daily_snapshot() to run at midnight every day.
    scheduler.add_job(store_daily_snapshot, 'cron', hour=0, minute=0)
    scheduler.start()
    
    logger.info("Scheduler started. Daily snapshot job scheduled at midnight.")
    # Keep the script running
    while True:
        await asyncio.sleep(3600)

if __name__ == "__main__":
    asyncio.run(main())

================================================
File: dashboardbackend/scripts/generate_daily_metrics.py
================================================
import json
from datetime import datetime, timedelta
import os

def interpolate(start_val, end_val, num_points):
    """Linear interpolation between two values"""
    if num_points <= 1:
        return []
    step = (end_val - start_val) / (num_points - 1)
    return [int(start_val + step * i) for i in range(num_points)]

def generate_daily_metrics():
    # Known data points
    known_points = {
        "2024-10-01": {"total_users": 0, "active_users": 0, "producers": 0},
        "2024-10-31": {"total_users": 9770, "active_users": 1213, "producers": 551},
        "2024-11-30": {"total_users": 18634, "active_users": 4231, "producers": 1923},
        "2024-12-31": {"total_users": 48850, "active_users": 9863, "producers": 4483},
        "2025-01-26": {"total_users": 55000, "active_users": 16560, "producers": 7527}
    }

    # Convert dates to datetime objects
    date_points = sorted([(datetime.strptime(d, "%Y-%m-%d"), metrics) 
                         for d, metrics in known_points.items()])

    daily_metrics = {}
    
    # Interpolate between each pair of known points
    for i in range(len(date_points) - 1):
        start_date, start_metrics = date_points[i]
        end_date, end_metrics = date_points[i + 1]
        
        # Calculate number of days between points
        num_days = (end_date - start_date).days + 1
        
        # Generate dates
        dates = [start_date + timedelta(days=x) for x in range(num_days)]
        
        # Interpolate each metric
        for metric in ['total_users', 'active_users', 'producers']:
            values = interpolate(start_metrics[metric], end_metrics[metric], num_days)
            
            # Add to daily metrics
            for date, value in zip(dates, values):
                date_str = date.strftime("%Y-%m-%d")
                if date_str not in daily_metrics:
                    daily_metrics[date_str] = {}
                daily_metrics[date_str][metric] = value

    # Add the final day
    final_date, final_metrics = date_points[-1]
    daily_metrics[final_date.strftime("%Y-%m-%d")] = final_metrics

    # Create the full JSON structure
    json_data = {
        "metadata": {
            "start_date": "2024-10-01",
            "end_date": "2025-01-26",
            "last_updated": "2025-01-26"
        },
        "daily_metrics": daily_metrics
    }

    # Write to file
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(script_dir, "..", "src", "data", "historical_metrics.json")
    
    with open(output_path, 'w') as f:
        json.dump(json_data, f, indent=4)

if __name__ == "__main__":
    generate_daily_metrics()


================================================
File: dashboardbackend/src/__init__.py
================================================
# Save as src/__init__.py
"""
Main src package
"""

================================================
File: dashboardbackend/src/app.py
================================================
"""
Main application factory
"""
import logging
import os
from quart import Quart, request
from quart_cors import cors
from dotenv import load_dotenv
from src.core import init_services
from datetime import datetime
from dateutil import parser
import pytz

def configure_logging():
    """Configure logging for the application"""
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

logger = logging.getLogger(__name__)

def create_app() -> Quart:
    """Create and configure the Quart application"""
    try:
        logger.info("Registering application blueprints...")
        
        # Load environment variables
        load_dotenv()
        
        # Configure logging
        configure_logging()
        
        # Create Quart app
        app = Quart(__name__)
        
        # Enable CORS
        app = cors(app, allow_origin="*")
        
        # Register blueprints
        from src.api.metrics import metrics_bp
        from src.api.health import health_bp
        
        app.register_blueprint(metrics_bp, url_prefix='/metrics')
        app.register_blueprint(health_bp, url_prefix='/health')
        
        logger.info("Blueprints registered successfully")
        return app
        
    except Exception as e:
        logger.error(f"Failed to register blueprints: {str(e)}")
        raise

async def init_app() -> Quart:
    """Initialize the application"""
    app = create_app()
    
    # Initialize services
    await init_services(app)
    
    return app

================================================
File: dashboardbackend/src/dub_app.py
================================================
"""
Main application factory for duplicated analytics dashboard
"""
import logging
import os
from quart import Quart
from quart_cors import cors
from dotenv import load_dotenv
from src.services.dub_analytics import AnalyticsService
from src.core import redis_client, opensearch_client

logger = logging.getLogger(__name__)

def create_app() -> Quart:
    """Create and configure the Quart application"""
    try:
        logger.info("Creating application...")
        
        # Load environment variables
        load_dotenv()
        
        # Create Quart app
        app = Quart(__name__)
        
        # Enable CORS
        app = cors(app, allow_origin="*")
        
        # Initialize analytics service
        logger.info("Initializing analytics service...")
        app.analytics_service = AnalyticsService(opensearch_client, redis_client)
        logger.info("Analytics service initialized")
        
        # Register blueprints
        logger.info("Registering blueprints...")
        from src.api.dub_metrics import metrics_bp
        
        # Register blueprints with explicit URL prefixes
        app.register_blueprint(metrics_bp, url_prefix='/dub_metrics')
        
        logger.info("Application created successfully")
        return app
        
    except Exception as e:
        logger.error(f"Failed to create application: {str(e)}", exc_info=True)
        raise

async def init_app() -> Quart:
    """Initialize the application"""
    app = create_app()
    return app

================================================
File: dashboardbackend/src/api/__init__.py
================================================
"""
API package for route handlers
"""

================================================
File: dashboardbackend/src/api/descope.py
================================================
# dashboardbackend/src/api/descope.py
from quart import Blueprint, jsonify
import os
import logging
import aiohttp
import ssl

logger = logging.getLogger(__name__)
descope_bp = Blueprint('descope', __name__)

@descope_bp.route('/getDescopeUsers', methods=['GET'])
async def get_descope_users():
    """
    Retrieves users from the Descope API.
    Expects DESCOPE_API_URL and DESCOPE_BEARER_TOKEN to be set in your environment.
    """
    descope_url = os.getenv('DESCOPE_API_URL')
    bearer_token = os.getenv('DESCOPE_BEARER_TOKEN')
    
    if not descope_url or not bearer_token:
        logger.error("Descope configuration missing.")
        return jsonify({"error": "Descope configuration missing"}), 500

    headers = {
        'Authorization': f'Bearer {bearer_token}',
        'Content-Type': 'application/json'
    }
    payload = {
        "tenantIds": [],
        "text": "",
        "roleNames": [],
        "loginIds": [],
        "ssoAppIds": [],
        "customAttributes": {}
    }

    # For development only: create an SSL context that disables certificate verification
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE

    try:
        async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=ssl_context)) as session:
            async with session.post(descope_url, headers=headers, json=payload) as response:
                if response.status == 200:
                    data = await response.json()
                    users = data.get('users', [])
                    return jsonify({"users": users})
                else:
                    error_text = await response.text()
                    logger.error(f"Descope API error: {response.status}: {error_text}")
                    return jsonify({"error": f"Descope API error: {response.status}", "details": error_text}), response.status
    except Exception as e:
        logger.error(f"Error fetching Descope users: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

def init_app(app):
    """Register the Descope blueprint with the Quart app."""
    app.register_blueprint(descope_bp)

================================================
File: dashboardbackend/src/api/dub_metrics.py
================================================
"""
Analytics metrics API endpoints for user activity analysis
"""
from datetime import datetime, timezone, timedelta
from quart import Blueprint, jsonify, request, current_app
from quart_cors import cors

dub_metrics_bp = Blueprint('dub_metrics', __name__)
dub_metrics_bp = cors(dub_metrics_bp, allow_origin="*")

@dub_metrics_bp.route('/activity-users', methods=['POST'])
async def get_activity_users():
    """Get users based on their activity patterns"""
    try:
        data = await request.get_json()
        start_date = datetime.fromisoformat(data.get('startDate').replace('Z', '+00:00'))
        end_date = datetime.fromisoformat(data.get('endDate').replace('Z', '+00:00'))
        filter_type = data.get('filterType')

        # Ensure dates are in UTC
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=timezone.utc)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=timezone.utc)

        current_app.logger.info(f"Fetching activity users with params: start={start_date}, end={end_date}, filter={filter_type}")

        # Build OpenSearch query based on filter type
        must_conditions = [
            {"range": {"timestamp": {
                "gte": int(start_date.timestamp() * 1000),
                "lte": int(end_date.timestamp() * 1000)
            }}}
        ]

        # Add filter for successful events only
        must_conditions.append({"term": {"status.keyword": "succeeded"}})

        # Build aggregation for user activity
        aggs = {
            "users": {
                "terms": {
                    "field": "trace_id.keyword",
                    "size": 10000
                },
                "aggs": {
                    "actions": {
                        "date_histogram": {
                            "field": "timestamp",
                            "calendar_interval": "day"
                        }
                    },
                    "first_action": {"min": {"field": "timestamp"}},
                    "last_action": {"max": {"field": "timestamp"}},
                    "user_email": {
                        "terms": {
                            "field": "email.keyword",
                            "size": 1
                        }
                    }
                }
            }
        }

        query = {
            "query": {"bool": {"must": must_conditions}},
            "aggs": aggs,
            "size": 0
        }

        # Execute query
        result = await current_app.analytics_service.opensearch.search(
            index=current_app.analytics_service.index,
            body=query
        )

        # Process results
        users = []
        for bucket in result["aggregations"]["users"]["buckets"]:
            user_id = bucket["key"]
            email = bucket["user_email"]["buckets"][0]["key"] if bucket["user_email"]["buckets"] else "Unknown"
            first_action = bucket["first_action"]["value"]
            last_action = bucket["last_action"]["value"]
            total_actions = bucket["doc_count"]
            days_between = (last_action - first_action) / (1000 * 60 * 60 * 24)  # Convert ms to days

            # Filter users based on activity pattern
            include_user = False
            if filter_type == "consecutive_days":
                # Check if user has actions on consecutive days
                action_dates = set(
                    datetime.fromtimestamp(hit["key"] / 1000, tz=timezone.utc).date()
                    for hit in bucket["actions"]["buckets"]
                )
                consecutive_days = any(
                    date + timedelta(days=1) in action_dates
                    for date in action_dates
                )
                include_user = consecutive_days
            elif filter_type == "one_to_two_weeks":
                include_user = 7 <= days_between <= 14
            elif filter_type == "two_to_three_weeks":
                include_user = 14 < days_between <= 21
            elif filter_type == "month_apart":
                include_user = days_between >= 28

            if include_user and total_actions >= 2:
                users.append({
                    "trace_id": user_id,
                    "email": email,
                    "firstAction": datetime.fromtimestamp(first_action / 1000, tz=timezone.utc).isoformat(),
                    "lastAction": datetime.fromtimestamp(last_action / 1000, tz=timezone.utc).isoformat(),
                    "daysBetween": round(days_between, 1),
                    "totalActions": total_actions
                })

        # Sort users by total actions descending
        users.sort(key=lambda x: x["totalActions"], reverse=True)

        return jsonify({
            "status": "success",
            "users": users,
            "timeRange": {
                "start": start_date.isoformat(),
                "end": end_date.isoformat()
            }
        })

    except Exception as e:
        current_app.logger.error(f"Error in get_activity_users: {str(e)}", exc_info=True)
        return jsonify({
            "status": "error",
            "error": str(e)
        }), 500

def init_app(app):
    """Initialize metrics blueprint with the app"""
    app.register_blueprint(dub_metrics_bp, url_prefix='/metrics')

================================================
File: dashboardbackend/src/api/health.py
================================================
"""
Health check endpoints
"""
from quart import Blueprint, jsonify
from src.core import redis_client, opensearch_client

health_bp = Blueprint('health', __name__)

def init_app(app):
    """Initialize health blueprint"""
    app.register_blueprint(health_bp, url_prefix='/health')

@health_bp.route('/', methods=['GET'])
async def health_check():
    """Basic health check endpoint"""
    try:
        # Check Redis connection
        redis_status = "healthy"
        try:
            await redis_client.ping()
        except Exception as e:
            redis_status = f"unhealthy: {str(e)}"

        # Check OpenSearch connection
        opensearch_status = "healthy"
        try:
            await opensearch_client.ping()
        except Exception as e:
            opensearch_status = f"unhealthy: {str(e)}"

        return jsonify({
            'status': 'ok',
            'services': {
                'redis': redis_status,
                'opensearch': opensearch_status
            }
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500

================================================
File: dashboardbackend/src/api/metrics.py
================================================
"""
Metrics API endpoints
"""
from datetime import datetime, timezone
from typing import Optional
from quart import Blueprint, jsonify, request, current_app
from src.services.analytics_service import AnalyticsService
import json
from pathlib import Path
from werkzeug.exceptions import HTTPException

metrics_bp = Blueprint('metrics', __name__)

def init_app(app):
    """Initialize metrics blueprint"""
    app.register_blueprint(metrics_bp, url_prefix='/metrics')

@metrics_bp.route('/', methods=['GET'])
async def get_metrics():
    """Get metrics for the dashboard"""
    try:
        # Get query parameters with defaults
        start_date_str = request.args.get('startDate')
        end_date_str = request.args.get('endDate')
        include_v1 = request.args.get('includeV1', 'false').lower() == 'true'

        # If no dates provided, default to current month
        if not start_date_str or not end_date_str:
            now = datetime.now(timezone.utc)
            start_date = datetime(now.year, now.month, 1, tzinfo=timezone.utc)
            end_date = now
            current_app.logger.info(f"Using default date range: {start_date} to {end_date}")
        else:
            # Parse dates from query parameters
            try:
                start_date = datetime.strptime(start_date_str, "%Y-%m-%dT%H:%M:%S.%fZ").replace(tzinfo=timezone.utc)
            except ValueError:
                start_date = datetime.strptime(start_date_str, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)

            try:
                end_date = datetime.strptime(end_date_str, "%Y-%m-%dT%H:%M:%S.%fZ").replace(tzinfo=timezone.utc)
            except ValueError:
                end_date = datetime.strptime(end_date_str, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)

        # Get metrics from analytics service
        metrics = await current_app.analytics_service.get_dashboard_metrics(start_date, end_date, include_v1)

        # Add time range to response
        response = {
            "metrics": metrics,
            "timeRange": {
                "start": start_date.strftime("%Y-%m-%dT%H:%M:%SZ"),
                "end": end_date.strftime("%Y-%m-%dT%H:%M:%SZ")
            }
        }
        return jsonify(response)

    except Exception as e:
        current_app.logger.error(f"Error in get_metrics: {str(e)}", exc_info=True)
        return jsonify({
            'error': str(e)
        }), 500

@metrics_bp.route('/user-stats', methods=['GET'])
async def get_user_stats():
    """Get user statistics"""
    try:
        # Parse parameters
        start_date = request.args.get('startDate')
        end_date = request.args.get('endDate')
        gauge_type = request.args.get('gaugeType')

        current_app.logger.info(f"Fetching user stats with params: start={start_date}, end={end_date}, gauge_type={gauge_type}")

        if not all([start_date, end_date, gauge_type]):
            current_app.logger.error("Missing required parameters")
            return jsonify({
                'error': 'Missing required parameters'
            }), 400

        try:
            # Convert to datetime objects
            start_date = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            end_date = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
        except ValueError as e:
            current_app.logger.error(f"Invalid date format: {str(e)}")
            return jsonify({
                'error': 'Invalid date format'
            }), 400

        # Get user statistics
        user_stats = await current_app.analytics_service.get_user_statistics(
            start_date=start_date,
            end_date=end_date,
            gauge_type=gauge_type
        )

        current_app.logger.info(f"Successfully fetched user stats: {len(user_stats)} users")
        return jsonify({
            'status': 'success',
            'data': user_stats,
            'timeRange': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            }
        })

    except Exception as e:
        current_app.logger.error(f"Error in get_user_stats: {str(e)}", exc_info=True)
        return jsonify({
            'error': str(e)
        }), 500

@metrics_bp.route('/user-events', methods=['GET'])
async def get_user_events():
    """Get user events"""
    try:
        # Parse parameters
        trace_id = request.args.get('traceId')
        start_date = request.args.get('startDate')
        end_date = request.args.get('endDate')

        current_app.logger.info(f"Fetching user events: traceId={trace_id}, start={start_date}, end={end_date}")

        if not all([trace_id, start_date, end_date]):
            current_app.logger.error("Missing required parameters")
            return jsonify({
                'status': 'error',
                'error': 'Missing required parameters'
            }), 400

        try:
            # Convert dates with UTC timezone
            start_date = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            end_date = datetime.fromisoformat(end_date.replace('Z', '+00:00'))

            # Ensure dates are in UTC
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=timezone.utc)
            if end_date.tzinfo is None:
                end_date = end_date.replace(tzinfo=timezone.utc)

            # Get user events
            events = await current_app.analytics_service.get_user_events(
                trace_id=trace_id,
                start_date=start_date,
                end_date=end_date
            )

            return jsonify({
                'status': 'success',
                'data': events,
                'timeRange': {
                    'start': start_date.isoformat(),
                    'end': end_date.isoformat()
                }
            })

        except ValueError as e:
            current_app.logger.error(f"Date format error: {e}")
            return jsonify({
                'status': 'error',
                'error': f'Invalid date format: {str(e)}'
            }), 400

    except Exception as e:
        current_app.logger.error(f"Error in get_user_events: {str(e)}", exc_info=True)
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500

@metrics_bp.route('/metrics/<metric_id>/target', methods=['POST'])
async def set_metric_target(metric_id: str):
    try:
        # Load current targets
        target_file = Path(__file__).parent.parent / "data" / "metric_targets.json"
        if target_file.exists():
            with open(target_file, "r") as f:
                targets = json.load(f)
        else:
            targets = {}
        
        # Update target
        target = await request.get_json()
        targets[metric_id] = target['target']
        
        # Save targets
        with open(target_file, "w") as f:
            json.dump(targets, f, indent=2)
        
        return {"success": True}
    except Exception as e:
        current_app.logger.error(f"Error setting metric target: {e}")
        raise HTTPException(status_code=500, detail=str(e))

================================================
File: dashboardbackend/src/api/tasks.py
================================================
from quart import Blueprint, request, jsonify
import os
import logging
from typing import Dict, List, Optional
from opensearchpy import AsyncOpenSearch
import aiohttp
import ssl
import dateutil.parser
import csv
import io
from datetime import datetime
from collections import defaultdict
import jwt
import asyncio
from opensearchpy.exceptions import ConnectionError, TransportError
from src.utils.query_builder import OpenSearchQueryBuilder

logger = logging.getLogger(__name__)
tasks_bp = Blueprint('tasks', __name__)

def get_opensearch_client():
    opensearch_url = os.getenv('OPENSEARCH_URL', 'https://localhost:9200')
    username = os.getenv('OPENSEARCH_USERNAME', 'elkadmin')
    password = os.getenv('OPENSEARCH_PASSWORD', '')
    
    # Create SSL context that doesn't verify certificates
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    client = AsyncOpenSearch(
        hosts=[opensearch_url],
        http_auth=(username, password),
        verify_certs=False,
        ssl_context=ssl_context,
        timeout=30
    )
    return client

async def fetch_descope_user_ids():
    """
    Fetch Descope users and return a list of v2userIds.
    We expect the Descope API to return a custom attribute (in customAttributes)
    called "v2UserId" that contains the OpenSearch user ID (a hex string matching trace_id).
    If not present, an empty list is returned.
    """
    descope_url = os.getenv('DESCOPE_API_URL')
    bearer_token = os.getenv('DESCOPE_BEARER_TOKEN')
    if not descope_url or not bearer_token:
        logger.error("Descope configuration missing.")
        return []
    headers = {
        'Authorization': f'Bearer {bearer_token}',
        'Content-Type': 'application/json'
    }
    payload = {
        "projectId": "P2riizmYDJ2VAIjBw7ST0Qb2cNpd",
        "tenantIds": [],
        "text": "",
        "roleNames": [],
        "loginIds": [],
        "ssoAppIds": [],
        "customAttributes": {}
    }
    try:
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        async with aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(ssl=ssl_context)
        ) as session:
            async with session.post(descope_url, headers=headers, json=payload) as response:
                if response.status == 200:
                    data = await response.json()
                    logger.debug(f"Descope API response: {data}")
                    users = data.get('users', [])
                    # Retrieve the v2 user IDs from the custom attribute "v2UserId"
                    descope_ids = [
                        user.get('customAttributes', {}).get('v2UserId')
                        for user in users
                        if user.get('customAttributes', {}).get('v2UserId')
                    ]
                    logger.debug(f"Fetched {len(descope_ids)} Descope v2userIds")
                    return descope_ids
                else:
                    error_text = await response.text()
                    logger.error(f"Descope API error: {response.status} - {error_text}")
                    return []
    except Exception as e:
        logger.error(f"Exception while fetching Descope v2userIds: {str(e)}", exc_info=True)
        return []

@tasks_bp.route('/getGaugeUserCandidates')
async def get_gauge_user_candidates():
    metric_id = request.args.get('metricId')
    start_date = request.args.get('startDate')
    end_date = request.args.get('endDate')
    logger.info("getGaugeUserCandidates: metricId=%s, startDate=%s, endDate=%s", metric_id, start_date, end_date)
    
    if not metric_id:
        return jsonify({"error": "metricId parameter is required"}), 400

    # Fetch v2userIds from Descope
    descope_ids = await fetch_descope_user_ids()
    logger.info(f"Fetched {len(descope_ids)} Descope user IDs")
    if not descope_ids:
        logger.error("No Descope users found")
        return jsonify({"userIds": []})

    client = get_opensearch_client()
    index = "events-v2"
    query_builder = OpenSearchQueryBuilder()
    
    # Build base conditions
    must_conditions = [
        {"term": {"status.keyword": "succeeded"}},
        {"term": {"event_name.keyword": "handleMessageInThread_start"}},
        {"terms": {"trace_id.keyword": descope_ids}}
    ]
    
    # Add date range filter if provided
    if start_date and end_date:
        try:
            parsed_start = dateutil.parser.parse(start_date)
            parsed_end = dateutil.parser.parse(end_date)
            logger.debug("Parsed startDate: %s, endDate: %s", parsed_start.isoformat(), parsed_end.isoformat())
            must_conditions.append({
                "range": {
                    "timestamp": {
                        "gte": parsed_start.isoformat(),
                        "lte": parsed_end.isoformat()
                    }
                }
            })
        except Exception as parse_err:
            logger.error("Error parsing dates: %s", str(parse_err))
            await client.close()
            return jsonify({"error": "Invalid date format for startDate or endDate"}), 400

    # Build query using the same structure as analytics service
    query = query_builder.build_composite_query(
        must_conditions=must_conditions,
        aggregations={
            "aggs": {
                "thread_count": {
                    "terms": {"field": "trace_id.keyword", "size": 10000},
                    "aggs": {
                        "thread_filter": {
                            "bucket_selector": {
                                "buckets_path": {"count": "_count"},
                                "script": "params.count > 20" if metric_id in ["active_chat_users", "power_users"] else
                                         "params.count >= 5 && params.count <= 20" if metric_id == "medium_chat_users" else
                                         "params.count > 0"
                            }
                        }
                    }
                }
            }
        }
    )

    logger.debug("Final Query: %s", query)
    try:
        result = await client.search(index=index, body=query, size=0)
        await client.close()
        
        # Extract user IDs from buckets that passed the bucket selector
        user_ids = [bucket["key"] for bucket in result["aggregations"]["thread_count"]["buckets"]]
        logger.info("Found %d matching users", len(user_ids))
        
        return jsonify({"userIds": user_ids})
    except Exception as e:
        logger.error(f"Error in getGaugeUserCandidates: {str(e)}", exc_info=True)
        await client.close()
        return jsonify({"error": str(e)}), 500

@tasks_bp.route('/getTaskResults')
async def get_task_results():
    metric_id = request.args.get('metricId')
    client = get_opensearch_client()
    index = "events-v2"
    must_conditions = [{"term": {"status.keyword": "succeeded"}}]
    
    if metric_id:
        if metric_id == "sketch_users":
            must_conditions.append({"exists": {"field": "sketchId"}})
        elif metric_id == "thread_users":
            must_conditions.append({"term": {"event_name.keyword": "handleMessageInThread_start"}})
        elif metric_id == "render_users":
            must_conditions.append({"term": {"event_name.keyword": "renderStart_end"}})
        elif metric_id in ["active_chat_users", "medium_chat_users"]:
            must_conditions.append({"term": {"event_name.keyword": "handleMessageInThread_start"}})
        size = 1000
    else:
        must_conditions.append({"exists": {"field": "email"}})
        size = 0

    query = {"query": {"bool": {"must": must_conditions}}}
    if not metric_id:
        query["aggs"] = {"distinct_emails": {"terms": {"field": "email.keyword", "size": 1000}}}
    
    try:
        result = await client.search(index=index, body=query, size=size)
        await client.close()
        if not metric_id:
            emails = [bucket["key"] for bucket in result["aggregations"]["distinct_emails"]["buckets"]]
            return jsonify({"emails": emails})
        else:
            events = []
            for hit in result["hits"]["hits"]:
                source = hit["_source"]
                events.append({
                    "email": source.get("email"),
                    "sketchId": source.get("sketchId"),
                    "timestamp": source.get("timestamp"),
                    "event_name": source.get("event_name")
                })
            return jsonify({"events": events})
    except Exception as e:
        logger.error(f"Error in getTaskResults: {str(e)}", exc_info=True)
        await client.close()
        return jsonify({"error": str(e)}), 500

@tasks_bp.route('/getTaskStatus')
async def get_task_status():
    sketch_id = request.args.get('sketchId')
    if not sketch_id:
        return jsonify({"error": "sketchId parameter is required"}), 400

    client = get_opensearch_client()
    index = "events-v2"
    query = {
        "query": {
            "bool": {
                "must": [
                    {"term": {"status.keyword": "succeeded"}},
                    {"term": {"sketchId.keyword": sketch_id}},
                    {"exists": {"field": "renderedAudioUrl"}}
                ]
            }
        }
    }
    
    try:
        result = await client.search(index=index, body=query, size=1000)
        await client.close()
        productions = []
        for hit in result["hits"]["hits"]:
            source = hit["_source"]
            productions.append({
                "renderedAudioUrl": source.get("renderedAudioUrl"),
                "timestamp": source.get("timestamp")
            })
        return jsonify({"productions": productions})
    except Exception as e:
        logger.error(f"Error in getTaskStatus: {str(e)}", exc_info=True)
        await client.close()
        return jsonify({"error": str(e)}), 500

@tasks_bp.route('/getUserEventsById')
async def get_user_events_by_id():
    user_id = request.args.get('userId')
    if not user_id:
        return jsonify({"error": "userId parameter is required"}), 400

    client = get_opensearch_client()
    index = "events-v2"
    query = {
        "query": {
            "bool": {
                "must": [
                    {"term": {"trace_id.keyword": user_id}},
                    {"term": {"status.keyword": "succeeded"}}
                ]
            }
        },
        "size": 1000
    }
    try:
        result = await client.search(index=index, body=query)
        await client.close()
        events = [hit["_source"] for hit in result["hits"]["hits"]]
        return jsonify({"events": events})
    except Exception as e:
        logger.error(f"Error in getUserEventsById: {str(e)}", exc_info=True)
        await client.close()
        return jsonify({"error": str(e)}), 500

@tasks_bp.route('/getGaugeUsers')
async def get_gauge_users():
    metric_id = request.args.get('metricId')
    if not metric_id:
        return jsonify({"error": "metricId parameter is required"}), 400

    client = get_opensearch_client()
    index = "events-v2"
    must_conditions = [
        {"term": {"status.keyword": "succeeded"}},
        {"exists": {"field": "trace_id"}}
    ]
    if metric_id == "sketch_users":
        must_conditions.append({"exists": {"field": "sketchId"}})
    elif metric_id == "thread_users":
        must_conditions.append({"term": {"event_name.keyword": "handleMessageInThread_start"}})
    elif metric_id == "render_users":
        must_conditions.append({"term": {"event_name.keyword": "renderStart_end"}})
    elif metric_id in ["active_chat_users", "medium_chat_users"]:
        must_conditions.append({"term": {"event_name.keyword": "handleMessageInThread_start"}})
    
    query = {
        "query": {"bool": {"must": must_conditions}},
        "aggs": {
            "distinct_users": {
                "terms": {
                    "field": "trace_id.keyword",
                    "size": 1000
                },
                "aggs": {
                    "user_email": {
                        "terms": {
                            "field": "email.keyword",
                            "size": 1
                        }
                    }
                }
            }
        },
        "size": 0
    }
    
    try:
        result = await client.search(index=index, body=query)
        await client.close()
        users = []
        for bucket in result["aggregations"]["distinct_users"]["buckets"]:
            if bucket["key"]:
                email_buckets = bucket["user_email"]["buckets"]
                email = email_buckets[0]["key"] if email_buckets else None
                users.append({
                    "userId": bucket["key"],
                    "email": email,
                    "count": bucket["doc_count"]
                })
        return jsonify({"users": users})
    except Exception as e:
        logger.error(f"Error in getGaugeUsers: {str(e)}", exc_info=True)
        await client.close()
        return jsonify({"error": str(e)}), 500

@tasks_bp.route('/getFields')
async def get_fields():
    client = get_opensearch_client()
    index = "events-v2"
    try:
        mapping = await client.indices.get_mapping(index=index)
        await client.close()
        properties = mapping.get(index, {}).get("mappings", {}).get("properties", {})
        fields = list(properties.keys())
        return jsonify({"fields": fields})
    except Exception as e:
        logger.error(f"Error in getFields: {str(e)}")
        await client.close()
        return jsonify({"error": str(e)}), 500

def init_app(app):
    """Register tasks blueprint with the Quart app."""
    app.register_blueprint(tasks_bp)

class EventDiscoveryService:
    def __init__(self, client: AsyncOpenSearch):
        self.client = client
        self.index = "events-v2"

    def _extract_user_id_from_token(self, auth_header: str) -> Optional[str]:
        """Extract user ID from JWT token."""
        if not auth_header or not auth_header.startswith('Bearer '):
            return None
        try:
            token = auth_header.split(' ')[1]
            payload = jwt.decode(token, options={"verify_signature": False})
            return payload.get('sub')
        except Exception as e:
            logger.error(f"Error extracting user ID from token: {str(e)}")
            return None

    async def get_user_events(self, auth_header: str) -> Dict:
        """Get events for a user identified by their JWT token."""
        user_id = self._extract_user_id_from_token(auth_header)
        if not user_id:
            return {"error": "Invalid or missing authentication"}

        query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"trace_id.keyword": user_id}},
                        {"term": {"status.keyword": "succeeded"}}
                    ]
                }
            },
            "size": 1000
        }

        try:
            result = await self.client.search(index=self.index, body=query)
            events = [hit["_source"] for hit in result["hits"]["hits"]]
            return {"events": events}
        except Exception as e:
            logger.error(f"Error getting user events: {str(e)}")
            return {"error": str(e)}

================================================
File: dashboardbackend/src/core/__init__.py
================================================
"""
Initialize application services and connections
"""
import logging
import os
from quart import Quart
from src.services.caching_service import CachingService
from src.services.analytics_service import AnalyticsService
from src.services.descope_service import DescopeService
from src.services.opensearch_service import OpenSearchService
from src.utils.query_builder import OpenSearchQueryBuilder
from opensearchpy import AsyncOpenSearch
import redis.asyncio as redis
from dotenv import load_dotenv

logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize Redis client
redis_client = redis.from_url(
    os.getenv('REDIS_URL', 'redis://localhost:6379'),
    decode_responses=True
)

# Initialize OpenSearch client
opensearch_client = AsyncOpenSearch(
    hosts=[os.getenv('OPENSEARCH_URL', 'https://localhost:9200')],
    http_auth=(
        os.getenv('OPENSEARCH_USERNAME', ''),
        os.getenv('OPENSEARCH_PASSWORD', '')
    ),
    use_ssl=True,
    verify_certs=False,
    ssl_show_warn=False
)

async def init_services(app: Quart) -> None:
    """Initialize all required services"""
    try:
        # Initialize CachingService
        caching_service = CachingService(redis_client)

        # Store cache instance in app context
        app.cache = caching_service

        # Initialize OpenSearchQueryBuilder
        query_builder = OpenSearchQueryBuilder()

        # Initialize OpenSearchService
        opensearch_service = OpenSearchService()
        
        # Verify OpenSearch connection
        connection_verified = await opensearch_service.verify_connection()
        if not connection_verified:
            logger.error("Failed to verify OpenSearch connection. Analytics functionality may be limited.")
        else:
            logger.info("OpenSearch connection verified successfully")

        # Initialize DescopeService
        descope_service = DescopeService()

        # Initialize AnalyticsService
        analytics_service = AnalyticsService(
            caching_service,
            opensearch_service,
            query_builder,
            descope_service
        )

        # Store services in app context
        app.cache = caching_service
        app.descope_service = descope_service
        app.analytics_service = analytics_service
        app.opensearch_service = opensearch_service

        # Add cleanup on app teardown
        @app.before_serving
        async def startup():
            logger.info("Starting up services...")

        @app.after_serving
        async def shutdown():
            logger.info("Shutting down services...")
            await caching_service.disconnect()
            await redis_client.close()
            await opensearch_service.client.close()

    except Exception as e:
        logger.error(f"Error initializing services: {str(e)}")
        raise

__all__ = ['redis_client', 'opensearch_client', 'init_services']

================================================
File: dashboardbackend/src/data/historical_metrics.json
================================================
{
    "metadata": {
        "start_date": "2024-10-01",
        "end_date": "2025-01-26",
        "last_updated": "2025-01-26"
    },
    "daily_metrics": {
        "2024-10-01": {
            "total_users": 0,
            "active_users": 0,
            "producers": 0
        },
        "2024-10-02": {
            "total_users": 325,
            "active_users": 40,
            "producers": 18
        },
        "2024-10-03": {
            "total_users": 651,
            "active_users": 80,
            "producers": 36
        },
        "2024-10-04": {
            "total_users": 977,
            "active_users": 121,
            "producers": 55
        },
        "2024-10-05": {
            "total_users": 1302,
            "active_users": 161,
            "producers": 73
        },
        "2024-10-06": {
            "total_users": 1628,
            "active_users": 202,
            "producers": 91
        },
        "2024-10-07": {
            "total_users": 1954,
            "active_users": 242,
            "producers": 110
        },
        "2024-10-08": {
            "total_users": 2279,
            "active_users": 283,
            "producers": 128
        },
        "2024-10-09": {
            "total_users": 2605,
            "active_users": 323,
            "producers": 146
        },
        "2024-10-10": {
            "total_users": 2931,
            "active_users": 363,
            "producers": 165
        },
        "2024-10-11": {
            "total_users": 3256,
            "active_users": 404,
            "producers": 183
        },
        "2024-10-12": {
            "total_users": 3582,
            "active_users": 444,
            "producers": 202
        },
        "2024-10-13": {
            "total_users": 3908,
            "active_users": 485,
            "producers": 220
        },
        "2024-10-14": {
            "total_users": 4233,
            "active_users": 525,
            "producers": 238
        },
        "2024-10-15": {
            "total_users": 4559,
            "active_users": 566,
            "producers": 257
        },
        "2024-10-16": {
            "total_users": 4885,
            "active_users": 606,
            "producers": 275
        },
        "2024-10-17": {
            "total_users": 5210,
            "active_users": 646,
            "producers": 293
        },
        "2024-10-18": {
            "total_users": 5536,
            "active_users": 687,
            "producers": 312
        },
        "2024-10-19": {
            "total_users": 5862,
            "active_users": 727,
            "producers": 330
        },
        "2024-10-20": {
            "total_users": 6187,
            "active_users": 768,
            "producers": 348
        },
        "2024-10-21": {
            "total_users": 6513,
            "active_users": 808,
            "producers": 367
        },
        "2024-10-22": {
            "total_users": 6839,
            "active_users": 849,
            "producers": 385
        },
        "2024-10-23": {
            "total_users": 7164,
            "active_users": 889,
            "producers": 404
        },
        "2024-10-24": {
            "total_users": 7490,
            "active_users": 929,
            "producers": 422
        },
        "2024-10-25": {
            "total_users": 7816,
            "active_users": 970,
            "producers": 440
        },
        "2024-10-26": {
            "total_users": 8141,
            "active_users": 1010,
            "producers": 459
        },
        "2024-10-27": {
            "total_users": 8467,
            "active_users": 1051,
            "producers": 477
        },
        "2024-10-28": {
            "total_users": 8793,
            "active_users": 1091,
            "producers": 495
        },
        "2024-10-29": {
            "total_users": 9118,
            "active_users": 1132,
            "producers": 514
        },
        "2024-10-30": {
            "total_users": 9444,
            "active_users": 1172,
            "producers": 532
        },
        "2024-10-31": {
            "total_users": 9770,
            "active_users": 1213,
            "producers": 551
        },
        "2024-11-01": {
            "total_users": 10065,
            "active_users": 1313,
            "producers": 596
        },
        "2024-11-02": {
            "total_users": 10360,
            "active_users": 1414,
            "producers": 642
        },
        "2024-11-03": {
            "total_users": 10656,
            "active_users": 1514,
            "producers": 688
        },
        "2024-11-04": {
            "total_users": 10951,
            "active_users": 1615,
            "producers": 733
        },
        "2024-11-05": {
            "total_users": 11247,
            "active_users": 1716,
            "producers": 779
        },
        "2024-11-06": {
            "total_users": 11542,
            "active_users": 1816,
            "producers": 825
        },
        "2024-11-07": {
            "total_users": 11838,
            "active_users": 1917,
            "producers": 871
        },
        "2024-11-08": {
            "total_users": 12133,
            "active_users": 2017,
            "producers": 916
        },
        "2024-11-09": {
            "total_users": 12429,
            "active_users": 2118,
            "producers": 962
        },
        "2024-11-10": {
            "total_users": 12724,
            "active_users": 2219,
            "producers": 1008
        },
        "2024-11-11": {
            "total_users": 13020,
            "active_users": 2319,
            "producers": 1054
        },
        "2024-11-12": {
            "total_users": 13315,
            "active_users": 2420,
            "producers": 1099
        },
        "2024-11-13": {
            "total_users": 13611,
            "active_users": 2520,
            "producers": 1145
        },
        "2024-11-14": {
            "total_users": 13906,
            "active_users": 2621,
            "producers": 1191
        },
        "2024-11-15": {
            "total_users": 14202,
            "active_users": 2722,
            "producers": 1237
        },
        "2024-11-16": {
            "total_users": 14497,
            "active_users": 2822,
            "producers": 1282
        },
        "2024-11-17": {
            "total_users": 14792,
            "active_users": 2923,
            "producers": 1328
        },
        "2024-11-18": {
            "total_users": 15088,
            "active_users": 3023,
            "producers": 1374
        },
        "2024-11-19": {
            "total_users": 15383,
            "active_users": 3124,
            "producers": 1419
        },
        "2024-11-20": {
            "total_users": 15679,
            "active_users": 3225,
            "producers": 1465
        },
        "2024-11-21": {
            "total_users": 15974,
            "active_users": 3325,
            "producers": 1511
        },
        "2024-11-22": {
            "total_users": 16270,
            "active_users": 3426,
            "producers": 1557
        },
        "2024-11-23": {
            "total_users": 16565,
            "active_users": 3526,
            "producers": 1602
        },
        "2024-11-24": {
            "total_users": 16861,
            "active_users": 3627,
            "producers": 1648
        },
        "2024-11-25": {
            "total_users": 17156,
            "active_users": 3728,
            "producers": 1694
        },
        "2024-11-26": {
            "total_users": 17452,
            "active_users": 3828,
            "producers": 1740
        },
        "2024-11-27": {
            "total_users": 17747,
            "active_users": 3929,
            "producers": 1785
        },
        "2024-11-28": {
            "total_users": 18043,
            "active_users": 4029,
            "producers": 1831
        },
        "2024-11-29": {
            "total_users": 18338,
            "active_users": 4130,
            "producers": 1877
        },
        "2024-11-30": {
            "total_users": 18634,
            "active_users": 4231,
            "producers": 1923
        },
        "2024-12-01": {
            "total_users": 19608,
            "active_users": 4412,
            "producers": 2005
        },
        "2024-12-02": {
            "total_users": 20583,
            "active_users": 4594,
            "producers": 2088
        },
        "2024-12-03": {
            "total_users": 21558,
            "active_users": 4776,
            "producers": 2170
        },
        "2024-12-04": {
            "total_users": 22532,
            "active_users": 4957,
            "producers": 2253
        },
        "2024-12-05": {
            "total_users": 23507,
            "active_users": 5139,
            "producers": 2335
        },
        "2024-12-06": {
            "total_users": 24482,
            "active_users": 5321,
            "producers": 2418
        },
        "2024-12-07": {
            "total_users": 25456,
            "active_users": 5502,
            "producers": 2501
        },
        "2024-12-08": {
            "total_users": 26431,
            "active_users": 5684,
            "producers": 2583
        },
        "2024-12-09": {
            "total_users": 27406,
            "active_users": 5866,
            "producers": 2666
        },
        "2024-12-10": {
            "total_users": 28381,
            "active_users": 6047,
            "producers": 2748
        },
        "2024-12-11": {
            "total_users": 29355,
            "active_users": 6229,
            "producers": 2831
        },
        "2024-12-12": {
            "total_users": 30330,
            "active_users": 6411,
            "producers": 2913
        },
        "2024-12-13": {
            "total_users": 31305,
            "active_users": 6592,
            "producers": 2996
        },
        "2024-12-14": {
            "total_users": 32279,
            "active_users": 6774,
            "producers": 3079
        },
        "2024-12-15": {
            "total_users": 33254,
            "active_users": 6956,
            "producers": 3161
        },
        "2024-12-16": {
            "total_users": 34229,
            "active_users": 7137,
            "producers": 3244
        },
        "2024-12-17": {
            "total_users": 35204,
            "active_users": 7319,
            "producers": 3326
        },
        "2024-12-18": {
            "total_users": 36178,
            "active_users": 7501,
            "producers": 3409
        },
        "2024-12-19": {
            "total_users": 37153,
            "active_users": 7682,
            "producers": 3492
        },
        "2024-12-20": {
            "total_users": 38128,
            "active_users": 7864,
            "producers": 3574
        },
        "2024-12-21": {
            "total_users": 39102,
            "active_users": 8046,
            "producers": 3657
        },
        "2024-12-22": {
            "total_users": 40077,
            "active_users": 8227,
            "producers": 3739
        },
        "2024-12-23": {
            "total_users": 41052,
            "active_users": 8409,
            "producers": 3822
        },
        "2024-12-24": {
            "total_users": 42027,
            "active_users": 8591,
            "producers": 3904
        },
        "2024-12-25": {
            "total_users": 43001,
            "active_users": 8772,
            "producers": 3987
        },
        "2024-12-26": {
            "total_users": 43976,
            "active_users": 8954,
            "producers": 4070
        },
        "2024-12-27": {
            "total_users": 44951,
            "active_users": 9136,
            "producers": 4152
        },
        "2024-12-28": {
            "total_users": 45925,
            "active_users": 9317,
            "producers": 4235
        },
        "2024-12-29": {
            "total_users": 46900,
            "active_users": 9499,
            "producers": 4317
        },
        "2024-12-30": {
            "total_users": 47875,
            "active_users": 9681,
            "producers": 4400
        },
        "2024-12-31": {
            "total_users": 48850,
            "active_users": 9863,
            "producers": 4483
        },
        "2025-01-01": {
            "total_users": 49086,
            "active_users": 10120,
            "producers": 4600
        },
        "2025-01-02": {
            "total_users": 49323,
            "active_users": 10378,
            "producers": 4717
        },
        "2025-01-03": {
            "total_users": 49559,
            "active_users": 10635,
            "producers": 4834
        },
        "2025-01-04": {
            "total_users": 49796,
            "active_users": 10893,
            "producers": 4951
        },
        "2025-01-05": {
            "total_users": 50032,
            "active_users": 11150,
            "producers": 5068
        },
        "2025-01-06": {
            "total_users": 50269,
            "active_users": 11408,
            "producers": 5185
        },
        "2025-01-07": {
            "total_users": 50505,
            "active_users": 11666,
            "producers": 5302
        },
        "2025-01-08": {
            "total_users": 50742,
            "active_users": 11923,
            "producers": 5419
        },
        "2025-01-09": {
            "total_users": 50978,
            "active_users": 12181,
            "producers": 5536
        },
        "2025-01-10": {
            "total_users": 51215,
            "active_users": 12438,
            "producers": 5653
        },
        "2025-01-11": {
            "total_users": 51451,
            "active_users": 12696,
            "producers": 5770
        },
        "2025-01-12": {
            "total_users": 51688,
            "active_users": 12953,
            "producers": 5887
        },
        "2025-01-13": {
            "total_users": 51925,
            "active_users": 13211,
            "producers": 6005
        },
        "2025-01-14": {
            "total_users": 52161,
            "active_users": 13469,
            "producers": 6122
        },
        "2025-01-15": {
            "total_users": 52398,
            "active_users": 13726,
            "producers": 6239
        },
        "2025-01-16": {
            "total_users": 52634,
            "active_users": 13984,
            "producers": 6356
        },
        "2025-01-17": {
            "total_users": 52871,
            "active_users": 14241,
            "producers": 6473
        },
        "2025-01-18": {
            "total_users": 53107,
            "active_users": 14499,
            "producers": 6590
        },
        "2025-01-19": {
            "total_users": 53344,
            "active_users": 14756,
            "producers": 6707
        },
        "2025-01-20": {
            "total_users": 53580,
            "active_users": 15014,
            "producers": 6824
        },
        "2025-01-21": {
            "total_users": 53817,
            "active_users": 15272,
            "producers": 6941
        },
        "2025-01-22": {
            "total_users": 54053,
            "active_users": 15529,
            "producers": 7058
        },
        "2025-01-23": {
            "total_users": 54290,
            "active_users": 15787,
            "producers": 7175
        },
        "2025-01-24": {
            "total_users": 54526,
            "active_users": 16044,
            "producers": 7292
        },
        "2025-01-25": {
            "total_users": 54763,
            "active_users": 16302,
            "producers": 7409
        },
        "2025-01-26": {
            "total_users": 55000,
            "active_users": 16560,
            "producers": 7527
        }
    }
}

================================================
File: dashboardbackend/src/data/metric_targets.json
================================================
{}


================================================
File: dashboardbackend/src/services/__init__.py
================================================
"""
Services package for business logic
"""

================================================
File: dashboardbackend/src/services/analytics_service.py
================================================
"""
AnalyticsService: Core service for fetching and aggregating analytics data
"""
from typing import Dict, Any, List, Optional
import logging
from datetime import datetime, timedelta
import os
from src.services.metrics_service import AnalyticsMetricsService
from src.services.descope_service import DescopeService
from src.services.opensearch_service import OpenSearchService
from src.services.historical_data_service import HistoricalDataService
from src.services.caching_service import CachingService
from src.utils.query_builder import OpenSearchQueryBuilder
from datetime import timezone

logger = logging.getLogger(__name__)

class AnalyticsService:
    def __init__(self, caching_service: CachingService, opensearch_service: OpenSearchService, query_builder: OpenSearchQueryBuilder, descope_service: DescopeService):
        self.caching_service = caching_service
        self.disable_cache = os.getenv('DISABLE_CACHE', 'false').lower() == 'true'
        self.opensearch_service = opensearch_service
        self.descope_service = descope_service
        self.historical_data_service = HistoricalDataService()
        self.analytics_metrics = AnalyticsMetricsService(
            self.opensearch_service,
            self.caching_service,
            query_builder,
            os.getenv('OPENSEARCH_INDEX', 'your_index_name'),
            os.getenv('OPENSEARCH_TIMESTAMP_FIELD', 'timestamp'),
            int(os.getenv('OPENSEARCH_REQUEST_TIMEOUT', '30')),
            self.descope_service
        )

    async def get_dashboard_metrics(self, start_date: datetime, end_date: datetime, include_v1: bool = False) -> Dict[str, Any]:
        """Get all dashboard metrics for the given time period."""
        try:
            # Get current period metrics from OpenSearch
            message_counts = await self.opensearch_service.get_user_counts(start_date, end_date, "handleMessageInThread_start")
            render_counts = await self.opensearch_service.get_user_counts(start_date, end_date, "renderStart_end")
            sketch_counts = await self.opensearch_service.get_user_counts(start_date, end_date, "uploadSketch_end")

            # Calculate user segments
            active_users = len([u for u in message_counts.values() if u > 0])
            power_users = len([u for u in message_counts.values() if u > 20])
            moderate_users = len([u for u in message_counts.values() if 5 <= u <= 20])
            producers = len([u for u in render_counts.values() if u > 0])
            producers_attempting = len([u for u in sketch_counts.values() if u > 0])

            # Get total users from Descope
            total_users = await self.descope_service.get_total_users()
            if total_users == 0:
                # Use unique users from OpenSearch as fallback
                all_users = set(message_counts.keys()) | set(render_counts.keys()) | set(sketch_counts.keys())
                total_users = len(all_users)
                logger.info(f"Using OpenSearch unique users as fallback for total users: {total_users}")

            # Calculate previous period dates
            days_diff = (end_date - start_date).days
            prev_end_date = start_date
            prev_start_date = prev_end_date - timedelta(days=days_diff)

            # Get previous period metrics from OpenSearch
            prev_message_counts = await self.opensearch_service.get_user_counts(prev_start_date, prev_end_date, "handleMessageInThread_start")
            prev_render_counts = await self.opensearch_service.get_user_counts(prev_start_date, prev_end_date, "renderStart_end")
            prev_sketch_counts = await self.opensearch_service.get_user_counts(prev_start_date, prev_end_date, "uploadSketch_end")

            # Calculate previous period metrics
            active_users_prev = len([u for u in prev_message_counts.values() if u > 0])
            power_users_prev = len([u for u in prev_message_counts.values() if u > 20])
            moderate_users_prev = len([u for u in prev_message_counts.values() if 5 <= u <= 20])
            producers_prev = len([u for u in prev_render_counts.values() if u > 0])
            producers_attempting_prev = len([u for u in prev_sketch_counts.values() if u > 0])

            # Format metrics for frontend
            current_date = datetime.now(timezone.utc)
            previous_date = current_date - timedelta(days=30)  # Use 30 days ago for previous period
            
            # Get current unfiltered totals
            current_date = datetime.now(timezone.utc)
            one_year_ago = current_date - timedelta(days=365)
            
            # Get unfiltered active users (all time)
            unfiltered_active_counts = await self.opensearch_service.get_user_counts(
                one_year_ago,
                current_date,
                "handleMessageInThread_start"
            )
            unfiltered_active_users = len([u for u in unfiltered_active_counts.values() if u > 0])

            # Get unfiltered producers (all time)
            unfiltered_producer_counts = await self.opensearch_service.get_user_counts(
                one_year_ago,
                current_date,
                "renderStart_end"
            )
            unfiltered_producers = len([u for u in unfiltered_producer_counts.values() if u > 0])

            # V1 historical numbers
            v1_total_users = 55000
            v1_active_users = 30000
            v1_producers = 15000

            # Add historical total metrics
            historical_total_metrics = [
                {
                    "id": "historical_total_users",
                    "name": "All Time Total Users",
                    "description": "Total users including V1",
                    "category": "historical",
                    "interval": "all_time",
                    "data": {
                        "value": v1_total_users + total_users,
                        "trend": "neutral"
                    }
                },
                {
                    "id": "historical_active_users",
                    "name": "All Time Active Users",
                    "description": "Total active users including V1",
                    "category": "historical",
                    "interval": "all_time",
                    "data": {
                        "value": v1_active_users + unfiltered_active_users,
                        "trend": "neutral"
                    }
                },
                {
                    "id": "historical_producers",
                    "name": "All Time Producers",
                    "description": "Total producers including V1",
                    "category": "historical",
                    "interval": "all_time",
                    "data": {
                        "value": v1_producers + unfiltered_producers,
                        "trend": "neutral"
                    }
                }
            ]

            formatted_metrics = [
                {
                    "id": "total_users",
                    "name": "Total Users",
                    "description": "Total number of registered users",
                    "category": "user",
                    "interval": "daily",
                    "data": {
                        "value": total_users,
                        "trend": "neutral"
                    }
                },
                {
                    "id": "active_users",
                    "name": "Active Users",
                    "description": "Users who have started at least one message thread",
                    "category": "user",
                    "interval": "daily",
                    "data": {
                        "value": active_users,
                        "previousValue": active_users_prev,
                        "trend": "neutral",
                        "changePercentage": 0,
                        "historical": [
                            {"date": previous_date.isoformat(), "value": active_users_prev},
                            {"date": current_date.isoformat(), "value": active_users}
                        ]
                    }
                },
                {
                    "id": "power_users",
                    "name": "Power Users", 
                    "description": "Users with more than 20 message threads",
                    "category": "engagement",
                    "interval": "daily",
                    "data": {
                        "value": power_users,
                        "previousValue": power_users_prev,
                        "trend": "neutral",
                        "changePercentage": 0,
                        "historical": [
                            {"date": previous_date.isoformat(), "value": power_users_prev},
                            {"date": current_date.isoformat(), "value": power_users}
                        ]
                    }
                },
                {
                    "id": "moderate_users",
                    "name": "Moderate Users",
                    "description": "Users with 5-20 message threads",
                    "category": "engagement",
                    "interval": "daily", 
                    "data": {
                        "value": moderate_users,
                        "previousValue": moderate_users_prev,
                        "trend": "neutral",
                        "changePercentage": 0,
                        "historical": [
                            {"date": previous_date.isoformat(), "value": moderate_users_prev},
                            {"date": current_date.isoformat(), "value": moderate_users}
                        ]
                    }
                },
                {
                    "id": "producers",
                    "name": "Producers",
                    "description": "Users who have completed at least one render",
                    "category": "engagement",
                    "interval": "daily",
                    "data": {
                        "value": producers,
                        "previousValue": producers_prev,
                        "trend": "neutral",
                        "changePercentage": 0,
                        "historical": [
                            {"date": previous_date.isoformat(), "value": producers_prev},
                            {"date": current_date.isoformat(), "value": producers}
                        ]
                    }
                },
                {
                    "id": "producers_attempting",
                    "name": "Producers Attempting",
                    "description": "Users who have uploaded at least one sketch",
                    "category": "engagement",
                    "interval": "daily",
                    "data": {
                        "value": producers_attempting,
                        "previousValue": producers_attempting_prev,
                        "trend": "neutral",
                        "changePercentage": 0,
                        "historical": [
                            {"date": previous_date.isoformat(), "value": producers_attempting_prev},
                            {"date": current_date.isoformat(), "value": producers_attempting}
                        ]
                    }
                }
            ]

            formatted_metrics.extend(historical_total_metrics)

            logger.info(f"Final dashboard metrics: {formatted_metrics}")
            return formatted_metrics

        except Exception as e:
            logger.error(f"Error getting dashboard metrics: {str(e)}", exc_info=True)
            raise

    async def get_user_statistics(self, start_date: datetime, end_date: datetime, gauge_type: str) -> List[Dict[str, Any]]:
        """Get user statistics including message and sketch counts"""
        try:
            # Get all user activity counts
            message_counts = await self.opensearch_service.get_user_counts(start_date, end_date, "handleMessageInThread_start")
            sketch_counts = await self.opensearch_service.get_user_counts(start_date, end_date, "uploadSketch_end")
            render_counts = await self.opensearch_service.get_user_counts(start_date, end_date, "renderStart_end")

            # Get user details from Descope
            all_user_ids = set(message_counts.keys()) | set(sketch_counts.keys()) | set(render_counts.keys())
            user_details = await self.descope_service.get_user_details(list(all_user_ids))

            # Filter users based on gauge type
            filtered_users = []
            for user_id in all_user_ids:
                message_count = message_counts.get(user_id, 0)
                sketch_count = sketch_counts.get(user_id, 0)
                render_count = render_counts.get(user_id, 0)

                # Apply filters based on gauge type
                include_user = False
                if gauge_type == 'active_users' and message_count > 0:
                    include_user = True
                elif gauge_type == 'power_users' and message_count > 20:
                    include_user = True
                elif gauge_type == 'moderate_users' and 5 <= message_count <= 20:
                    include_user = True
                elif gauge_type == 'producers' and render_count > 0:
                    include_user = True
                elif gauge_type == 'producers_attempting' and sketch_count > 0:
                    include_user = True

                if include_user:
                    user_detail = user_details.get(user_id, {})
                    filtered_users.append({
                        'trace_id': user_id,
                        'email': user_detail.get('email', ''),
                        'name': user_detail.get('name', ''),
                        'messageCount': message_count,
                        'sketchCount': sketch_count,
                        'renderCount': render_count
                    })

            return filtered_users

        except Exception as e:
            logger.error(f"Error getting user statistics: {str(e)}", exc_info=True)
            return []

    async def get_user_events(self, trace_id: str, start_date: datetime, end_date: datetime) -> List[Dict[str, Any]]:
        """Fetch user events based on trace_id"""
        return await self.opensearch_service.get_user_events(trace_id, start_date, end_date)

    async def merge_metrics(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Merge metrics from all sources"""
        # Define the data transition points
        historical_end = datetime(2025, 1, 26, tzinfo=timezone.utc)  # Historical data ends Jan 26th
        opensearch_start = datetime(2025, 1, 20, tzinfo=timezone.utc)  # OpenSearch data starts Jan 20th
        
        self.logger.debug(f"Date range: {start_date.isoformat()} to {end_date.isoformat()}")
        self.logger.debug(f"Historical end: {historical_end.isoformat()}")
        self.logger.debug(f"OpenSearch start: {opensearch_start.isoformat()}")
        
        metrics = {
            "total_users": 0,
            "thread_users_count": 0,
            "render_users": 0,
            "producers_count": 0,
            "daily_total_users": 0,
            "daily_thread_users": 0,
            "daily_render_users": 0,
            "daily_producers": 0
        }
        
        # Get historical metrics if date range includes Oct-Jan 26th
        if start_date <= historical_end:
            historical_metrics = await self.historical_data_service.get_v1_metrics(
                start_date,
                min(end_date, historical_end),
                include_v1=True
            )
            self.logger.debug(f"Historical metrics: {historical_metrics}")
            
            # Update metrics with historical data
            metrics.update({
                "total_users": historical_metrics.get("total_users", 0),
                "thread_users_count": historical_metrics.get("active_users", 0),  # Active users maps to thread users
                "producers_count": historical_metrics.get("producers", 0),
                "daily_total_users": historical_metrics.get("daily_total_users", 0),
                "daily_thread_users": historical_metrics.get("daily_active_users", 0),
                "daily_producers": historical_metrics.get("daily_producers", 0)
            })
        
        # Get OpenSearch metrics if date range includes Jan 20th onwards
        if end_date >= opensearch_start:
            os_metrics = await self.opensearch_service.get_metrics(
                max(start_date, opensearch_start),
                end_date
            )
            self.logger.debug(f"OpenSearch metrics: {os_metrics}")
            
            # If we're in the overlap period (Jan 20-26), merge the metrics
            if start_date <= historical_end and end_date >= opensearch_start:
                overlap_days = (min(end_date, historical_end) - opensearch_start).days + 1
                total_days = (end_date - start_date).days + 1
                
                # Weight the metrics based on the overlap period
                historical_weight = (historical_end - start_date).days + 1
                opensearch_weight = (end_date - opensearch_start).days + 1
                total_weight = historical_weight + opensearch_weight
                
                self.logger.debug(f"Overlap period - historical days: {historical_weight}, opensearch days: {opensearch_weight}")
                
                # Merge metrics with weighted averages for the overlap period
                metrics.update({
                    "thread_users_count": max(metrics["thread_users_count"], os_metrics["thread_users_count"]),
                    "producers_count": max(metrics["producers_count"], os_metrics["producers_count"]),
                    "daily_thread_users": int((metrics["daily_thread_users"] * historical_weight + 
                                            (os_metrics["thread_users_count"] / opensearch_weight) * opensearch_weight) / total_weight),
                    "daily_producers": int((metrics["daily_producers"] * historical_weight + 
                                        (os_metrics["producers_count"] / opensearch_weight) * opensearch_weight) / total_weight)
                })
            
            # If we're only in the OpenSearch period (after Jan 26th), use OpenSearch metrics
            elif start_date > historical_end:
                days_in_range = (end_date - start_date).days + 1
                metrics.update({
                    "thread_users_count": os_metrics["thread_users_count"],
                    "producers_count": os_metrics["producers_count"],
                    "daily_thread_users": int(os_metrics["thread_users_count"] / days_in_range),
                    "daily_producers": int(os_metrics["producers_count"] / days_in_range)
                })
        
        # Get current total users from Descope for the most up-to-date count
        if end_date >= datetime.now(timezone.utc) - timedelta(days=1):
            total_users = await self.descope_service.get_total_users()
            metrics["total_users"] = max(metrics["total_users"], total_users)
        
        self.logger.debug(f"Final merged metrics: {metrics}")
        return metrics

    async def get_metrics(self, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> Dict[str, Any]:
        """Get all metrics for the dashboard"""
        # If no dates provided, default to October 2024
        if start_date is None:
            start_date = datetime(2024, 10, 1, tzinfo=timezone.utc)
        if end_date is None:
            end_date = datetime(2024, 11, 1, tzinfo=timezone.utc)

        # Ensure dates are timezone-aware
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=timezone.utc)
        if end_date.tzinfo is None:
            end_date = end_date.replace(tzinfo=timezone.utc)

        self.logger.debug(f"Getting metrics for date range: {start_date.isoformat()} to {end_date.isoformat()}")

        # Get raw metrics and daily averages
        metrics = await self.merge_metrics(start_date, end_date)
        self.logger.debug(f"Final metrics: {metrics}")

        # Build response with both raw and daily values
        response = {
            "metrics": [
                {
                    "id": "descope_users",
                    "name": "Total Users",
                    "description": "Total number of registered users",
                    "category": "user",
                    "interval": "daily",
                    "data": {
                        "value": metrics["total_users"],  # Raw total
                        "previousValue": 0,
                        "trend": "up",
                        "changePercentage": 0,
                        "daily_average": metrics["daily_total_users"]  # Daily average
                    }
                },
                {
                    "id": "thread_users",
                    "name": "Thread Users",
                    "description": "Users who have started at least one message thread",
                    "category": "engagement",
                    "interval": "daily",
                    "data": {
                        "value": metrics["thread_users_count"],  # Raw total
                        "previousValue": 0,
                        "trend": "up",
                        "changePercentage": 0,
                        "daily_average": metrics["daily_thread_users"]  # Daily average
                    }
                },
                {
                    "id": "render_users",
                    "name": "Render Users",
                    "description": "Users who have completed at least one render",
                    "category": "performance",
                    "interval": "daily",
                    "data": {
                        "value": metrics["render_users"],  # Raw total
                        "previousValue": 0,
                        "trend": "neutral",
                        "changePercentage": 0,
                        "daily_average": metrics["daily_render_users"]  # Daily average
                    }
                },
                {
                    "id": "producers",
                    "name": "Producers",
                    "description": "Total number of producers",
                    "category": "user",
                    "interval": "daily",
                    "data": {
                        "value": metrics["producers_count"],  # Raw total
                        "previousValue": 0,
                        "trend": "up",
                        "changePercentage": 0,
                        "daily_average": metrics["daily_producers"]  # Daily average
                    }
                }
            ],
            "timeRange": {
                "start": self._format_date_iso(start_date),
                "end": self._format_date_iso(end_date)
            }
        }

        return response

    def _format_date_iso(self, dt: datetime) -> str:
        """Format datetime to UTC ISO string"""
        if dt.tzinfo is None:
            dt = dt.astimezone()
        return dt.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

================================================
File: dashboardbackend/src/services/caching_service.py
================================================
"""
Caching service for storing and retrieving cached data
"""
import json
import logging
from typing import Any, Optional, Dict
from datetime import datetime, timedelta
import redis.asyncio as redis

logger = logging.getLogger(__name__)

class CachingService:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.default_ttl = timedelta(minutes=5)

    async def disconnect(self) -> None:
        """Disconnect from Redis"""
        if self.redis:
            await self.redis.close()
            logger.info("Disconnected from Redis")

    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        try:
            value = await self.redis.get(key)
            if value:
                logger.debug(f"Successfully retrieved cached data for key: {key}")
                return json.loads(value)
            return None
        except Exception as e:
            logger.warning(f"Redis get failed: {str(e)}")
            return None

    async def set(self, key: str, value: Any, ttl: Optional[timedelta] = None) -> bool:
        """Set value in cache with optional expiration"""
        try:
            if isinstance(value, (dict, list)):
                value = json.dumps(value)
            elif not isinstance(value, str):
                value = str(value)

            expiry = ttl or self.default_ttl
            await self.redis.set(key, value, ex=int(expiry.total_seconds()))
            logger.debug(f"Successfully cached data for key: {key}")
            return True
        except Exception as e:
            logger.warning(f"Redis set failed: {str(e)}")
            return False

    async def delete(self, key: str) -> bool:
        """Delete value from cache"""
        try:
            await self.redis.delete(key)
            logger.debug(f"Successfully deleted cached data for key: {key}")
            return True
        except Exception as e:
            logger.warning(f"Redis delete failed: {str(e)}")
            return False

    async def clear_all(self) -> bool:
        """Clear all cached data"""
        try:
            await self.redis.flushdb()
            logger.info("Successfully cleared all cached data")
            return True
        except Exception as e:
            logger.warning(f"Redis flush failed: {str(e)}")
            return False

    async def get_or_set(self, key: str, value_func, ttl: Optional[timedelta] = None) -> Any:
        """Get value from cache or set it if not present"""
        cached_value = await self.get(key)
        if cached_value is not None:
            return cached_value

        value = await value_func()
        await self.set(key, value, ttl)
        return value

    async def get_many(self, keys: list) -> dict:
        """Get multiple values from cache"""
        try:
            values = await self.redis.mget(keys)
            result = {key: json.loads(value) if value else None for key, value in zip(keys, values)}
            logger.debug(f"Successfully retrieved multiple cached data for keys: {keys}")
            return result
        except Exception as e:
            logger.warning(f"Redis mget failed: {str(e)}")
            return {key: None for key in keys}

    async def set_many(self, data: dict, ttl: Optional[timedelta] = None) -> bool:
        """Set multiple values in cache with optional expiration"""
        try:
            pipeline = self.redis.pipeline()
            for key, value in data.items():
                if isinstance(value, (dict, list)):
                    value = json.dumps(value)
                elif not isinstance(value, str):
                    value = str(value)
                
                expiry = ttl or self.default_ttl
                pipeline.set(key, value, ex=int(expiry.total_seconds()))
            
            await pipeline.execute()
            logger.debug(f"Successfully cached multiple data for keys: {list(data.keys())}")
            return True
        except Exception as e:
            logger.warning(f"Redis mset failed: {str(e)}")
            return False

    async def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics
        
        Returns:
            Dict with cache stats
        """
        if not self.redis:
            logger.error("Redis not connected")
            return {}

        try:
            info = await self.redis.info()
            keys = await self.redis.keys('*')
            
            stats = {
                "total_keys": len(keys),
                "used_memory": info.get('used_memory_human'),
                "connected_clients": info.get('connected_clients'),
                "last_save_time": datetime.fromtimestamp(
                    int(info.get('rdb_last_save_time', 0))
                ).isoformat()
            }
            logger.debug("Successfully retrieved cache stats")
            return stats
            
        except Exception as e:
            logger.error(f"Error getting cache stats: {str(e)}")
            return {}

================================================
File: dashboardbackend/src/services/descope_service.py
================================================
"""
Descope service for user management and authentication
"""
import logging
from typing import Dict, List, Optional
import os
import aiohttp
import certifi
import ssl
from datetime import datetime
from tenacity import retry, stop_after_attempt, wait_fixed

logger = logging.getLogger(__name__)

class DescopeService:
    """Service for interacting with Descope API"""

    def __init__(self):
        """Initialize Descope service"""
        self.api_url = os.getenv('DESCOPE_API_URL', 'https://api.descope.com/v1/mgmt/user/search')
        self.bearer_token = os.getenv('DESCOPE_BEARER_TOKEN')
        
        # Configure SSL context
        self.ssl_context = ssl.create_default_context(cafile=certifi.where())
        self.ssl_context.check_hostname = True
        self.ssl_context.verify_mode = ssl.CERT_REQUIRED
        
        if not self.bearer_token:
            logger.warning("Missing Descope bearer token. Some features may be limited.")
        else:
            logger.info("Successfully initialized Descope service with bearer token")

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
    async def get_total_users(self, date: Optional[datetime] = None) -> int:
        """Get total number of users from Descope.
        
        Args:
            date: Optional datetime to filter users by registration date
            
        Returns:
            int: Total number of users
            
        Raises:
            Exception: If there's an error fetching users from Descope
        """
        try:
            if not self.bearer_token:
                logger.warning("Missing Descope bearer token, returning 0 users")
                return 0

            headers = {
                'Authorization': f'Bearer {self.bearer_token}',
                'Content-Type': 'application/json'
            }

            total_users = 0
            page_size = 100
            has_more = True
            page_number = 1

            while has_more:
                # Use empty filters to get all users with pagination
                body = {
                    "tenantIds": [],
                    "roleNames": [],
                    "customAttributes": {},
                    "limit": page_size,
                    "page": page_number
                }

                async with aiohttp.ClientSession() as session:
                    async with session.post(self.api_url, headers=headers, json=body, ssl=self.ssl_context) as response:
                        if response.status == 200:
                            data = await response.json()
                            users = data.get('users', [])
                            total_users += len(users)
                            
                            # Check if we have more pages
                            if len(users) < page_size:
                                has_more = False
                            else:
                                page_number += 1
                                logger.debug(f"Fetching page {page_number} of users")
                        else:
                            error_text = await response.text()
                            logger.error(f"Failed to get users from Descope. Status: {response.status}, Error: {error_text}")
                            return 0

            logger.info(f"Successfully fetched total users from Descope: {total_users}")
            return total_users

        except Exception as e:
            logger.error(f"Error getting total users from Descope: {str(e)}", exc_info=True)
            return 0

    async def get_active_users(self, start_date: datetime, end_date: datetime) -> int:
        """Get number of active users in date range"""
        if not self.bearer_token:
            logger.error("Missing Descope credentials")
            return 0

        try:
            headers = {
                'Authorization': f'Bearer {self.bearer_token}',
                'Content-Type': 'application/json'
            }
            
            query = {
                "pageSize": 1,
                "page": 1,
                "loginTime": {
                    "after": start_date.isoformat(),
                    "before": end_date.isoformat()
                }
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(f"{self.api_url}/activity", headers=headers, json=query, ssl=self.ssl_context) as response:
                    if response.status == 200:
                        data = await response.json()
                        active = data.get('totalUsers', 0)
                        logger.info(f"Successfully fetched active users from Descope: {active}")
                        return active
                    else:
                        error_text = await response.text()
                        logger.error(f"Failed to get active users from Descope. Status: {response.status}, Error: {error_text}")
                        return 0

        except Exception as e:
            logger.error(f"Error getting active users from Descope: {str(e)}")
            return 0

    async def get_user_details(self, user_ids: List[str]) -> Dict[str, Dict]:
        """Get user details from Descope.
        
        Args:
            user_ids: List of user IDs to fetch details for
            
        Returns:
            Dict[str, Dict]: Dictionary mapping user IDs to their details
            
        Raises:
            Exception: If there's an error fetching user details from Descope
        """
        try:
            if not self.bearer_token:
                logger.warning("Missing Descope bearer token, returning empty user details")
                return {}

            headers = {
                'Authorization': f'Bearer {self.bearer_token}',
                'Content-Type': 'application/json'
            }

            # Search for specific users by their IDs
            body = {
                "tenantIds": [],
                "roleNames": [],
                "customAttributes": {},
                "loginIds": user_ids,
                "limit": len(user_ids)
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(self.api_url, headers=headers, json=body, ssl=self.ssl_context) as response:
                    if response.status == 200:
                        data = await response.json()
                        users = data.get('users', [])
                        # Map user details by their ID
                        user_details = {user.get('loginIds', [''])[0]: user for user in users}
                        logger.info(f"Successfully fetched details for {len(user_details)} users")
                        return user_details
                    else:
                        error_text = await response.text()
                        logger.error(f"Failed to get user details from Descope. Status: {response.status}, Error: {error_text}")
                        return {}

        except Exception as e:
            logger.error(f"Error getting user details from Descope: {str(e)}", exc_info=True)
            return {}

================================================
File: dashboardbackend/src/services/dub_analytics.py
================================================
"""
Analytics service for user activity analysis
"""
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional
from opensearchpy import AsyncOpenSearch
import redis.asyncio as redis
import os
import ssl
import certifi
import logging

logger = logging.getLogger(__name__)

class AnalyticsService:
    """Analytics service for user activity"""
    
    def __init__(self, opensearch_client: AsyncOpenSearch, redis_client: redis.Redis):
        self.opensearch = opensearch_client
        self.redis = redis_client
        self.index = "events-v2"
        self.cache_ttl = timedelta(minutes=5)
        self.disable_cache = os.getenv('DISABLE_CACHE', 'false').lower() == 'true'
        self.request_timeout = int(os.getenv('MAX_QUERY_TIME', '30'))
        self.min_date = datetime(2024, 10, 1, tzinfo=timezone.utc)

        self._configure_opensearch()

    def _configure_opensearch(self):
        """Configure OpenSearch client with SSL and authentication"""
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        self.opensearch._ssl_context = ssl_context
        self.opensearch.use_ssl = True
        self.opensearch.verify_certs = False
        self.opensearch.ssl_assert_hostname = False
        self.opensearch.ssl_show_warn = True

    async def get_user_activity(self, start_date: datetime, end_date: datetime, filter_type: str) -> Dict[str, Any]:
        """Get user activity based on filter type"""
        logger.info(f"Getting user activity: start={start_date}, end={end_date}, filter={filter_type}")

        # Ensure dates are timezone-aware
        start_date = self._ensure_timezone(start_date)
        end_date = self._ensure_timezone(end_date)

        # Ensure no data before Oct 1st 2024
        if start_date < self.min_date:
            start_date = self.min_date

        cache_key = f"user_activity_{self._format_date_iso(start_date)}_{self._format_date_iso(end_date)}_{filter_type}"
        
        if not self.disable_cache:
            cached_data = await self._get_from_cache(cache_key)
            if cached_data:
                return cached_data

        try:
            # Build OpenSearch query
            must_conditions = [
                {"range": {"timestamp": {
                    "gte": int(start_date.timestamp() * 1000),
                    "lte": int(end_date.timestamp() * 1000)
                }}}
            ]

            # Add filter for successful events only
            must_conditions.append({"term": {"status.keyword": "succeeded"}})

            # Build aggregation for user activity
            aggs = {
                "users": {
                    "terms": {
                        "field": "trace_id.keyword",
                        "size": 10000
                    },
                    "aggs": {
                        "actions": {
                            "date_histogram": {
                                "field": "timestamp",
                                "calendar_interval": "day"
                            }
                        },
                        "first_action": {"min": {"field": "timestamp"}},
                        "last_action": {"max": {"field": "timestamp"}},
                        "user_email": {
                            "terms": {
                                "field": "email.keyword",
                                "size": 1
                            }
                        }
                    }
                }
            }

            query = {
                "query": {"bool": {"must": must_conditions}},
                "aggs": aggs,
                "size": 0
            }

            # Execute query
            result = await self.opensearch.search(
                index=self.index,
                body=query,
                request_timeout=self.request_timeout
            )

            # Process results
            users = []
            for bucket in result["aggregations"]["users"]["buckets"]:
                user_id = bucket["key"]
                email = bucket["user_email"]["buckets"][0]["key"] if bucket["user_email"]["buckets"] else "Unknown"
                first_action = bucket["first_action"]["value"]
                last_action = bucket["last_action"]["value"]
                total_actions = bucket["doc_count"]
                days_between = (last_action - first_action) / (1000 * 60 * 60 * 24)  # Convert ms to days

                # Filter users based on activity pattern
                include_user = False
                if filter_type == "consecutive_days":
                    # Check if user has actions on consecutive days
                    action_dates = set(
                        datetime.fromtimestamp(hit["key"] / 1000, tz=timezone.utc).date()
                        for hit in bucket["actions"]["buckets"]
                    )
                    consecutive_days = any(
                        date + timedelta(days=1) in action_dates
                        for date in action_dates
                    )
                    include_user = consecutive_days
                elif filter_type == "one_to_two_weeks":
                    include_user = 7 <= days_between <= 14
                elif filter_type == "two_to_three_weeks":
                    include_user = 14 < days_between <= 21
                elif filter_type == "month_apart":
                    include_user = days_between >= 28

                if include_user and total_actions >= 2:
                    users.append({
                        "trace_id": user_id,
                        "email": email,
                        "firstAction": datetime.fromtimestamp(first_action / 1000, tz=timezone.utc).isoformat(),
                        "lastAction": datetime.fromtimestamp(last_action / 1000, tz=timezone.utc).isoformat(),
                        "daysBetween": round(days_between, 1),
                        "totalActions": total_actions
                    })

            # Sort users by total actions descending
            users.sort(key=lambda x: x["totalActions"], reverse=True)

            response_data = {
                "status": "success",
                "users": users,
                "timeRange": {
                    "start": start_date.isoformat(),
                    "end": end_date.isoformat()
                }
            }

            if not self.disable_cache:
                await self._save_to_cache(cache_key, response_data)

            return response_data

        except Exception as e:
            logger.error(f"Error getting user activity: {str(e)}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }

    def _ensure_timezone(self, dt: datetime) -> datetime:
        """Ensure datetime has UTC timezone"""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt

    def _format_date_iso(self, dt: datetime) -> str:
        """Format datetime to ISO string"""
        return dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    async def _get_from_cache(self, key: str) -> Optional[Dict]:
        """Get data from Redis cache"""
        try:
            data = await self.redis.get(key)
            return data if data is None else data
        except Exception as e:
            logger.warning(f"Redis cache retrieval failed: {str(e)}")
            return None

    async def _save_to_cache(self, key: str, data: Dict) -> None:
        """Save data to Redis cache"""
        try:
            await self.redis.set(
                key,
                data,
                ex=int(self.cache_ttl.total_seconds())
            )
        except Exception as e:
            logger.warning(f"Redis cache storage failed: {str(e)}")

================================================
File: dashboardbackend/src/services/historical_data_service.py
================================================
"""
Historical data service for fetching and processing historical metrics
"""
from typing import Dict, Any, Optional
import os
import json
import logging
from datetime import datetime, timezone

logger = logging.getLogger(__name__)

class HistoricalDataService:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self._load_historical_data()

    def _load_historical_data(self):
        """Load historical data from JSON file"""
        try:
            data_file = os.path.join(os.path.dirname(__file__), '../data/historical_metrics.json')
            with open(data_file, 'r') as f:
                self.historical_data = json.load(f)
            
            # Convert date strings to datetime objects
            self.min_date = datetime.strptime(self.historical_data['metadata']['start_date'], '%Y-%m-%d').replace(tzinfo=timezone.utc)
            self.max_date = datetime.strptime(self.historical_data['metadata']['end_date'], '%Y-%m-%d').replace(tzinfo=timezone.utc)
            
            self.logger.info(f"Loaded historical data from {self.min_date} to {self.max_date}")
        except Exception as e:
            self.logger.error(f"Failed to load historical data: {e}")
            self.historical_data = None
            self.min_date = None
            self.max_date = None

    def _ensure_timezone(self, dt: datetime) -> datetime:
        """Ensure datetime is timezone-aware"""
        return dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)

    def _find_nearest_date(self, target_date: datetime, direction: str = 'before') -> Optional[str]:
        """Find nearest date in historical data
        
        Args:
            target_date: Target date to find nearest for
            direction: Either 'before' or 'after'
        
        Returns:
            Date string in YYYY-MM-DD format or None if not found
        """
        target_str = target_date.strftime('%Y-%m-%d')
        dates = sorted(self.historical_data['daily_metrics'].keys())
        
        # If exact date exists, return it
        if target_str in dates:
            return target_str
            
        # Find nearest date
        for i, date in enumerate(dates):
            if direction == 'before':
                if date > target_str:
                    return dates[i-1] if i > 0 else None
            else:
                if date >= target_str:
                    return date
                    
        # If we get here and want date before, return last date
        if direction == 'before':
            return dates[-1]
        return None

    def get_v1_metrics(self, start_date: datetime, end_date: datetime, include_v1: bool = True) -> Dict[str, Any]:
        """Get historical metrics for date range"""
        if not include_v1 or not self.historical_data:
            return {}

        # Ensure dates are timezone-aware
        start_date = self._ensure_timezone(start_date)
        end_date = self._ensure_timezone(end_date)

        # Find nearest dates in our data
        start_date_str = self._find_nearest_date(start_date, 'before')
        end_date_str = self._find_nearest_date(end_date, 'before')

        if not start_date_str or not end_date_str:
            self.logger.warning(f"No historical data found for date range: {start_date} to {end_date}")
            return {}

        # Get metrics for start and end dates
        start_metrics = self.historical_data['daily_metrics'][start_date_str]
        end_metrics = self.historical_data['daily_metrics'][end_date_str]

        self.logger.debug(f"Using metrics from {start_date_str} to {end_date_str}")
        self.logger.debug(f"Start metrics: {start_metrics}")
        self.logger.debug(f"End metrics: {end_metrics}")

        # Calculate growth
        total_days = (end_date - start_date).days + 1
        growth = {
            'total_users': end_metrics['total_users'] - start_metrics['total_users'],
            'active_users': end_metrics['active_users'] - start_metrics['active_users'],
            'producers': end_metrics['producers'] - start_metrics['producers']
        }

        # Calculate daily averages
        daily_averages = {
            metric: value / total_days if total_days > 0 else 0
            for metric, value in growth.items()
        }

        return {
            'daily_averages': daily_averages,
            'cumulative': growth,
            'previous': start_metrics
        }

================================================
File: dashboardbackend/src/services/metrics_service.py
================================================
"""
Consolidated metrics service for analytics
"""
from typing import Dict, Any
import logging
from datetime import datetime
from src.utils.query_builder import OpenSearchQueryBuilder
from src.services.analytics.metrics.base import BaseMetricsService
from src.services.descope_service import DescopeService
from src.services.caching_service import CachingService

logger = logging.getLogger(__name__)

class AnalyticsMetricsService(BaseMetricsService):
    def __init__(self, opensearch_client, caching_service: CachingService, query_builder: OpenSearchQueryBuilder, index: str, timestamp_field: str, request_timeout: int, descope_service: DescopeService):
        super().__init__(opensearch_client, query_builder, index, timestamp_field, request_timeout, descope_service)
        self.caching_service = caching_service

    async def get_thread_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get count of users with message threads in time range"""
        logger.info(f"Fetching thread users for date range: {start_date} to {end_date}")
        query = self._get_date_range_query(start_date, end_date, "handleMessageInThread_start")
        result = await self._execute_query(query, "thread users", start_date, end_date, "engagement")
        logger.info(f"Thread users result: {result}")
        return result

    async def get_medium_chat_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get users with 5-20 message threads in time range"""
        logger.info(f"Fetching medium chat users for date range: {start_date} to {end_date}")
        query = self._get_date_range_query(start_date, end_date, "handleMessageInThread_start")
        query.update(self._build_thread_count_aggregation(min_count=5, max_count=20))
        result = await self._execute_query(query, "medium chat users", start_date, end_date, "engagement")
        logger.info(f"Medium chat users result: {result}")
        return result

    async def get_power_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get users with more than 20 message threads in time range"""
        logger.info(f"Fetching power users for date range: {start_date} to {end_date}")
        query = self._get_date_range_query(start_date, end_date, "handleMessageInThread_start")
        query.update(self._build_thread_count_aggregation(min_count=21))
        result = await self._execute_query(query, "power users", start_date, end_date, "engagement")
        logger.info(f"Power users result: {result}")
        return result

    async def get_total_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get total number of users in time range"""
        logger.info(f"Fetching total users for date range: {start_date} to {end_date}")
        query = self._get_date_range_query(start_date, end_date)
        result = await self._execute_query(query, "total users", start_date, end_date, "user")
        logger.info(f"Total users result: {result}")
        return result

    async def get_producers(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get count of producers in time range"""
        logger.info(f"Fetching producers for date range: {start_date} to {end_date}")
        query = self._get_date_range_query(start_date, end_date, "producer_activity")
        result = await self._execute_query(query, "producers", start_date, end_date, "user")
        logger.info(f"Producers result: {result}")
        return result

    async def get_sketch_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get count of users who have uploaded sketches in time range"""
        logger.info(f"Fetching sketch users for date range: {start_date} to {end_date}")
        query = self._get_date_range_query(start_date, end_date, "uploadSketch_end")
        result = await self._execute_query(query, "sketch users", start_date, end_date, "performance")
        logger.info(f"Sketch users result: {result}")
        return result

    async def get_render_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get count of users who have completed renders in time range"""
        logger.info(f"Fetching render users for date range: {start_date} to {end_date}")
        query = self._get_date_range_query(start_date, end_date, "renderStart_end")
        result = await self._execute_query(query, "render users", start_date, end_date, "performance")
        logger.info(f"Render users result: {result}")
        return result

    async def fetch_metrics(self, start_date: datetime, end_date: datetime) -> None:
        """Fetch all metrics at once"""
        logger.info(f"Fetching all metrics for date range: {start_date} to {end_date}")
        start_date, end_date = self._validate_dates(start_date, end_date)
        self.thread_users = await self.get_thread_users(start_date, end_date)
        self.sketch_users = await self.get_sketch_users(start_date, end_date)
        self.render_users = await self.get_render_users(start_date, end_date)
        self.medium_chat_users = await self.get_medium_chat_users(start_date, end_date)
        self.power_users = await self.get_power_users(start_date, end_date)
        self.total_users = await self.get_total_users(start_date, end_date)
        self.producers = await self.get_producers(start_date, end_date)
        logger.info("Finished fetching all metrics")

    def get_metrics(self) -> Dict[str, Dict[str, Any]]:
        """Get all metrics"""
        metrics = {
            "thread_users": self.thread_users,
            "sketch_users": self.sketch_users,
            "render_users": self.render_users,
            "medium_chat_users": self.medium_chat_users,
            "power_users": self.power_users,
            "total_users": self.total_users,
            "producers": self.producers
        }
        logger.info(f"Returning metrics: {metrics}")
        return metrics

    def combine_with_historical_data(self, current_data: Dict[str, Any], historical_data: Dict[str, Any]) -> Dict[str, Any]:
        """Combine current metrics with historical data"""
        combined_data = current_data.copy()
        combined_data["value"] += historical_data.get("value", 0)
        combined_data["daily_average"] += historical_data.get("daily_average", 0)
        logger.info(f"Combined data: {combined_data}")
        return combined_data

================================================
File: dashboardbackend/src/services/opensearch_service.py
================================================
"""
OpenSearch service for querying and aggregating analytics data
"""
import os
import logging
import ssl
import certifi
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
from opensearchpy import AsyncOpenSearch, ConnectionError, TransportError
import pytz
from dateutil import tz
from datetime import timezone

logger = logging.getLogger(__name__)

class OpenSearchService:
    def __init__(self):
        self.index = "events-v2"
        self.timestamp_field = "timestamp"
        self.request_timeout = int(os.getenv('MAX_QUERY_TIME', '30'))  # Request timeout in seconds
        self.max_retries = 3
        self.base_delay = 1  # Base delay in seconds

        # Configure OpenSearch client
        self.opensearch_url = os.getenv('OPENSEARCH_URL', 'https://localhost:9200')
        self.opensearch_username = os.getenv('OPENSEARCH_USERNAME')
        self.opensearch_password = os.getenv('OPENSEARCH_PASSWORD')
        
        # Configure SSL context for OpenSearch
        self.ssl_context = ssl.create_default_context(cafile=certifi.where())
        self.ssl_context.check_hostname = False  # Disable hostname checking for localhost
        self.ssl_context.verify_mode = ssl.CERT_NONE  # Allow self-signed certificates for local development
        
        # Initialize OpenSearch client
        self.client = AsyncOpenSearch(
            hosts=[self.opensearch_url],
            http_auth=(self.opensearch_username, self.opensearch_password),
            use_ssl=True,
            verify_certs=False,
            ssl_assert_hostname=False,
            ssl_show_warn=False,
            ssl_context=self.ssl_context
        )

    async def verify_connection(self) -> bool:
        """Verify the connection to OpenSearch and the existence of the index"""
        try:
            # Check if we can connect to OpenSearch
            info = await self.client.info()
            logger.info(f"Successfully connected to OpenSearch cluster: {info['cluster_name']}")

            # Check if the index exists
            index_exists = await self.client.indices.exists(index=self.index)
            if index_exists:
                logger.info(f"Index '{self.index}' exists")
                return True
            else:
                logger.error(f"Index '{self.index}' does not exist")
                return False
        except ConnectionError as e:
            logger.error(f"Failed to connect to OpenSearch: {str(e)}")
            return False
        except Exception as e:
            logger.error(f"Error verifying OpenSearch connection: {str(e)}")
            return False

    async def _execute_with_retry(self, operation):
        """Execute an OpenSearch operation with exponential backoff retry."""
        for attempt in range(self.max_retries):
            try:
                return await operation()
            except (ConnectionError, TransportError) as e:
                if attempt == self.max_retries - 1:
                    raise e
                delay = self.base_delay * (2**attempt)  # Exponential backoff
                await asyncio.sleep(delay)

    async def search(self, query: Dict[str, Any], size: int = 0) -> Dict[str, Any]:
        """Execute a search query on OpenSearch"""
        logger.debug(f"Executing OpenSearch query: index={self.index}, query={query}, size={size}")
        
        async def execute():
            return await self.client.search(
                index=self.index,
                body=query,
                size=size,
                request_timeout=self.request_timeout
            )

        try:
            result = await self._execute_with_retry(execute)
            logger.debug(f"OpenSearch query result: {result}")
            return result
        except Exception as e:
            logger.error(f"Error executing OpenSearch query: {str(e)}", exc_info=True)
            raise

    async def get_user_counts(self, start_date: datetime, end_date: datetime, event_name: str) -> Dict[str, int]:
        """Get counts of users who performed a specific event"""
        logger.debug(f"Getting user counts for event: {event_name}, start_date: {start_date}, end_date: {end_date}")
        
        # Ensure dates are in UTC
        if start_date.tzinfo is None:
            start_date = start_date.astimezone()
        if end_date.tzinfo is None:
            end_date = end_date.astimezone()
            
        start_utc = start_date.astimezone(pytz.utc)
        end_utc = end_date.astimezone(pytz.utc)
        
        # Convert to milliseconds
        start_ms = int(start_utc.timestamp() * 1000)
        end_ms = int(end_utc.timestamp() * 1000)
        
        logger.debug(f"Converted timestamps - Start UTC: {start_utc.isoformat()}, End UTC: {end_utc.isoformat()}")
        logger.debug(f"Millisecond timestamps - Start: {start_ms}, End: {end_ms}")
        
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"event_name.keyword": event_name}},
                        {"range": {"timestamp": {"gte": start_ms, "lte": end_ms}}}
                    ]
                }
            },
            "aggs": {
                "users": {
                    "terms": {
                        "field": "trace_id.keyword",
                        "size": 10000
                    }
                }
            }
        }
        
        logger.debug(f"Executing OpenSearch query: index={self.index}, query={query}, size=0")
        
        try:
            response = await self.client.search(
                index=self.index,
                body=query,
                size=0
            )
            logger.debug(f"OpenSearch query result: {response}")
            
            # Extract user counts from aggregation buckets
            user_counts = {
                bucket["key"]: bucket["doc_count"]
                for bucket in response["aggregations"]["users"]["buckets"]
            }
            logger.debug(f"User counts result: {user_counts}")
            return user_counts
            
        except Exception as e:
            logger.error(f"Error executing OpenSearch query: {str(e)}")
            return {}

    async def get_user_events(self, trace_id: str, start_date: datetime, end_date: datetime) -> List[Dict[str, Any]]:
        """Fetch user events based on trace_id"""
        logger.debug(f"Getting user events for trace_id: {trace_id}, start_date: {start_date}, end_date: {end_date}")
        time_filter = {
            "range": {
                self.timestamp_field: {
                    "gte": self._format_date_os(start_date),
                    "lte": self._format_date_os(end_date)
                }
            }
        }

        query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"trace_id.keyword": trace_id}},
                        time_filter
                    ]
                }
            },
            "sort": [{self.timestamp_field: {"order": "desc"}}]
        }

        try:
            result = await self.search(query, size=100)  # Limit to 100 most recent events
            events = [hit["_source"] for hit in result["hits"]["hits"]]
            logger.debug(f"User events result: {events}")
            return events
        except Exception as e:
            logger.error(f"Error fetching user events: {str(e)}", exc_info=True)
            return []

    async def get_producers_count(self, date: Optional[datetime] = None) -> int:
        """Get count of unique producers up to a specific date"""
        logger.info("Starting get_producers_count")
        logger.info(f"Executing producers search query on index: {self.index}")
        
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"event_name.keyword": "uploadSketch_end"}}
                    ]
                }
            }
        }

        # Add date filter if specified
        if date:
            date_utc = date.astimezone(pytz.utc)
            date_ms = int(date_utc.timestamp() * 1000)
            query["query"]["bool"]["must"].append({
                "range": {
                    "timestamp": {
                        "lte": date_ms
                    }
                }
            })
        
        query["aggs"] = {
            "unique_producers": {
                "cardinality": {
                    "field": "trace_id.keyword"
                }
            }
        }
        
        logger.debug(f"Executing OpenSearch query: index={self.index}, query={query}, size=0")
        
        try:
            response = await self.client.search(
                index=self.index,
                body=query,
                size=0
            )
            logger.debug(f"OpenSearch query result: {response}")
            
            producers_count = response["aggregations"]["unique_producers"]["value"]
            logger.info(f"Found {producers_count} producers")
            return producers_count
            
        except Exception as e:
            logger.error(f"Error executing producers query: {str(e)}")
            return 0

    async def list_event_names(self) -> List[str]:
        """List all event names in OpenSearch"""
        query = {
            "size": 0,
            "aggs": {
                "event_names": {
                    "terms": {
                        "field": "event_name.keyword",
                        "size": 100
                    }
                }
            }
        }
        
        try:
            result = await self.client.search(
                index=self.index,
                body=query
            )
            logger.debug(f"Event names query result: {result}")
            
            buckets = result.get("aggregations", {}).get("event_names", {}).get("buckets", [])
            event_names = [bucket["key"] for bucket in buckets]
            logger.info(f"Found event names: {event_names}")
            return event_names
            
        except Exception as e:
            logger.error(f"Error listing event names: {e}")
            return []

    async def get_metrics(self, start_date: datetime, end_date: datetime) -> Dict[str, int]:
        """Get metrics from OpenSearch"""
        # Check if date range is before our data starts
        opensearch_start = datetime(2025, 1, 20, tzinfo=timezone.utc)
        if end_date < opensearch_start:
            logger.debug(f"Date range {start_date.isoformat()} to {end_date.isoformat()} is before OpenSearch data starts ({opensearch_start.isoformat()})")
            return {
                "thread_users_count": 0,
                "render_users": 0,
                "producers_count": 0
            }
            
        # Adjust start date if needed
        if start_date < opensearch_start:
            logger.debug(f"Adjusting start date from {start_date.isoformat()} to {opensearch_start.isoformat()}")
            start_date = opensearch_start

        # List all event names first
        event_names = await self.list_event_names()
        logger.info(f"Available event names: {event_names}")
        
        # Get thread users (active users) - try different event names
        thread_users = {}
        thread_event_names = ["handleMessageInThread_start", "threadStart", "thread_start"]
        for event_name in thread_event_names:
            if event_name in event_names:
                users = await self.get_user_counts(event_name, start_date, end_date)
                thread_users.update(users)
        
        # Get producers count - this might come from a specific event or tag
        producers = await self.get_producers_count(end_date)
        
        logger.debug(f"OpenSearch metrics: thread_users={len(thread_users)}, producers={producers}")
        
        return {
            "thread_users_count": len(thread_users),
            "producers_count": producers
        }

    def _format_date_os(self, dt: datetime) -> int:
        """Convert datetime to OpenSearch timestamp (milliseconds)"""
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        
        # Convert to milliseconds since epoch
        timestamp_ms = int(dt.timestamp() * 1000)
        
        # Add debug logging
        logger.debug(f"Converting {dt.isoformat()} to timestamp: {timestamp_ms}")
        
        # Validate timestamp is in reasonable range
        current_time_ms = int(time.time() * 1000)
        if timestamp_ms > current_time_ms:
            logger.warning(f"Generated timestamp {timestamp_ms} is in the future! Using current time instead.")
            timestamp_ms = current_time_ms
            
        return timestamp_ms

================================================
File: dashboardbackend/src/services/analytics/queries.py
================================================
"""OpenSearch query builder utility"""
from typing import Dict, Optional
from datetime import datetime
from dateutil.parser import parse


class OpenSearchQueryBuilder:
    def build_date_range_query(self, start_time: str, end_time: str) -> Dict:
        """
        Build a date range query for OpenSearch.

        Args:
            start_time: ISO format timestamp
            end_time: ISO format timestamp

        Returns:
            Dict containing the date range query
        """
        return {"range": {"timestamp": {"gte": start_time, "lte": end_time}}}

    def build_aggregation_query(
        self, agg_field: str, interval: Optional[str] = None
    ) -> Dict:
        """
        Build an aggregation query for OpenSearch.

        Args:
            agg_field: Field to aggregate on
            interval: Time interval for date histogram aggregation ('hour', 'day', 'week', 'month')

        Returns:
            Dict containing the aggregation query
        """
        # Define interval mappings and types
        fixed_intervals = {"hour": "1h", "day": "1d"}
        calendar_intervals = {"week": "week", "month": "month"}

        aggs = {}

        if interval:
            date_histogram = {
                "field": "timestamp",
                "min_doc_count": 0,
                "format": "yyyy-MM-dd'T'HH:mm:ssZ",
            }

            if interval in fixed_intervals:
                date_histogram["fixed_interval"] = fixed_intervals[interval]
            elif interval in calendar_intervals:
                date_histogram["calendar_interval"] = calendar_intervals[interval]
            else:
                # Default to 1d fixed interval
                date_histogram["fixed_interval"] = "1d"

            aggs["time_buckets"] = {"date_histogram": date_histogram}

        aggs[f"{agg_field}_buckets"] = {"terms": {"field": agg_field, "size": 10000}}

        return {"aggs": aggs}

    def build_paginated_query(
        self, search_after: Optional[str] = None, size: int = 100
    ) -> Dict:
        """
        Build a paginated query using search_after for deep pagination.

        Args:
            search_after: Token for pagination
            size: Number of results per page

        Returns:
            Dict containing the pagination parameters
        """
        query = {
            "sort": [{"timestamp": "desc"}, {"_id": "desc"}],
            "size": min(size, 1000),  # Enforce maximum page size
        }

        if search_after:
            query["search_after"] = search_after.split(",")

        return query

    def build_composite_query(
        self,
        must_conditions: list,
        source_fields: Optional[list] = None,
        aggregations: Optional[Dict] = None,
        pagination: Optional[Dict] = None,
    ) -> Dict:
        """
        Build a complete composite query combining multiple conditions.

        Args:
            must_conditions: List of must conditions for bool query
            source_fields: List of fields to include in _source
            aggregations: Aggregation queries
            pagination: Pagination parameters

        Returns:
            Dict containing the complete query
        """
        query = {"query": {"bool": {"must": must_conditions}}}

        if source_fields:
            query["_source"] = source_fields

        if aggregations:
            query.update(aggregations)

        if pagination:
            query.update(pagination)

        return query

================================================
File: dashboardbackend/src/services/analytics/metrics/__init__.py
================================================
"""
Entry point for metrics analytics
"""
from .base import (
    ensure_timezone,
    calculate_delta,
    create_metric_object,
    get_empty_metric,
    format_date_iso,
    BaseMetricsService
)

__all__ = [
    'BaseMetricsService',
    'ensure_timezone',
    'calculate_delta',
    'create_metric_object',
    'get_empty_metric',
    'format_date_iso'
]

================================================
File: dashboardbackend/src/services/analytics/metrics/base.py
================================================
"""
Base class for metrics analytics
"""
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List, Tuple, Optional
from opensearchpy import AsyncOpenSearch, NotFoundError, RequestError

from src.utils.query_builder import OpenSearchQueryBuilder
from src.services.descope_service import DescopeService
from src.services.historical_data_service import HistoricalDataService
from src.services.analytics.metrics.utils import (
    ensure_timezone,
    calculate_delta,
    create_metric_object,
    get_empty_metric,
    format_date_iso
)

logger = logging.getLogger(__name__)

class BaseMetricsService:
    """Base class for all metrics calculations"""

    def __init__(self, 
                 opensearch_client: AsyncOpenSearch, 
                 query_builder: OpenSearchQueryBuilder, 
                 index: str,
                 timestamp_field: str,
                 request_timeout: int,
                 descope_service: DescopeService):
        self.opensearch = opensearch_client
        self.query_builder = query_builder
        self.index = index
        self.timestamp_field = timestamp_field
        self.request_timeout = request_timeout
        self.descope = descope_service
        self.history = HistoricalDataService()
        self.min_date = datetime(2024, 10, 1, tzinfo=timezone.utc)
        self.descope_start_date = datetime(2025, 1, 27, tzinfo=timezone.utc)

    async def _execute_opensearch_query(self, query: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """Execute OpenSearch query with error handling"""
        try:
            logger.debug(f"Executing OpenSearch query: {query}")
            result = await self.opensearch.search(query=query, size=0)
            logger.debug(f"OpenSearch query result: {result}")
            return result
        except NotFoundError:
            logger.error(f"{error_message}: Index not found", exc_info=True)
            return self._get_empty_result()
        except RequestError as e:
            logger.error(f"{error_message}: Invalid query - {str(e)}", exc_info=True)
            return self._get_empty_result()
        except Exception as e:
            logger.error(f"{error_message}: {str(e)}", exc_info=True)
            return self._get_empty_result()

    def _get_date_range_query(self, start_date: datetime, end_date: datetime, event_name: Optional[str] = None) -> Dict[str, Any]:
        """Build date range query for OpenSearch"""
        must_conditions = [self.query_builder.build_date_range_query(
            self._format_date_os(start_date),
            self._format_date_os(end_date)
        )]
        if event_name:
            must_conditions.append({"term": {"event_name.keyword": event_name}})

        return self.query_builder.build_composite_query(
            must_conditions=must_conditions,
            aggregations={
                "aggs": {
                    "unique_users": {"cardinality": {"field": "trace_id.keyword"}}
                }
            }
        )

    async def _get_user_details(self, users: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Get user details from Descope"""
        if not users:
            return []

        try:
            user_ids = [user["trace_id"] for user in users]
            user_details = await self.descope.get_user_details(user_ids)

            # Merge Descope details with user stats
            detailed_users = []
            for user in users:
                details = user_details.get(user["trace_id"], {})
                detailed_user = {
                    **user,
                    "email": details.get("email", "Unknown"),
                    "name": details.get("name", "Unknown"),
                    "lastLoginTime": details.get("lastLoginTime", ""),
                    "createdTime": details.get("createdTime", "")
                }
                detailed_users.append(detailed_user)

            return detailed_users
        except Exception as e:
            logger.error(f"Error getting user details: {str(e)}", exc_info=True)
            return users  # Return original users without details rather than empty list

    def calculate_daily_average(self, total: float, start_date: datetime, end_date: datetime) -> float:
        """Calculate daily average for a metric"""
        days_in_range = (end_date - start_date).days + 1
        return total / days_in_range if days_in_range > 0 else 0

    def _validate_dates(self, start_date: datetime, end_date: datetime) -> Tuple[datetime, datetime]:
        """Validate and adjust date range"""
        start_date = ensure_timezone(start_date)
        end_date = ensure_timezone(end_date)

        if start_date < self.min_date:
            logger.info(f"Adjusting start date from {start_date} to min date {self.min_date}")
            start_date = self.min_date

        if start_date > end_date:
            logger.warning(f"Start date {start_date} is after end date {end_date}, swapping dates")
            start_date, end_date = end_date, start_date

        return start_date, end_date

    def _format_date_os(self, dt: datetime) -> int:
        """Format datetime for OpenSearch timestamp"""
        return int(dt.timestamp() * 1000)

    def _get_empty_result(self) -> Dict[str, Any]:
        """Get empty result structure for OpenSearch queries"""
        return {
            "aggregations": {
                "unique_users": {"value": 0},
                "users": {"buckets": []},
                "thread_count": {"buckets": []}
            }
        }

    def _build_user_aggregation(self, size: int = 10000) -> Dict[str, Any]:
        """Build user aggregation for OpenSearch queries"""
        return {
            "aggs": {
                "users": {
                    "terms": {
                        "field": "trace_id.keyword",
                        "size": size
                    }
                }
            }
        }

    def _build_thread_count_aggregation(self, min_count: Optional[int] = None, max_count: Optional[int] = None) -> Dict[str, Any]:
        """Build thread count aggregation for OpenSearch queries"""
        logger.info(f"Building thread count aggregation with min_count={min_count}, max_count={max_count}")
        aggs = {
            "thread_count": {
                "terms": {
                    "field": "trace_id.keyword",
                    "size": 10000
                },
                "aggs": {
                    "thread_count": {"value_count": {"field": "thread_id.keyword"}}
                }
            }
        }

        if min_count is not None or max_count is not None:
            script_parts = []
            if min_count is not None:
                script_parts.append(f"params.thread_count >= {min_count}")
            if max_count is not None:
                script_parts.append(f"params.thread_count <= {max_count}")

            aggs["thread_count"]["aggs"]["thread_filter"] = {
                "bucket_selector": {
                    "buckets_path": {"thread_count": "thread_count"},
                    "script": " && ".join(script_parts)
                }
            }

        logger.debug(f"Thread count aggregation: {aggs}")
        return {"aggs": aggs}

    def get_date_range(self, start_date: datetime, end_date: datetime) -> Dict[str, str]:
        """Get the date range for the metrics"""
        return {
            "start": format_date_iso(start_date),
            "end": format_date_iso(end_date)
        }

    def create_metric(self, value: float, previous_value: float, start_date: datetime, end_date: datetime, category: str) -> Dict[str, Any]:
        """Create a metric object with calculated daily average"""
        daily_average = self.calculate_daily_average(value, start_date, end_date)
        return create_metric_object(value, previous_value, daily_average, category, value)

    async def _execute_query(self, query: Dict[str, Any], metric_name: str, start_date: datetime, end_date: datetime, category: str) -> Dict[str, Any]:
        try:
            logger.info(f"Executing query for {metric_name}")
            logger.debug(f"Query: {query}")
            result = await self._execute_opensearch_query(query, f"Error getting {metric_name}")
            logger.debug(f"Raw result for {metric_name}: {result}")
            count = 0
        
            # Extract count from aggregations
            if "aggregations" in result:
                aggs = result["aggregations"]
                logger.debug(f"Aggregations for {metric_name}: {aggs}")
                if "unique_users" in aggs:
                    count = aggs["unique_users"]["value"]
                    logger.debug(f"{metric_name} unique users count: {count}")
                elif "users" in aggs and "buckets" in aggs["users"]:
                    count = len(aggs["users"]["buckets"])
                    logger.debug(f"{metric_name} users bucket count: {count}")
                elif "thread_count" in aggs and "buckets" in aggs["thread_count"]:
                    count = len([b for b in aggs["thread_count"]["buckets"] 
                            if "thread_filter" not in b or b["thread_count"]["value"] > 0])
                    logger.debug(f"{metric_name} thread count: {count}")
                    logger.debug(f"Thread count buckets: {aggs['thread_count']['buckets']}")
                else:
                    logger.warning(f"Unexpected aggregation structure for {metric_name}: {aggs}")
            else:
                logger.warning(f"No aggregations found in result for {metric_name}")

            logger.info(f"{metric_name} count: {count}")
            metric = self.create_metric(count, 0, start_date, end_date, category)
            logger.debug(f"Created metric for {metric_name}: {metric}")
            return metric
        except Exception as e:
            logger.error(f"Error getting {metric_name}: {str(e)}", exc_info=True)
            return self.create_metric(0, 0, start_date, end_date, category)

================================================
File: dashboardbackend/src/services/analytics/metrics/utils.py
================================================
"""
Utility functions for metrics calculations
"""
import logging
from datetime import datetime, timezone
from typing import Dict, Any, Union

logger = logging.getLogger(__name__)

def ensure_timezone(dt: datetime) -> datetime:
    """Ensure datetime has UTC timezone"""
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    return dt

def calculate_delta(curr_value: int, prev_value: int, daily_average: float = 0) -> Dict[str, Any]:
    """Calculate delta between current and previous values"""
    delta = curr_value - prev_value
    
    return {
        "value": curr_value,
        "previousValue": prev_value,
        "trend": "up" if delta > 0 else "down" if delta < 0 else "neutral",
        "changePercentage": round((delta / prev_value * 100) if prev_value > 0 else 100 if delta > 0 else 0, 2),
        "daily_average": daily_average
    }

def create_metric_object(
    metric_id: str,
    name: str,
    description: str,
    category: str,
    current_value: Union[int, Dict[str, Any]],
    v1_value: int = 0
) -> Dict[str, Any]:
    """Create a standardized metric object"""
    logger.debug(f"Creating metric object for {metric_id}")
    logger.debug(f"Input - current_value: {current_value}, v1_value: {v1_value}")

    if isinstance(current_value, int):
        current_value = {
            "value": current_value,
            "previousValue": 0,
            "trend": "neutral",
            "changePercentage": 0,
            "daily_average": 0
        }

    total_value = current_value["value"] + v1_value
    previous_value = current_value.get("previousValue", 0)
    days_in_range = current_value.get("days_in_range", 1)
    daily_average = current_value.get("daily_average", total_value / days_in_range if days_in_range > 0 else 0)

    # Calculate trend and change percentage
    if total_value > previous_value:
        trend = "up"
        change_percentage = (total_value - previous_value) / previous_value * 100 if previous_value > 0 else 100
    elif total_value < previous_value:
        trend = "down"
        change_percentage = (previous_value - total_value) / previous_value * 100 if previous_value > 0 else 100
    else:
        trend = "stable"
        change_percentage = 0

    metric = {
        "id": metric_id,
        "name": name,
        "description": description,
        "category": category,
        "interval": "daily",
        "data": {
            "value": total_value,
            "previousValue": previous_value,
            "trend": trend,
            "changePercentage": round(change_percentage, 2),
            "daily_average": daily_average
        }
    }

    logger.debug(f"Created metric object: {metric}")
    return metric

def get_empty_metric() -> Dict[str, Any]:
    """Return empty metric structure"""
    return {
        "value": 0,
        "previousValue": 0,
        "trend": "neutral",
        "daily_average": 0,
        "changePercentage": 0
    }

def format_date_iso(dt: datetime) -> str:
    """Format datetime to ISO string"""
    return dt.strftime("%Y-%m-%dT%H:%M:%SZ")

================================================
File: dashboardbackend/src/utils/query_builder.py
================================================
"""OpenSearch query builder utility"""
from typing import Dict, Optional, List, Any, Union
from datetime import datetime
from dateutil.parser import parse


class OpenSearchQueryBuilder:
    def build_date_range_query(self, start_time: Union[int, str], end_time: Union[int, str]) -> Dict:
        """
        Build a date range query for OpenSearch.

        Args:
            start_time: Timestamp in milliseconds since epoch or ISO format string
            end_time: Timestamp in milliseconds since epoch or ISO format string

        Returns:
            Dict containing the date range query
        """
        return {"range": {"timestamp": {"gte": start_time, "lte": end_time}}}

    def build_aggregation_query(
        self, agg_field: str, interval: Optional[str] = None
    ) -> Dict:
        """
        Build an aggregation query for OpenSearch.

        Args:
            agg_field: Field to aggregate on
            interval: Time interval for date histogram aggregation ('hour', 'day', 'week', 'month')

        Returns:
            Dict containing the aggregation query
        """
        # Define interval mappings and types
        fixed_intervals = {"hour": "1h", "day": "1d"}
        calendar_intervals = {"week": "week", "month": "month"}

        aggs = {}

        if interval:
            date_histogram = {
                "field": "timestamp",
                "min_doc_count": 0,
                "format": "yyyy-MM-dd'T'HH:mm:ssZ",
            }

            if interval in fixed_intervals:
                date_histogram["fixed_interval"] = fixed_intervals[interval]
            elif interval in calendar_intervals:
                date_histogram["calendar_interval"] = calendar_intervals[interval]
            else:
                # Default to 1d fixed interval
                date_histogram["fixed_interval"] = "1d"

            aggs["time_buckets"] = {"date_histogram": date_histogram}

        aggs[f"{agg_field}_buckets"] = {"terms": {"field": agg_field, "size": 10000}}

        return {"aggs": aggs}

    def build_paginated_query(
        self, search_after: Optional[str] = None, size: int = 100
    ) -> Dict:
        """
        Build a paginated query using search_after for deep pagination.

        Args:
            search_after: Token for pagination
            size: Number of results per page

        Returns:
            Dict containing the pagination parameters
        """
        query = {
            "sort": [{"timestamp": "desc"}, {"_id": "desc"}],
            "size": min(size, 1000),  # Enforce maximum page size
        }

        if search_after:
            query["search_after"] = search_after.split(",")

        return query

    def build_composite_query(
        self,
        must_conditions: List[Dict[str, Any]],
        source_fields: Optional[List[str]] = None,
        aggregations: Optional[Dict[str, Any]] = None,
        pagination: Optional[Dict[str, Any]] = None,
        sort: Optional[List[Dict[str, Any]]] = None,
    ) -> Dict[str, Any]:
        """
        Build a complete composite query combining multiple conditions.

        Args:
            must_conditions: List of must conditions for bool query
            source_fields: List of fields to include in _source
            aggregations: Aggregation queries
            pagination: Pagination parameters
            sort: List of sort conditions

        Returns:
            Dict containing the complete query
        """
        query = {"query": {"bool": {"must": must_conditions}}}

        if source_fields:
            query["_source"] = source_fields

        if aggregations:
            query.update(aggregations)

        if pagination:
            query.update(pagination)

        if sort:
            query["sort"] = sort

        return query

