# Combined Backend Files



# File: dashboardbackend/src/services/analytics_service.py

"""
AnalyticsService: Core service for fetching and aggregating analytics data
"""
from typing import Dict, Any, List
import logging
from datetime import datetime, timedelta
import os
from src.services.metrics_service import AnalyticsMetricsService
from src.services.descope_service import DescopeService
from src.services.opensearch_service import OpenSearchService
from src.services.historical_data_service import HistoricalDataService
from src.services.caching_service import CachingService
from src.utils.query_builder import OpenSearchQueryBuilder

logger = logging.getLogger(__name__)

class AnalyticsService:
    def __init__(self, caching_service: CachingService, opensearch_service: OpenSearchService, query_builder: OpenSearchQueryBuilder, descope_service: DescopeService):
        self.caching_service = caching_service
        self.disable_cache = os.getenv('DISABLE_CACHE', 'false').lower() == 'true'
        self.opensearch_service = opensearch_service
        self.descope_service = descope_service
        self.historical_data_service = HistoricalDataService()
        self.analytics_metrics = AnalyticsMetricsService(
            self.opensearch_service,
            self.caching_service,
            query_builder,
            os.getenv('OPENSEARCH_INDEX', 'your_index_name'),
            os.getenv('OPENSEARCH_TIMESTAMP_FIELD', 'timestamp'),
            int(os.getenv('OPENSEARCH_REQUEST_TIMEOUT', '30')),
            self.descope_service
        )

    async def get_dashboard_metrics(self, start_date: datetime, end_date: datetime, include_v1: bool = False) -> Dict[str, Any]:
        """Get all metrics needed for the dashboard"""
        cache_key = f"dashboard_metrics_{self._format_date_iso(start_date)}_{self._format_date_iso(end_date)}_{include_v1}"
        
        if not self.disable_cache:
            cached_data = await self.caching_service.get(cache_key)
            if cached_data and self._are_metrics_valid(cached_data.get('metrics', [])):
                logger.info("Returning cached dashboard metrics")
                return cached_data

        try:
            # Get metrics first
            await self.analytics_metrics.fetch_metrics(start_date, end_date)
            metrics_data = self.analytics_metrics.get_metrics()
            
            # Fetch total users with fallback
            try:
                total_users = await self.descope_service.get_total_users(start_date, end_date)
            except Exception as e:
                logger.error(f"Error fetching total users: {str(e)}")
                total_users = {"value": 0, "previousValue": 0, "trend": "neutral", "changePercentage": 0, "daily_average": 0}

            # Only get and add V1 data if requested
            if include_v1:
                try:
                    v1_data = self.historical_data_service.get_v1_metrics(start_date, end_date, include_v1)
                    total_users["value"] += v1_data["total_users"]
                    metrics_data["thread_users"]["value"] += v1_data["active_users"]
                    metrics_data["producers"]["value"] += v1_data["producers"]
                except Exception as e:
                    logger.error(f"Error fetching V1 metrics: {str(e)}")

            metrics = [
                self._create_metric("descope_users", "Total Users", "Total number of registered users", "user", total_users),
                self._create_metric("thread_users", "Thread Users", "Users who have started at least one message thread", "engagement", metrics_data["thread_users"]),
                self._create_metric("render_users", "Render Users", "Users who have completed at least one render", "performance", metrics_data["render_users"]),
                self._create_metric("active_chat_users", "Power Users", "Users with more than 20 message threads", "engagement", metrics_data["power_users"]),
                self._create_metric("medium_chat_users", "Medium Activity Users", "Users with 5-20 message threads", "engagement", metrics_data["medium_chat_users"]),
                self._create_metric("sketch_users", "Sketch Users", "Users who have uploaded at least one sketch", "performance", metrics_data["sketch_users"]),
                self._create_metric("producers", "Producers", "Total number of producers", "user", metrics_data["producers"])
            ]

            response = {
                "metrics": metrics,
                "timeRange": {
                    "start": self._format_date_iso(start_date),
                    "end": self._format_date_iso(end_date)
                }
            }

            if not self.disable_cache and self._are_metrics_valid(metrics):
                await self.caching_service.set(cache_key, response, timedelta(minutes=5))
                logger.info("Dashboard metrics cached successfully")

            return response

        except Exception as e:
            logger.error(f"Error fetching dashboard metrics: {str(e)}", exc_info=True)
            return {
                "metrics": [],
                "timeRange": {
                    "start": self._format_date_iso(start_date),
                    "end": self._format_date_iso(end_date)
                }
            }

    def _create_metric(self, id: str, name: str, description: str, category: str, data: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "id": id,
            "name": name,
            "description": description,
            "category": category,
            "interval": "daily",
            "data": data if data and data.get("value", 0) > 0 else {"value": 0, "previousValue": 0, "trend": "neutral", "changePercentage": 0, "daily_average": 0}
        }

    def _are_metrics_valid(self, metrics: List[Dict[str, Any]]) -> bool:
        return all(metric.get("data", {}).get("value", 0) > 0 for metric in metrics)

    async def get_user_statistics(self, start_date: datetime, end_date: datetime, gauge_type: str) -> List[Dict[str, Any]]:
        """Get user statistics including message and sketch counts"""
        try:
            message_counts = await self.opensearch_service.get_user_counts(start_date, end_date, "handleMessageInThread_start")
            sketch_counts = await self.opensearch_service.get_user_counts(start_date, end_date, "uploadSketch_end")
            render_counts = await self.opensearch_service.get_user_counts(start_date, end_date, "renderStart_end")
            
            user_ids = list(set(list(message_counts.keys()) + list(sketch_counts.keys()) + list(render_counts.keys())))
            user_details = await self.descope_service.get_user_details(user_ids)
            
            user_stats = []
            for user_id in user_ids:
                if user_id in user_details:
                    user_stat = {
                        "email": user_details[user_id]["email"],
                        "trace_id": user_id,
                        "messageCount": message_counts.get(user_id, 0),
                        "sketchCount": sketch_counts.get(user_id, 0),
                        "renderCount": render_counts.get(user_id, 0)
                    }
                    
                    should_include = False
                    if gauge_type == "thread_users" and user_stat["messageCount"] > 0:
                        should_include = True
                    elif gauge_type == "sketch_users" and user_stat["sketchCount"] > 0:
                        should_include = True
                    elif gauge_type == "render_users" and user_stat["renderCount"] > 0:
                        should_include = True
                    elif gauge_type == "medium_chat_users" and 5 <= user_stat["messageCount"] <= 20:
                        should_include = True
                    elif gauge_type == "active_chat_users" and user_stat["messageCount"] > 20:
                        should_include = True

                    if should_include:
                        user_stats.append(user_stat)
            
            if gauge_type in ["thread_users", "medium_chat_users", "active_chat_users"]:
                user_stats.sort(key=lambda x: x["messageCount"], reverse=True)
            elif gauge_type == "sketch_users":
                user_stats.sort(key=lambda x: x["sketchCount"], reverse=True)
            elif gauge_type == "render_users":
                user_stats.sort(key=lambda x: x["renderCount"], reverse=True)
            
            return user_stats
            
        except Exception as e:
            logger.error(f"Error getting user statistics: {str(e)}")
            raise

    async def get_user_events(self, trace_id: str, start_date: datetime, end_date: datetime) -> List[Dict[str, Any]]:
        """Fetch user events based on trace_id"""
        return await self.opensearch_service.get_user_events(trace_id, start_date, end_date)

    def _format_date_iso(self, dt: datetime) -> str:
        """Format datetime to ISO string"""
        return dt.strftime("%Y-%m-%dT%H:%M:%SZ")

# File: dashboardbackend/src/services/metrics_service.py

"""
Consolidated metrics service for analytics
"""
from typing import Dict, Any
import logging
from datetime import datetime
from src.utils.query_builder import OpenSearchQueryBuilder
from src.services.analytics.metrics.base import BaseMetricsService
from src.services.descope_service import DescopeService
from src.services.caching_service import CachingService

logger = logging.getLogger(__name__)

class AnalyticsMetricsService(BaseMetricsService):
    def __init__(self, opensearch_client, caching_service: CachingService, query_builder: OpenSearchQueryBuilder, index: str, timestamp_field: str, request_timeout: int, descope_service: DescopeService):
        super().__init__(opensearch_client, query_builder, index, timestamp_field, request_timeout, descope_service)
        self.caching_service = caching_service

    async def get_thread_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get count of users with message threads in time range"""
        query = self._get_date_range_query(start_date, end_date, "handleMessageInThread_start")
        return await self._execute_query(query, "thread users", start_date, end_date, "engagement")

    async def get_medium_chat_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get users with 5-20 message threads in time range"""
        logger.info(f"Fetching medium chat users for date range: {start_date} to {end_date}")
        query = self._get_date_range_query(start_date, end_date, "handleMessageInThread_start")
        query.update(self._build_thread_count_aggregation(min_count=5, max_count=20))
        return await self._execute_query(query, "medium chat users", start_date, end_date, "engagement")

    async def get_power_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get users with more than 20 message threads in time range"""
        logger.info(f"Fetching power users for date range: {start_date} to {end_date}")
        query = self._get_date_range_query(start_date, end_date, "handleMessageInThread_start")
        query.update(self._build_thread_count_aggregation(min_count=21))
        return await self._execute_query(query, "power users", start_date, end_date, "engagement")

    async def get_total_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get total number of users in time range"""
        query = self._get_date_range_query(start_date, end_date)
        return await self._execute_query(query, "total users", start_date, end_date, "user")

    async def get_producers(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get count of producers in time range"""
        query = self._get_date_range_query(start_date, end_date, "producer_activity")
        return await self._execute_query(query, "producers", start_date, end_date, "user")

    async def get_sketch_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get count of users who have uploaded sketches in time range"""
        query = self._get_date_range_query(start_date, end_date, "uploadSketch_end")
        return await self._execute_query(query, "sketch users", start_date, end_date, "performance")

    async def get_render_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Get count of users who have completed renders in time range"""
        query = self._get_date_range_query(start_date, end_date, "renderStart_end")
        return await self._execute_query(query, "render users", start_date, end_date, "performance")

    async def fetch_metrics(self, start_date: datetime, end_date: datetime) -> None:
        """Fetch all metrics at once"""
        start_date, end_date = self._validate_dates(start_date, end_date)
        self.thread_users = await self.get_thread_users(start_date, end_date)
        self.sketch_users = await self.get_sketch_users(start_date, end_date)
        self.render_users = await self.get_render_users(start_date, end_date)
        self.medium_chat_users = await self.get_medium_chat_users(start_date, end_date)
        self.power_users = await self.get_power_users(start_date, end_date)
        self.total_users = await self.get_total_users(start_date, end_date)
        self.producers = await self.get_producers(start_date, end_date)

    def get_metrics(self) -> Dict[str, Dict[str, Any]]:
        """Get all metrics"""
        return {
            "thread_users": self.thread_users,
            "sketch_users": self.sketch_users,
            "render_users": self.render_users,
            "medium_chat_users": self.medium_chat_users,
            "power_users": self.power_users,
            "total_users": self.total_users,
            "producers": self.producers
        }

    def combine_with_historical_data(self, current_data: Dict[str, Any], historical_data: Dict[str, Any]) -> Dict[str, Any]:
        """Combine current metrics with historical data"""
        combined_data = current_data.copy()
        combined_data["value"] += historical_data.get("value", 0)
        combined_data["daily_average"] += historical_data.get("daily_average", 0)
        return combined_data

# File: dashboardbackend/src/services/historical_data_service.py

"""
Historical data service for fetching and processing historical metrics
"""
from typing import Dict, Any, Tuple
from datetime import datetime, timezone, timedelta

class HistoricalDataService:
    # Historical data by month (cumulative totals)
    V1_DATA = [
        {
            "date": datetime(2024, 10, 1, tzinfo=timezone.utc),
            "total_users": 9770,
            "active_users": 1213,
            "producers": 1220
        },
        {
            "date": datetime(2024, 11, 1, tzinfo=timezone.utc),
            "total_users": 18634,
            "active_users": 4231,
            "producers": 4320
        },
        {
            "date": datetime(2024, 12, 1, tzinfo=timezone.utc),
            "total_users": 27058,
            "active_users": 9863,
            "producers": 9830
        },
        {
            "date": datetime(2025, 1, 26, tzinfo=timezone.utc),
            "total_users": 48850,
            "active_users": 16560,
            "producers": 16800
        }
    ]

    def __init__(self):
        self.min_date = datetime(2024, 10, 1, tzinfo=timezone.utc)
        self.max_date = datetime(2025, 1, 26, tzinfo=timezone.utc)
        self.latest_v1_data = self.V1_DATA[-1]

    @classmethod
    def get_v1_metrics(cls, start_date: datetime, end_date: datetime, include_v1: bool = True) -> Dict[str, Any]:
        """Get V1 metrics for the specific time range with daily averages"""
        if not include_v1:
            return cls._get_empty_metrics()

        # Return empty metrics if the date range is entirely after V1 end date
        if start_date > cls.V1_DATA[-1]["date"]:
            return cls._get_empty_metrics()

        start_date = cls._ensure_timezone(start_date)
        end_date = cls._ensure_timezone(end_date)

        # Don't look at data before Oct 1st 2024
        if start_date < cls.V1_DATA[0]["date"]:
            start_date = cls.V1_DATA[0]["date"]

        # Cap end date at V1 end date
        if end_date > cls.V1_DATA[-1]["date"]:
            end_date = cls.V1_DATA[-1]["date"]

        # Get metrics at start and end of range
        start_metrics, end_metrics = cls._get_metrics_for_range(start_date - timedelta(days=1), end_date)

        # Calculate the differences
        total_users = end_metrics["total_users"] - start_metrics["total_users"]
        active_users = end_metrics["active_users"] - start_metrics["active_users"]
        producers = end_metrics["producers"] - start_metrics["producers"]

        # Calculate days in range
        days_in_range = (end_date - start_date).days + 1
        if days_in_range < 1:
            days_in_range = 1

        # Calculate daily averages
        daily_averages = {
            "total_users": total_users / days_in_range,
            "active_users": active_users / days_in_range,
            "producers": producers / days_in_range
        }

        return {
            "total_users": total_users,
            "active_users": active_users,
            "producers": producers,
            "daily_averages": daily_averages,
            "cumulative": {
                "total_users": end_metrics["total_users"],
                "active_users": end_metrics["active_users"],
                "producers": end_metrics["producers"]
            }
        }

    @classmethod
    def get_all_time_metrics(cls) -> Dict[str, Any]:
        """Get all-time metrics (always includes full V1 data)"""
        metrics = {
            "total_users": cls.V1_DATA[-1]["total_users"],
            "active_users": cls.V1_DATA[-1]["active_users"],
            "producers": cls.V1_DATA[-1]["producers"]
        }

        # Calculate total days from Oct 1, 2024 to Jan 26, 2025
        total_days = (cls.V1_DATA[-1]["date"] - cls.V1_DATA[0]["date"]).days + 1
        if total_days < 1:
            total_days = 1

        daily_averages = {
            "total_users": metrics["total_users"] / total_days,
            "active_users": metrics["active_users"] / total_days,
            "producers": metrics["producers"] / total_days
        }

        return {
            "total_users": metrics["total_users"],
            "active_users": metrics["active_users"],
            "producers": metrics["producers"],
            "daily_averages": daily_averages,
            "start_date": cls.V1_DATA[0]["date"].isoformat(),
            "end_date": cls.V1_DATA[-1]["date"].isoformat()
        }

    @classmethod
    def _get_metrics_for_range(cls, start_date: datetime, end_date: datetime) -> Tuple[Dict[str, int], Dict[str, int]]:
        """Get metrics at start and end of range"""
        start_date = cls._ensure_timezone(start_date)
        end_date = cls._ensure_timezone(end_date)

        # Find metrics at start date
        start_metrics = None
        for i, data in enumerate(cls.V1_DATA):
            if data["date"] <= start_date:
                start_metrics = data
            else:
                break

        # Find metrics at end date
        end_metrics = None
        for data in reversed(cls.V1_DATA):
            if data["date"] <= end_date:
                end_metrics = data
                break

        # Use initial values if no data found
        if not start_metrics:
            start_metrics = cls.V1_DATA[0]
        if not end_metrics:
            end_metrics = cls.V1_DATA[0]

        return start_metrics, end_metrics

    @staticmethod
    def _ensure_timezone(date: datetime) -> datetime:
        """Ensure date is timezone-aware"""
        if date.tzinfo is None:
            return date.replace(tzinfo=timezone.utc)
        return date

    @staticmethod
    def _get_empty_metrics() -> Dict[str, Any]:
        """Return empty metrics structure"""
        return {
            "total_users": 0,
            "active_users": 0,
            "producers": 0,
            "daily_averages": {
                "total_users": 0,
                "active_users": 0,
                "producers": 0
            },
            "cumulative": {
                "total_users": 0,
                "active_users": 0,
                "producers": 0
            }
        }

# File: dashboardbackend/src/services/descope_service.py

"""
Descope service for user management and authentication
"""
import os
import logging
import ssl
import aiohttp
import certifi
from typing import Dict, List, Optional, Any
from datetime import datetime

logger = logging.getLogger(__name__)

class DescopeService:
    """Handles all Descope API interactions"""

    def __init__(self):
        self.api_url = os.getenv('DESCOPE_API_URL', 'https://api.descope.com/v1/mgmt/user/search')
        self.bearer_token = os.getenv('DESCOPE_BEARER_TOKEN')
        
        # Configure SSL context
        self.ssl_context = ssl.create_default_context(cafile=certifi.where())
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE

    async def get_total_users(self, end_date: Optional[datetime] = None, start_date: Optional[datetime] = None) -> Dict[str, Any]:
        """Get total users count within date range"""
        if not self.bearer_token:
            logger.error("No Descope bearer token provided")
            return {
                "value": 0,
                "previousValue": 0,
                "trend": "neutral",
                "changePercentage": 0,
                "daily_average": 0
            }

        try:
            headers = {
                'Authorization': f'Bearer {self.bearer_token}',
                'Content-Type': 'application/json'
            }

            payload = {
                "tenantIds": [],
                "text": "",
                "roleNames": [],
                "loginIds": [],
                "ssoAppIds": [],
                "customAttributes": {}
            }

            if end_date:
                payload["endTime"] = end_date.isoformat()
            if start_date:
                payload["startTime"] = start_date.isoformat()

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_url,
                    headers=headers,
                    json=payload,
                    ssl=self.ssl_context
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        users = data.get('users', [])
                        total_value = len(users)
                        logger.info(f"Found {total_value} users in Descope")

                        # Calculate metrics
                        days_in_range = (end_date - start_date).days + 1 if start_date and end_date else 1
                        daily_average = total_value / days_in_range if days_in_range > 0 else 0

                        return {
                            "value": total_value,
                            "previousValue": 0,  # We don't have historical data
                            "trend": "up" if total_value > 0 else "neutral",
                            "changePercentage": 0,  # No historical comparison
                            "daily_average": daily_average
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"Descope API error: {response.status} - {error_text}")
                        return {
                            "value": 0,
                            "previousValue": 0,
                            "trend": "neutral",
                            "changePercentage": 0,
                            "daily_average": 0
                        }

        except Exception as e:
            logger.error(f"Error fetching total users from Descope: {str(e)}", exc_info=True)
            return {
                "value": 0,
                "previousValue": 0,
                "trend": "neutral",
                "changePercentage": 0,
                "daily_average": 0
            }

    async def get_user_details(self, user_ids: List[str]) -> Dict[str, Dict[str, Any]]:
        """Get user details from Descope"""
        if not self.bearer_token:
            logger.error("No Descope bearer token provided")
            return {}

        if not user_ids:
            logger.warning("No user IDs provided")
            return {}

        try:
            headers = {
                'Authorization': f'Bearer {self.bearer_token}',
                'Content-Type': 'application/json'
            }

            # Split user IDs into chunks to avoid too large requests
            chunk_size = 100
            user_details = {}

            for i in range(0, len(user_ids), chunk_size):
                chunk = user_ids[i:i + chunk_size]
                
                payload = {
                    "customAttributes": {
                        "v2UserId": chunk
                    }
                }

                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        self.api_url,
                        headers=headers,
                        json=payload,
                        ssl=self.ssl_context
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            users = data.get('users', [])
                            logger.info(f"Found {len(users)} users in Descope for chunk of {len(chunk)} IDs")
                            
                            # Create mapping of v2UserId to user details
                            for user in users:
                                v2user_id = user.get('customAttributes', {}).get('v2UserId')
                                if v2user_id and v2user_id in chunk:
                                    user_details[v2user_id] = {
                                        'email': user.get('email', ''),
                                        'name': user.get('name', ''),
                                        'createdTime': user.get('createdTime', ''),
                                        'lastLoginTime': user.get('lastLoginTime', '')
                                    }
                        else:
                            error_text = await response.text()
                            logger.error(f"Descope API error: {response.status} - {error_text}")

            # For any user IDs that weren't found, add placeholder data
            for user_id in user_ids:
                if user_id not in user_details:
                    user_details[user_id] = {
                        'email': f'Unknown ({user_id})',
                        'name': '',
                        'createdTime': '',
                        'lastLoginTime': ''
                    }

            return user_details

        except Exception as e:
            logger.error(f"Error fetching user details from Descope: {str(e)}", exc_info=True)
            return {user_id: {
                'email': f'Error ({user_id})',
                'name': '',
                'createdTime': '',
                'lastLoginTime': ''
            } for user_id in user_ids}

# File: dashboardbackend/src/services/caching_service.py

"""
Caching service for storing and retrieving cached data
"""
import json
import logging
from typing import Any, Optional, Dict
from datetime import datetime, timedelta
import redis.asyncio as redis

logger = logging.getLogger(__name__)

class CachingService:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.default_ttl = timedelta(minutes=5)

    async def disconnect(self) -> None:
        """Disconnect from Redis"""
        if self.redis:
            await self.redis.close()
            logger.info("Disconnected from Redis")

    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        try:
            value = await self.redis.get(key)
            if value:
                logger.debug(f"Successfully retrieved cached data for key: {key}")
                return json.loads(value)
            return None
        except Exception as e:
            logger.warning(f"Redis get failed: {str(e)}")
            return None

    async def set(self, key: str, value: Any, ttl: Optional[timedelta] = None) -> bool:
        """Set value in cache with optional expiration"""
        try:
            if isinstance(value, (dict, list)):
                value = json.dumps(value)
            elif not isinstance(value, str):
                value = str(value)

            expiry = ttl or self.default_ttl
            await self.redis.set(key, value, ex=int(expiry.total_seconds()))
            logger.debug(f"Successfully cached data for key: {key}")
            return True
        except Exception as e:
            logger.warning(f"Redis set failed: {str(e)}")
            return False

    async def delete(self, key: str) -> bool:
        """Delete value from cache"""
        try:
            await self.redis.delete(key)
            logger.debug(f"Successfully deleted cached data for key: {key}")
            return True
        except Exception as e:
            logger.warning(f"Redis delete failed: {str(e)}")
            return False

    async def clear_all(self) -> bool:
        """Clear all cached data"""
        try:
            await self.redis.flushdb()
            logger.info("Successfully cleared all cached data")
            return True
        except Exception as e:
            logger.warning(f"Redis flush failed: {str(e)}")
            return False

    async def get_or_set(self, key: str, value_func, ttl: Optional[timedelta] = None) -> Any:
        """Get value from cache or set it if not present"""
        cached_value = await self.get(key)
        if cached_value is not None:
            return cached_value

        value = await value_func()
        await self.set(key, value, ttl)
        return value

    async def get_many(self, keys: list) -> dict:
        """Get multiple values from cache"""
        try:
            values = await self.redis.mget(keys)
            result = {key: json.loads(value) if value else None for key, value in zip(keys, values)}
            logger.debug(f"Successfully retrieved multiple cached data for keys: {keys}")
            return result
        except Exception as e:
            logger.warning(f"Redis mget failed: {str(e)}")
            return {key: None for key in keys}

    async def set_many(self, data: dict, ttl: Optional[timedelta] = None) -> bool:
        """Set multiple values in cache with optional expiration"""
        try:
            pipeline = self.redis.pipeline()
            for key, value in data.items():
                if isinstance(value, (dict, list)):
                    value = json.dumps(value)
                elif not isinstance(value, str):
                    value = str(value)
                
                expiry = ttl or self.default_ttl
                pipeline.set(key, value, ex=int(expiry.total_seconds()))
            
            await pipeline.execute()
            logger.debug(f"Successfully cached multiple data for keys: {list(data.keys())}")
            return True
        except Exception as e:
            logger.warning(f"Redis mset failed: {str(e)}")
            return False

    async def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics
        
        Returns:
            Dict with cache stats
        """
        if not self.redis:
            logger.error("Redis not connected")
            return {}

        try:
            info = await self.redis.info()
            keys = await self.redis.keys('*')
            
            stats = {
                "total_keys": len(keys),
                "used_memory": info.get('used_memory_human'),
                "connected_clients": info.get('connected_clients'),
                "last_save_time": datetime.fromtimestamp(
                    int(info.get('rdb_last_save_time', 0))
                ).isoformat()
            }
            logger.debug("Successfully retrieved cache stats")
            return stats
            
        except Exception as e:
            logger.error(f"Error getting cache stats: {str(e)}")
            return {}

# File: dashboardbackend/src/services/opensearch_service.py

"""
OpenSearch service for querying and aggregating analytics data
"""
import os
import logging
import ssl
import certifi
from typing import Dict, Any, List
from datetime import datetime
from opensearchpy import AsyncOpenSearch
from src.utils.query_builder import OpenSearchQueryBuilder

logger = logging.getLogger(__name__)

class OpenSearchService:
    def __init__(self):
        self.index = "events-v2"
        self.timestamp_field = "timestamp"
        self.request_timeout = int(os.getenv('MAX_QUERY_TIME', '30'))  # Request timeout in seconds
        self.query_builder = OpenSearchQueryBuilder()

        # Configure OpenSearch client
        self.opensearch_url = os.getenv('OPENSEARCH_URL', 'https://localhost:9200')
        self.opensearch_username = os.getenv('OPENSEARCH_USERNAME')
        self.opensearch_password = os.getenv('OPENSEARCH_PASSWORD')
        
        # Configure SSL context for OpenSearch
        self.ssl_context = ssl.create_default_context(cafile=certifi.where())
        self.ssl_context.check_hostname = False  # Disable hostname checking for localhost
        self.ssl_context.verify_mode = ssl.CERT_NONE  # Allow self-signed certificates for local development
        
        # Initialize OpenSearch client
        self.client = AsyncOpenSearch(
            hosts=[self.opensearch_url],
            http_auth=(self.opensearch_username, self.opensearch_password),
            use_ssl=True,
            verify_certs=False,
            ssl_assert_hostname=False,
            ssl_show_warn=False,
            ssl_context=self.ssl_context
        )

    async def search(self, query: Dict[str, Any], size: int = 0) -> Dict[str, Any]:
        """Execute a search query on OpenSearch"""
        logger.debug(f"Executing OpenSearch query: index={self.index}, query={query}, size={size}")
        try:
            result = await self.client.search(
                index=self.index,
                body=query,
                size=size,
                request_timeout=self.request_timeout
            )
            logger.debug(f"OpenSearch query result: {result}")
            return result
        except Exception as e:
            logger.error(f"Error executing OpenSearch query: {str(e)}", exc_info=True)
            raise

    async def get_user_counts(self, start_date: datetime, end_date: datetime, event_name: str) -> Dict[str, int]:
        """Get user counts for a specific event in the given time range"""
        logger.debug(f"Getting user counts for event: {event_name}, start_date: {start_date}, end_date: {end_date}")
        time_filter = {
            "range": {
                self.timestamp_field: {
                    "gte": self._format_date_os(start_date),
                    "lte": self._format_date_os(end_date)
                }
            }
        }

        query = self.query_builder.build_composite_query(
            must_conditions=[
                {"term": {"event_name.keyword": event_name}},
                time_filter
            ],
            aggregations={
                "aggs": {
                    "users": {
                        "terms": {
                            "field": "trace_id.keyword",
                            "size": 10000
                        }
                    }
                }
            }
        )

        try:
            result = await self.search(query)
            user_counts = {
                bucket["key"]: bucket["doc_count"]
                for bucket in result["aggregations"]["users"]["buckets"]
            }
            logger.debug(f"User counts result: {user_counts}")
            return user_counts
        except Exception as e:
            logger.error(f"Error getting user counts for event {event_name}: {str(e)}", exc_info=True)
            return {}

    async def get_user_events(self, trace_id: str, start_date: datetime, end_date: datetime) -> List[Dict[str, Any]]:
        """Fetch user events based on trace_id"""
        logger.debug(f"Getting user events for trace_id: {trace_id}, start_date: {start_date}, end_date: {end_date}")
        time_filter = {
            "range": {
                self.timestamp_field: {
                    "gte": self._format_date_os(start_date),
                    "lte": self._format_date_os(end_date)
                }
            }
        }

        query = self.query_builder.build_composite_query(
            must_conditions=[
                {"term": {"trace_id.keyword": trace_id}},
                time_filter
            ],
            sort=[{self.timestamp_field: {"order": "desc"}}]
        )

        try:
            result = await self.search(query, size=100)  # Limit to 100 most recent events
            events = [hit["_source"] for hit in result["hits"]["hits"]]
            logger.debug(f"User events result: {events}")
            return events
        except Exception as e:
            logger.error(f"Error fetching user events: {str(e)}", exc_info=True)
            return []

    def _format_date_os(self, dt: datetime) -> int:
        """Format datetime to OpenSearch timestamp"""
        return int(dt.timestamp() * 1000)

# File: dashboardbackend/src/utils/query_builder.py

"""OpenSearch query builder utility"""
from typing import Dict, Optional, List, Any, Union
from datetime import datetime
from dateutil.parser import parse


class OpenSearchQueryBuilder:
    def build_date_range_query(self, start_time: Union[int, str], end_time: Union[int, str]) -> Dict:
        """
        Build a date range query for OpenSearch.

        Args:
            start_time: Timestamp in milliseconds since epoch or ISO format string
            end_time: Timestamp in milliseconds since epoch or ISO format string

        Returns:
            Dict containing the date range query
        """
        return {"range": {"timestamp": {"gte": start_time, "lte": end_time}}}

    def build_aggregation_query(
        self, agg_field: str, interval: Optional[str] = None
    ) -> Dict:
        """
        Build an aggregation query for OpenSearch.

        Args:
            agg_field: Field to aggregate on
            interval: Time interval for date histogram aggregation ('hour', 'day', 'week', 'month')

        Returns:
            Dict containing the aggregation query
        """
        # Define interval mappings and types
        fixed_intervals = {"hour": "1h", "day": "1d"}
        calendar_intervals = {"week": "week", "month": "month"}

        aggs = {}

        if interval:
            date_histogram = {
                "field": "timestamp",
                "min_doc_count": 0,
                "format": "yyyy-MM-dd'T'HH:mm:ssZ",
            }

            if interval in fixed_intervals:
                date_histogram["fixed_interval"] = fixed_intervals[interval]
            elif interval in calendar_intervals:
                date_histogram["calendar_interval"] = calendar_intervals[interval]
            else:
                # Default to 1d fixed interval
                date_histogram["fixed_interval"] = "1d"

            aggs["time_buckets"] = {"date_histogram": date_histogram}

        aggs[f"{agg_field}_buckets"] = {"terms": {"field": agg_field, "size": 10000}}

        return {"aggs": aggs}

    def build_paginated_query(
        self, search_after: Optional[str] = None, size: int = 100
    ) -> Dict:
        """
        Build a paginated query using search_after for deep pagination.

        Args:
            search_after: Token for pagination
            size: Number of results per page

        Returns:
            Dict containing the pagination parameters
        """
        query = {
            "sort": [{"timestamp": "desc"}, {"_id": "desc"}],
            "size": min(size, 1000),  # Enforce maximum page size
        }

        if search_after:
            query["search_after"] = search_after.split(",")

        return query

    def build_composite_query(
        self,
        must_conditions: List[Dict[str, Any]],
        source_fields: Optional[List[str]] = None,
        aggregations: Optional[Dict[str, Any]] = None,
        pagination: Optional[Dict[str, Any]] = None,
        sort: Optional[List[Dict[str, Any]]] = None,
    ) -> Dict[str, Any]:
        """
        Build a complete composite query combining multiple conditions.

        Args:
            must_conditions: List of must conditions for bool query
            source_fields: List of fields to include in _source
            aggregations: Aggregation queries
            pagination: Pagination parameters
            sort: List of sort conditions

        Returns:
            Dict containing the complete query
        """
        query = {"query": {"bool": {"must": must_conditions}}}

        if source_fields:
            query["_source"] = source_fields

        if aggregations:
            query.update(aggregations)

        if pagination:
            query.update(pagination)

        if sort:
            query["sort"] = sort

        return query

# File: dashboardbackend/src/api/metrics.py

"""
Metrics API endpoints
"""
from datetime import datetime, timezone
from typing import Optional
from quart import Blueprint, jsonify, request, current_app
from src.services.analytics_service import AnalyticsService

metrics_bp = Blueprint('metrics', __name__)

def init_app(app):
    """Initialize metrics blueprint"""
    app.register_blueprint(metrics_bp, url_prefix='/metrics')

@metrics_bp.route('/', methods=['GET'])
async def get_metrics():
    """Get dashboard metrics"""
    try:
        # Parse date parameters
        start_date = request.args.get('startDate')
        end_date = request.args.get('endDate')
        include_v1 = request.args.get('includeV1', 'true').lower() == 'true'

        current_app.logger.info(f"Fetching metrics with params: start={start_date}, end={end_date}, include_v1={include_v1}")

        if not start_date or not end_date:
            current_app.logger.error("Missing date parameters")
            return jsonify({
                'error': 'Missing date parameters'
            }), 400

        try:
            # Convert to datetime objects
            start_date = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            end_date = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
        except ValueError as e:
            current_app.logger.error(f"Invalid date format: {str(e)}")
            return jsonify({
                'error': 'Invalid date format'
            }), 400

        # Get metrics with V1 data
        metrics_data = await current_app.analytics_service.get_dashboard_metrics(
            start_date=start_date,
            end_date=end_date,
            include_v1=include_v1
        )

        current_app.logger.info(f"Successfully fetched metrics: {metrics_data}")
        return jsonify(metrics_data)

    except Exception as e:
        current_app.logger.error(f"Error in get_metrics: {str(e)}", exc_info=True)
        return jsonify({
            'error': str(e)
        }), 500

@metrics_bp.route('/user-stats', methods=['GET'])
async def get_user_stats():
    """Get user statistics"""
    try:
        # Parse parameters
        start_date = request.args.get('startDate')
        end_date = request.args.get('endDate')
        gauge_type = request.args.get('gaugeType')

        current_app.logger.info(f"Fetching user stats with params: start={start_date}, end={end_date}, gauge_type={gauge_type}")

        if not all([start_date, end_date, gauge_type]):
            current_app.logger.error("Missing required parameters")
            return jsonify({
                'error': 'Missing required parameters'
            }), 400

        try:
            # Convert to datetime objects
            start_date = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            end_date = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
        except ValueError as e:
            current_app.logger.error(f"Invalid date format: {str(e)}")
            return jsonify({
                'error': 'Invalid date format'
            }), 400

        # Get user statistics
        user_stats = await current_app.analytics_service.get_user_statistics(
            start_date=start_date,
            end_date=end_date,
            gauge_type=gauge_type
        )

        current_app.logger.info(f"Successfully fetched user stats: {len(user_stats)} users")
        return jsonify({
            'status': 'success',
            'data': user_stats,
            'timeRange': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            }
        })

    except Exception as e:
        current_app.logger.error(f"Error in get_user_stats: {str(e)}", exc_info=True)
        return jsonify({
            'error': str(e)
        }), 500

@metrics_bp.route('/user-events', methods=['GET'])
async def get_user_events():
    """Get user events"""
    try:
        # Parse parameters
        trace_id = request.args.get('traceId')
        start_date = request.args.get('startDate')
        end_date = request.args.get('endDate')

        current_app.logger.info(f"Fetching user events: traceId={trace_id}, start={start_date}, end={end_date}")

        if not all([trace_id, start_date, end_date]):
            current_app.logger.error("Missing required parameters")
            return jsonify({
                'status': 'error',
                'error': 'Missing required parameters'
            }), 400

        try:
            # Convert dates with UTC timezone
            start_date = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            end_date = datetime.fromisoformat(end_date.replace('Z', '+00:00'))

            # Ensure dates are in UTC
            if start_date.tzinfo is None:
                start_date = start_date.replace(tzinfo=timezone.utc)
            if end_date.tzinfo is None:
                end_date = end_date.replace(tzinfo=timezone.utc)

            # Get user events
            events = await current_app.analytics_service.get_user_events(
                trace_id=trace_id,
                start_date=start_date,
                end_date=end_date
            )

            return jsonify({
                'status': 'success',
                'data': events,
                'timeRange': {
                    'start': start_date.isoformat(),
                    'end': end_date.isoformat()
                }
            })

        except ValueError as e:
            current_app.logger.error(f"Date format error: {e}")
            return jsonify({
                'status': 'error',
                'error': f'Invalid date format: {str(e)}'
            }), 400

    except Exception as e:
        current_app.logger.error(f"Error in get_user_events: {str(e)}", exc_info=True)
        return jsonify({
            'status': 'error',
            'error': str(e)
        }), 500

# File: dashboardbackend/src/core/__init__.py

"""
Initialize application services and connections
"""
import logging
import os
from quart import Quart
from src.services.caching_service import CachingService
from src.services.analytics_service import AnalyticsService
from src.services.descope_service import DescopeService
from src.services.opensearch_service import OpenSearchService
from src.utils.query_builder import OpenSearchQueryBuilder
from opensearchpy import AsyncOpenSearch
import redis.asyncio as redis
from dotenv import load_dotenv

logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize Redis client
redis_client = redis.from_url(
    os.getenv('REDIS_URL', 'redis://localhost:6379'),
    decode_responses=True
)

# Initialize OpenSearch client
opensearch_client = AsyncOpenSearch(
    hosts=[os.getenv('OPENSEARCH_URL', 'https://localhost:9200')],
    http_auth=(
        os.getenv('OPENSEARCH_USERNAME', ''),
        os.getenv('OPENSEARCH_PASSWORD', '')
    ),
    use_ssl=True,
    verify_certs=False,
    ssl_show_warn=False
)

async def init_services(app: Quart) -> None:
    """Initialize all required services"""
    try:
        # Initialize CachingService
        caching_service = CachingService(redis_client)

        # Store cache instance in app context
        app.cache = caching_service

        # Initialize OpenSearchQueryBuilder
        query_builder = OpenSearchQueryBuilder()

        # Initialize OpenSearchService
        opensearch_service = OpenSearchService()
        opensearch_service.client = opensearch_client

        # Initialize DescopeService
        descope_service = DescopeService()

        # Initialize AnalyticsService
        analytics_service = AnalyticsService(
            caching_service,
            opensearch_service,
            query_builder,
            descope_service
        )

        # Store services in app context
        app.cache = caching_service
        app.descope_service = descope_service
        app.analytics_service = analytics_service

        # Add cleanup on app teardown
        @app.before_serving
        async def startup():
            logger.info("Starting up services...")

        @app.after_serving
        async def shutdown():
            logger.info("Shutting down services...")
            await caching_service.disconnect()
            await redis_client.close()
            await opensearch_client.close()

    except Exception as e:
        logger.error(f"Error initializing services: {str(e)}")
        raise

__all__ = ['redis_client', 'opensearch_client', 'init_services']

# File: dashboardbackend/tests/test_analytics_service.py

# Error: File not found

# File: dashboardbackend/requirements.txt

aiohttp==3.9.3
certifi==2022.12.7
opensearch-py==2.3.0
APScheduler==3.10.1
SQLAlchemy==1.4.47
python-dotenv==1.0.0

# File: dashboardbackend/run.py

"""
Run the application server
"""
import asyncio
import logging
import os
from hypercorn.config import Config
from hypercorn.asyncio import serve
from dotenv import load_dotenv
from src.app import init_app

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# Initialize application
app = None

async def init():
    global app
    try:
        # Load environment variables
        load_dotenv()
        
        # Initialize application
        app = await init_app()
        return app
    except Exception as e:
        logger.error(f"Failed to initialize app: {str(e)}")
        raise

async def main():
    """Main entry point"""
    try:
        app = await init()
        
        # Configure Hypercorn
        config = Config()
        config.bind = [f"0.0.0.0:{os.getenv('PORT', '5001')}"]
        config.use_reloader = True
        
        # Start server
        await serve(app, config)
        
    except Exception as e:
        logger.error(f"Failed to start server: {str(e)}")
        raise

# Initialize app for Flask CLI
asyncio.run(init())

if __name__ == "__main__":
    asyncio.run(main())